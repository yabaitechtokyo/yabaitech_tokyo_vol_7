@require: class-yabaitech/yabaitech
@require: azmath/azmath
@require: easytable/easytable
@require: latexcmds/latexcmds

@import: ../lib/code

module Binary : sig

  val article : block-text

end = struct

  open Code2
  open EasyTableAlias

  let vop-scheme charf s =
    let mop = charf MathOp s in
      math-pull-in-scripts MathOp MathOp
        (fun moptS moptT -> (
          let m =
            match moptS with
            | None     -> mop
            | Some(mS) -> math-lower mop mS
          in
            match moptT with
            | None     -> m
            | Some(mT) -> math-upper m mT
        ))

  let vop = vop-scheme math-char

  let-math \argmax = vop `arg max`

  let article = '<
    +chapter(|
      bibliography = [];
      title = {不完全情報ゲームのナッシュ均衡を CFR (Counterfactual Regret Minimization) アルゴリズムで求めよう};
      title-for-toc = Option.none;
      subtitle = Option.none;
      author = {ばいなり};
    |)
    <
      +section{はじめに} <
        +p{
          yabaitech.tokyo vol.7 を手に取ってくださりありがとうございます。
          前回の vol.6 では、世界的に見て最もポピュラーなポーカーの変種と言える
          \dfn{テキサスホールデム}の役判定を高速に行う話を書かせていただきましたが、
          今回も引き続いてポーカーを念頭に置いた記事を投稿したいと思います。
        }

        +p{
          具体的には、ポーカーを含む\dfn{不完全情報ゲーム}において\dfn{最適解}とも言える
          \dfn{ナッシュ均衡}を求めるプログラムの実装を目指します。
          もちろん、実際にナッシュ均衡が現実的な時間内に求まる（より正確には計算結果が十分収束する）
          かどうかはゲームの複雑さに依るため、複雑性の非常に高いテキサスホールデムの
          完全解析を行おうと思うと本記事の内容ではまったく不可能ですが、それほど複雑でない
          ゲームであれば最適解をプログラムによって求めることができるようになります。
        }

        +p{
          ゲーム理論周りに馴染みの無い方の中には、タイトルには仰々しい単語しか並んどらんし
          ナッシュ均衡とか言われてもさっぱり分からんという方もいらっしゃるかもしれませんが、
          そういった方にもせめて雰囲気は伝わるような記事となるよう心掛けたので、
          ややタフな内容かもしれませんが各々の楽しみ方で読み進めていただけると幸いです
          \footnote{と言うよりむしろ、筆者も特にゲーム理論の専門家でも何でもなくて、
          ゲーム理論を齧っただけの筆者なりに初学者に興味を持ってもらうのを目的に記事を書いています。}
          。
        }
      >

      +section{用語の説明} <
        +p{
          本章では、本記事のタイトルを構成する主なキーワードとなっている「不完全情報ゲーム」
          「ナッシュ均衡」「CFRアルゴリズム」の三点について、それぞれ説明をしていきます。
        }

        +subsection{不完全情報ゲーム} <
          +p{
            まず、タイトルの最初の単語である\dfn{不完全情報ゲーム}とは何ぞや？　
            というところから見ていきましょう。
            簡単に言ってしまえば、ボードゲームなどのゲームにおいて、
            各々のプレイヤーがすべての情報にアクセスできるならばそのゲームは\dfn{完全情報ゲーム}、
            そうでないなら\dfn{不完全情報ゲーム}であると呼ばれます。
          }

          +p{
            具体的な例を挙げると、\dfn{オセロ}や\dfn{将棋}といったゲームはお互いのプレイヤーが
            同じ盤面の情報（より正確には棋譜の情報）を共有していて、特定のプレイヤーからしか
            アクセスできないような情報は存在しないため、完全情報ゲームに分類されます。
            逆に、\dfn{ポーカー}や\dfn{麻雀}といったゲームは各々のプレイヤーが自分にしか分からない
            手札を持っていて、各プレイヤーがアクセスできる情報は同じではないため、不完全情報ゲームに
            分類されます。
            さらに、\dfn{じゃんけん}といった複数のプレイヤーが同時にアクションを起こす
            必要のあるゲームも不完全情報ゲームです。
            \dfn{バックギャモン}や\dfn{すごろく}といったゲームはどちらに分類すべきか微妙なところで、
            盤面がプレイヤーにすべて公開されているという観点では完全情報ゲームですが、
            サイコロなどによる確率的なアクションは仮想的なプレイヤーが行っているものと見なした上で、
            そのプレイヤーは将来の出目をすべて知っているのに順番にしか教えてくれないものとして
            考えると、持っている情報に格差があるため不完全情報ゲームであるとも解釈できます。
            これについては「不確定完全情報ゲーム」という分類の仕方もあり、そちらの方がより正確では
            あるのですが、本記事ではこれらは不完全情報ゲームとして扱うことにしておきます。
          }

          +p{
            完全情報ゲームでは、盤面が与えられると神の視点からはすでに勝敗（または引き分け）が
            定まっており、例えば ${6 \times 6} マスのオセロは初期盤面からお互いが最善を尽くすと
            後手が４石差で勝つことが知られています。
            この完全情報ゲームの攻略に関しては、完全読み切りが極めて困難であるような複雑なゲームに
            対しても、ディープラーニングと強化学習を組み合わせた\dfn{深層強化学習}を用いて
            \dfn{AlphaGo}や\dfn{AlphaZero}といったプログラムが盤面評価の精度の面でブレークスルーを
            最近引き起こしたことをご存じの方も多いでしょう。
          }

          +p{
            不完全情報ゲームは、完全情報ゲームが持つこのような性質を持っていません。
            すべてのプレイヤーがそれぞれに与えられた情報をもとに「最善手」を取り続けても、
            さまざまな要因によって勝敗は変化しますし、そもそも「最善手」というのがある一手に
            定まらずに複数の候補から確率的にアクションを選択するべき状況となることも少なくありません。
            「最善手」がある一手に定まらないというのは、例えばじゃんけんにおいて常にグーを出すという
            戦略が非常に弱いことを考えてもらえば分かりやすいでしょう。
            また、不完全情報ゲームにおいては評価が可能なのは戦略であってアクションではないという点も
            非常に重要です。
            つまり、例えばじゃんけんにおいて「グーとチョキとパーを等確率で出す戦略」の良し悪しを
            評価することはできますが、「実際にグーを出した」というアクションを単体で評価することは
            できないということです。
            この性質がプログラムによる精度の高い盤面評価を難しいものにしています。
          }
        >

        +subsection{ナッシュ均衡} <
          +p{
            不完全情報ゲームについて説明したところで、次のキーワードである\dfn{ナッシュ均衡}に
            ついても説明していきましょう。
            ナッシュ均衡とは、ざっくり言ってしまえば「どのプレイヤーも利得（の期待値）を
            これ以上増やすことができない状態」のことを指します。
            はじめにナッシュ均衡のことを「最適解」と呼びましたが、具体的にはこのような意味だった
            わけですね。
          }

          +p{
            厳密な定義については記号などをちゃんと導入した後で再度確認しますが、もうちょっとだけ
            正確に文章で記述すると「どのプレイヤーも、その他のプレイヤーがそのナッシュ均衡に従って
            戦略を選択している限りは、自分の戦略を変更することによって利得（の期待値）をこれ以上
            増やすことができないような戦略の組」のことをナッシュ均衡と言います。
          }

          +subsubsection{男女の争い} <
            +p{
              簡単な例として\dfn{男女の争い}と呼ばれるゲームを見てみましょう。
              今どきナントカコレクトネスに反しそうな名称・設定ではありますが、そこには目を瞑って
              いただくとして、このゲームはある男女の組がデートに行こうとしており、一緒に行動したいと
              いう点では同意しているものの、お互いが行きたい場所が異なるという状況をモデル化したものです。
              ここでは男性はスポーツ観戦に、女性は映画鑑賞に行きたいということにしておきましょう。
              このとき、それぞれ男性と女性が得る利得は次のように定められるものとします：
            }

            +centering{
              \easytable?:[t; b; m 1; v 1][c; c; c] {
                | 男性 ＼ 女性 | スポーツ観戦 | 映画鑑賞
                | スポーツ観戦 | (2, 1) | (0, 0)
                | 映画鑑賞 | (0, 0) | (1, 2)
              |}
            }

            +p{
              つまり、男性側は女性とともにスポーツ観戦を行った場合に最も利得を多く得ますが（２点）、
              次いで利得を多く得られるのは女性とともに映画鑑賞を行った場合で（１点）、
              一緒に行動できなかった場合は利得を得ることができません（０点）。
            }

            +p{
              このような設定下におけるナッシュ均衡は、当たり前のようですが
              （男性 ${\to} スポーツ鑑賞, 女性 ${\to} スポーツ鑑賞）と
              （男性 ${\to} 映画鑑賞, 女性 ${\to} 映画鑑賞）の二組となります
              \footnote{本当はこの二組だけではないのですがそれに関しては後述します。}。
              男性側にとっては女性とともに映画鑑賞に行くというのは次善策なわけですが、女性側が
              映画鑑賞を選択している場合は、男性側の利得を最大化するためには男性側も映画鑑賞を
              選択する必要があるため、これが均衡の一つとなります。
            }

            +p{
              この単純な例からナッシュ均衡の重要な性質を二点見出すことができます。
              一つは、一般にナッシュ均衡は必ずしも一組とは限らないということです。
              またもう一つは、ナッシュ均衡はすべてのプレイヤーの戦略の「組」として与えられるため、
              ある一人のプレイヤーにとって「相手の戦略に関わらず最善」という戦略は存在しない場合が
              あるということです。
            }
          >

          +subsubsection{じゃんけんと混合戦略} <
            +p{
              ところで、本記事では主にポーカーの戦略を考えたいわけですから、ゲームの条件として
              「各プレイヤーの利得はゼロサムである」（各プレイヤーの利得を足し合わせるとゼロになる）
              ことも追加しましょう。
              先ほどの男女の争いの例はゼロサムではありませんでしたので、ゼロサムであるゲームとして
              \dfn{じゃんけん}
              \footnote{
                知人が豪語していたじゃんけんの「必勝法」を紹介しましょう。曰く、
                「『最初はグー』の状態から遷移に最も時間が掛かるのはパーに切り替える場合だ。
                相手がパーに切り替えようとしているかどうかを見極めるには、相手の小指に注目すれば良い。
                よって、基本はこちらはグーのままだが、相手の小指が動いたらこちらはチョキに切り替えろ。
                グーからチョキに切り替えるのは、パーに切り替えるよりは素早く行えるから、
                優れた動体視力と反射神経があれば間に合わせることができる」。
                全員がこの「必勝法」を採用するとグーを出し続ける無限ループに陥ることになるので、
                そもそも「必勝法」の定義を満たしていないように筆者的には思うのですが、
                額面通りに事を運ばせられるのならば確かにまあ負けることは無いので、
                動体視力と反射神経に自信のある方は試してみると良いのではないでしょうか。
              }
              を考えてみることにしましょう。
              特に解説は要らないと思いますが、じゃんけんの利得表は次のようになります：
            }

            +centering{
              \easytable?:[t; b; m 1; v 1][c; c; c; c] {
                | | グー | チョキ | パー
                | グー | (0, 0) | (1, -1) | (-1, 1)
                | チョキ | (-1, 1) | (0, 0) | (1, -1)
                | パー | (1, -1) | (-1, 1) | (0, 0)
              |}
            }

            +p{
              このようなゲームにおいてもナッシュ均衡は存在するのでしょうか？　
              例えばプレイヤーAがグーを出した場合を想定すると、プレイヤーBはパーを出すのが最善で、
              そうするとプレイヤーAはチョキを出すのが最善で…と循環してしまい、お互いが妥協できる
              均衡状態は一見して存在せず、よってナッシュ均衡も存在しないように思えます。
            }

            +p{
              ここで登場する概念が\dfn{混合戦略}というものです。
              混合戦略とは、例えばグーを50\%、チョキを30\%、パーを20\%で出すといったように、
              確率的にアクションを決定する戦略のことです。
              なお、これに対して例えば決まってグーを出すといったように確率によらない戦略は
              \dfn{純粋戦略}と呼ばれます。
              今回のじゃんけんの例では、直感的か非直感的かは人に依ると思いますが、三種類の手を
              それぞれ等確率に出す混合戦略を両者が採用する、というのがこのゲームにおける唯一の
              ナッシュ均衡となります。
            }

            +p{
              このナッシュ均衡の解釈は一筋縄ではいかないものです。
              三種類の手をそれぞれ等確率に出す混合戦略というのは、必ずグーを出すような非常に
              弱い相手に対しても利得の期待値がゼロにしかならない戦略です。
              ナッシュ均衡とはどのプレイヤーも利得をこれ以上増やすことができない状態のことで、
              言わば「最適解」であると説明されたはずなのに、そのような混合戦略を両者ともに
              取ることがそのナッシュ均衡なんです、と言われても納得できない方もいると思います。
              この事態を直感的に捉えられるようにするには、少なくとも筆者にとっては
              次のような言い換えを経る必要がありました：
            }

            +with-param(AZMathEquation.vmargin-between-eqn)(fun ctx -> get-font-size ctx) <
              +align?:(AZMathEquation.notag)(${
                |           |\text!{ナッシュ均衡ではどのプレイヤーも利得をこれ以上増やすことができない}
                |\Rightarrow|\text!{\dfn{二人ゼロサムゲーム}において一人がナッシュ均衡から外れたと仮定する}
                |\Rightarrow|\text!{ナッシュ均衡から外れたプレイヤーは利得が増えることはない}
                |\Rightarrow|\text!{\dfn{ゼロサム性}よりナッシュ均衡の戦略を取り続けたプレイヤーは利得が減ることはない}
                |\Rightarrow|\text!{\emph{ナッシュ均衡の戦略を取り続けることで利得の最低値が保証される}}
              |});
            >

            +p{
              つまり、\dfn{二人ゼロサムゲーム}においては、ナッシュ均衡の戦略を取ることは
              「相手の戦略に関わらず保証される利得を最大化できる」という意味で「最適解」なのであり、
              「相手の特定の戦略に対して利得を最大化できる」というようなものではないのです。
              なお後者のように特定の戦略を狙い撃ちするような戦略は\dfn{搾取戦略}と呼ばれ、
              例えば必ずグーを出す相手に対して必ずパーを出すといった戦略はこれに該当します。
              ただし、このように必ずパーを出すという戦略は、必ずチョキを出すという
              \dfn{対抗搾取戦略}に搾取されてしまいます。
              ナッシュ均衡とは、言わばすべてのプレイヤーがお互いを最大限搾取している理想的な状態で、
              達人どうしが戦ったらこうなりますという状態なわけですから、必ずパーを出すというような
              簡単に搾取されてしまう戦略は、その理想からはかけ離れてしまっているわけですね。
            }

            +p{
              なお、今回のじゃんけんの例で得られた「最適解」は三種類の手をそれぞれ等確率に出すと
              いうもので、これはどのような相手に対しても利得がゼロになってしまうというものでしたが、
              これはどちらかと言うと具体例が悪いという類のものです。
              よくあるポーカーの状況などでは、こちらがナッシュ均衡に基づく最善手を取っていて
              相手が最善手から離れているという場合、相手も最善手を取っている場合と比べて
              得られる利得は増加することがほとんどです。
            }

            +p{
              ところで、ある時点からゲームが突然に二人ゲームに限定されてしまいましたが、
              三人以上が関与するゼロサムゲームではナッシュ均衡に意味はあるのでしょうか。
              この点については、ふわっとした結論として「ナッシュ均衡に従うのは実用上は
              それなりに強いが、理論的な保証は無くなってしまう」と言うくらいが精々です。
              理論的な保証が無くなってしまうというのは、三人ゼロサムゲームは二人非ゼロサムゲームの
              一般化である（ゼロサムとなるようにダミープレイヤーを一人追加すれば良い）ことに気付くと
              分かりやすいですが、男女の争いの例で学んだように「相手の戦略に関わらず最善」と
              いうような戦略は存在しない場合があるという意味です。
              ただし、実用上はナッシュ均衡に従ってプレイするとそれなりに強いことが多く、
              理論的な保証が無くなることを理解していながらも、実用上の汎用的な強さを重視して、
              三人以上が関与するゲームにおいてもナッシュ均衡を求めることが実際にはよく目標と
              されています。
            }

            +p{
              さて、説明がやや長くなってしまったものの、じゃんけんを例にゼロサムゲームにおける
              ナッシュ均衡の性質について深堀りしてみましたが、いかがでしたでしょうか。
              本記事ではナッシュ均衡を求めるプログラムをこれから語っていくことになるので、
              「ナッシュ均衡を求めることにどのような意味があるのか」についてあやふやなままだと、
              記事としての説得力に欠けるかなあということで詳しめに説明してみました。
            }

            +p{
              なおこれは余談ですが、ほとんどすべての有限ゲームにはナッシュ均衡が奇数個存在することが
              知られています（ここでの「ほとんどすべて」は数学用語です）。
              じゃんけんについてはナッシュ均衡は一つだったので良いとして、男女の争いについては
              中途半端に二つしか見つけられていませんが、どういうことなのでしょうか。
              実は、これは混合戦略を考慮していなかったことが原因で、混合戦略までちゃんと考えると
              （男性 ${\to} ${2\ /\ 3} の確率でスポーツ観戦,
                女性 ${\to} ${2\ /\ 3} の確率で映画鑑賞）
              という戦略の組もナッシュ均衡となります。
              ナッシュ均衡、定義は簡単なようでいて奥が深いです。
            }
          >
        >

        +subsection{CFR (Counterfactual Regret Minimization) アルゴリズム} <
          +p{
            さて、ナッシュ均衡を乗り越えたと思ったらまたしても仰々しいキーワードが出てきて
            しまいましたね。
            本節ではCFRアルゴリズムがどのようなアルゴリズムなのか、まずは数式を使わずに文章で
            雰囲気を掴んでいただければと思います。
          }

          +p{
            まず、キーワード中の\dfn{リグレット(regret)}の部分から説明を始めることにしましょう。
            リグレットとは直訳すると\dfn{後悔}のことで、日本語訳する際にリグレットと呼ばずに
            後悔という語がそのまま使われることもありますが、これは過去の自分のアクションに対して、
            あのときああすれば良かったのにという後悔の度合いを定量化したものです。
          }

          +p{
            例えば再びじゃんけんを考えるとして、自分はグーを出し、相手にパーを出されて
            負けた（１点を失った）としましょう。
            このとき、もし自分もパーを出していれば引き分けとなって１点を失わずに済んだはず
            だったので、パーに対するリグレットは１点となります。
            もっと言えば、自分がチョキを出せていれば勝ちとなり、むしろ１点を得ることが出来ていたはず
            だったので、チョキに対するリグレットは差し引き２点となります。
            なお、もし逆に自分がパーを出し、相手はグーを出して勝負に勝った（１点を得た）場合、
            グーとチョキに対するリグレットはそれぞれ－１点と－２点になり、リグレットは負の値を
            取ることになります。
          }

          +p{
            さて、じゃんけんも何戦かこなすと、これまでのリグレットの総和を計算することで
            「過去にどのようなアクションを取るべきだったのか」がだんだんと見えてくることになるでしょう。
            このリグレットの総和（ただし総和が負の値になった場合はゼロとして扱う）に比例するように
            混合戦略を立てて将来的なリグレットを最小化しよう、という単純かつ強力な手法が
            \dfn{regret-matchingアルゴリズム}と呼ばれるものです。
            具体的には、例えばグー、チョキ、パーそれぞれに対する現在のリグレットの総和が
            ２点、－１点、８点であるとしましょう。
            このとき、負の値となっているチョキに対するリグレットの総和は０点として扱うこととし、
            ２点、０点、８点に比例するように、グー、チョキ、パーをそれぞれ20\%、0\%、80\%の確率で
            出す混合戦略を立てるということになります。
          }

          +p{
            このregret-matchingアルゴリズムを用いて\dfn{自己対戦}を繰り返し行うと、各時刻の
            混合戦略の平均
            \footnote{時刻が無限大のときの混合戦略そのものではない点に注意。}
            が\dfn{相関均衡}に収束することが知られています
            \footnote{S. Hart and A. Mas-Colell. \emph{A simple adaptive procedure leading to
            correlated equilibrium.} Econometrica, 68(5):1127–1150. 2000.}。
            なお、ここで相関均衡とかいう用語が突然出てきてしまいましたが、これはナッシュ均衡を
            一般化した解概念で、今回の二人じゃんけんの例ではナッシュ均衡と一致するので特に
            気にしないことにしましょう。
            ちなみに、シンプルでありながら均衡状態を近似的に必ず求めることができるという優れた
            アルゴリズムであるにも関わらず、脚注に示したように論文が出たのは2000年と結構最近の
            話だったりします。
          }

          +p{
            さて、リグレットおよびregret-matchingアルゴリズムについてはこのあたりで良いとして、
            \dfn{counterfactual regret}とは何なのかについても見ていきましょう。
            counterfactualという英単語は「事実に反する」「反事実的」という意味の形容詞で、
            それだけ聞くと「そもそも後悔(regret)とは事実に反することに対してなされるものなので、
            “counterfactual regret”というのは『馬から落馬』『頭痛が痛い』のようなものではないか」
            と思われるかもしれません。
            しかし、そうではなくて“counterfactual value”と呼ばれる値を利得として解釈したときの
            リグレットなので、“counterfactual regret”と呼ばれるわけですね。
          }

          +p{
            Counterfactual valueは、\dfn{展開型ゲーム}と呼ばれる種別のゲームにおいて定義できる値です。
            展開型ゲームとは、ゲームの状態を表現する頂点と、アクションによって遷移できることを示す
            有向辺からなる\dfn{ゲーム木}を用いた、グラフ形式で表現できるゲームのことです。
            グラフに馴染みのない方にはまた何のことやらという感じだと思いますが、このモデルにおいて
            重要なのは、\dfn{手番}が存在してプレイヤーが１人ずつアクションを起こすようにゲームが
            モデル化されているという点です。
            一方で、\dfn{標準型ゲーム}（あるいは\dfn{戦略型ゲーム}）には手番は存在せず、すべての
            プレイヤーが同時に戦略を決定する必要があります。
            これまでに見てきた男女の争いやじゃんけんの例はどちらも標準型ゲームです。
          }

          +p{
            なお、すべての標準型ゲームは展開型ゲームとして表現することもできます。
            具体的には、便宜上の手番を順番に割り振って、手番が後のプレイヤーはそれまでになされた
            アクションの区別がつかない、という設定にすれば良いだけです。
            例えばじゃんけんならば、あるプレイヤーがグーを出すことを予め決定しているものの、
            他のプレイヤーからはそれが観測できないというような状況に対応しています。
          }

          +p{
            さて、ここまで説明してようやくcounterfactual regretの説明に移ることができます。
            と言っても、数式を使わずに説明するのは無理がある内容なので、ちゃんとした説明は記号を
            導入してから行いますが、各プレイヤーの戦略の組${\sigma}が固定されているもとでの、
            プレイヤー${i}の手番であるような状態${h}への\dfn{counterfactual-到達確率}を、
            「プレイヤー${i}の戦略${\sigma_i}の寄与を無視した状態${h}への到達確率」として
            まず定義します。
            これがなぜ“counterfactual”であるかというと、状態${h}にたどり着くにはプレイヤー${i}は
            戦略${\sigma_i}に従って何らかの確率的選択を実際には行っていたかもしれないのに、
            「過去の意思決定において私は常に状態${h}を目指していました」と主張して到達確率を
            計算するのに相当するわけですから、これは確かに事実に反していますよね。
            このcounterfactual-到達確率に、状態${h}におけるプレイヤー${i}の利得の期待値
            \footnote{利得は本来はゲームの終端状態のみにおいて定まる値ですが、各プレイヤーの戦略が
            固定されているので、非終端状態においてもボトムアップに計算を行うことで利得の期待値が
            定まります。}
            を掛け合わせたものが\dfn{counterfactual value}になります。
            最後に、状態${h}における\dfn{counterfactual regret}は、利得の値を直接使う代わりに
            このcounterfactual valueを用いて計算されたリグレットのことです。
          }

          +p{
            ${\cdots\cdots}とまあ、文章だけでの説明を試みてみましたが、数式が無いと
            理解してもらうのは厳しい内容でしたね。
            後ほどちゃんと数式を用いて説明しなおすことにしましょう。
          }

          +p{
            このcounterfactual regretによるregret-matchingアルゴリズムを用いて自己対戦を行うと、
            各時刻の戦略の重み付き平均を取ることで展開型の二人ゼロサムゲームのナッシュ均衡を
            近似的に求めることができます
            \footnote{M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. \emph{Regret
            minimization in games with incomplete information.} In NIPS, pages 1729–1736. 2007.}
            \footnote{ナッシュ均衡の説明に引き続いてまたしても「二人ゼロサム」の条件が突然加えられて
            しまいましたが、今回も三人以上の設定でも「実用上それなりに強い」解を求められることが
            知られています。}。
            今回も説明が長くなってしまいましたが、これがCFR (Counterfactual Regret Minimization)
            アルゴリズムと呼ばれるものの正体で、ポーカーを含む不完全情報ゲームの近年の解析における
            基礎的なアルゴリズムの一つとなっています。
          }
        >
      >

      +section{数学的な定義} <
        +p{
          本記事のタイトルを構成するキーワードについての大雑把な説明が済んだところで、
          数学的な議論を行えるようにいよいよ記号を導入していきましょう。
          定義が突然大量に出てきて混乱してしまうかもしれませんが、どれも今後の議論で必要となる
          定義ですので、必要に応じてここに立ち返ってきてください。
        }

        +listing{
          * ${N \coloneq \set{0,\ 1,\ \cdots,\ n - 1}}:
            ${n}人のプレイヤー全体の集合。

          * ${c}:
            \dfn{偶然手番}。
            アクションを予め定められた確率に基づいて実行する仮想的なプレイヤー。
            サイコロを振る、カードをシャッフルする、といった行為はこの偶然手番が行っているものと
            見なします。

          * ${h \in H}:
            \dfn{履歴}${h}および履歴全体の集合${H}．
            履歴はゲーム木における頂点のことで、今まで「状態」と呼んでいたものに相当します。
            単に「状態」だと、例えば履歴は異なるが盤面などは同じという状況を同一視するようにも
            解釈できてしまうので、それらを区別することを明示するためにも「履歴」という語を用います。

          * ${Z \subseteq H}:
            \dfn{終端履歴}の集合。
            終端履歴${z \in Z}まで到達するとゲームは終了し、各プレイヤーに対する利得が定まります。

          * ${P: H \setminus Z \to N \cup \set{c}}:
            \dfn{手番関数}。
            ${\app{P}{h}}は非終端履歴${h \in H \setminus Z}において次にアクションを行うプレイヤーを表します。

          * ${\app{A}{h} \coloneq \set{a \mid \p{h,\ a} \in H}}:
            \dfn{アクション集合}。
            ${\app{A}{h}}は非終端履歴${h \in H \setminus Z}においてプレイヤー${\app{P}{h}}が
            行うことが可能なアクションの集合を表します。
            ここで、${\p{h,\ a}}は履歴${h}の後にアクション${a}を行ったときの履歴を表します。

          * ${u_i: Z \to \mathbb{R}}:
            プレイヤー${i}の\dfn{利得関数}。
            ${\app{u_i}{z}}は、終端履歴${z \in Z}における、プレイヤー${i}にとっての利得の実数値を表します。
            二人ゼロサムゲームの場合${\app{u_0}{z} = -\app{u_1}{z}}が成り立ちます。

          * ${I_i \in \mathcal{I}_i}:
            プレイヤー${i}に関する\dfn{情報集合}${I_i}および\dfn{情報分割}${\mathcal{I}_i}．
            なおプレイヤーに関して興味が無い場合は、添字の${i}を省略することがあります。
            プレイヤー${i}に関する情報集合${I_i}は、手番がプレイヤー${i}であるような履歴の集合の
            ことで、${h,\ h' \in I_i}のとき、つまり履歴${h}と${h'}がともに同じ情報集合${I_i}に
            属するとき、プレイヤー${i}からはこれらの履歴を区別することができないことを意味します。
            また、すべての履歴は必ずただ一つの情報集合に属します。
            さらに、手番関数${P}やアクション集合${A}について、${h,\ h' \in I}ならば
            ${\app{P}{h} = \app{P}{h'}}および${\app{A}{h} = \app{A}{h'}}が成り立つので、
            これらを${\app{P}{I},\ \app{A}{I}}と型を混同して記述することにします。

          * ${\sigma_i \in \Sigma_i}:
            プレイヤー${i}の\dfn{戦略}${\sigma_i}および戦略全体の集合${\Sigma_i}．
            戦略${\sigma_i}は、情報集合${I_i}を受け取って次に行うアクション候補の集合
            ${\app{A}{I_i}}上の確率分布を返す関数です。
            また、戦略${\sigma_i}のもとで情報集合${I_i}においてアクション${a}を行う確率を
            ${\app{\sigma_i}{I_i,\ a}}と表記します。
            なお、${\app{\sigma_i}{I_i}}は確率分布なので、
            ${\sum_{a \in \app{A}{I_i}} \app{\sigma_i}{I_i,\ a} = 1}が成り立ちます。
            さらに、各プレイヤーの戦略の組${\set{\sigma_0,\ \sigma_1,\ \cdots,\ \sigma_{n - 1}}}を
            ${\sigma}と表記し、${\sigma}から${\sigma_i}を除いたものを${\sigma_{-i}}と表記します。

          * ${\pi^\sigma: H \to \pB{0,\ 1}}:
            ${\app{\pi^\sigma}{h}}は、各プレイヤーが戦略の組${\sigma}に従ったときの履歴${h}への
            \dfn{到達確率}を表します。
            また、${\app{\pi_i^\sigma}{h}}を${\app{\pi^\sigma}{h}}におけるプレイヤー${i}の
            寄与として定めます。
            すなわち、${\app{\pi^\sigma}{h} = \prod_{i \in N \cup \set{c}} \app{\pi_i^\sigma}{h}}．
            さらに、${\app{\pi_{-i}^\sigma}{h}}を${\app{\pi^\sigma}{h}}からプレイヤー${i}の寄与
            ${\app{\pi_i^\sigma}{h}}を除外した到達確率として定めます。
            すなわち、${\app{\pi_{-i}^\sigma}{h} \coloneq \app{\pi^\sigma}{h}\ /\ \app{\pi_i^\sigma}{h}}．

          * ${\pi^\sigma: \mathcal{I} \to \pB{0,\ 1}}:
            ${\app{\pi^\sigma}{I}}は、各プレイヤーが戦略の組${\sigma}に従ったときの、情報集合${I}に
            含まれるいずれかの履歴への到達確率を表します。
            すなわち、${\app{\pi^\sigma}{I} \coloneq \sum_{h \in I} \app{\pi^\sigma}{h}}．
            また、${\app{\pi_i^\sigma}{I}}および${\app{\pi_{-i}^\sigma}{I}}もこれまでと同様に
            定義します。

          * ${\app{u^\sigma_i}{h} \coloneq \sum_{z \in Z} \app{\pi^\sigma}{h \to z} \app{u_i}{z}}:
            各プレイヤーが戦略の組${\sigma}に従ったときの、履歴${h}におけるプレイヤー${i}の
            \dfn{利得の期待値}。
            ここで、${\app{\pi^\sigma}{h \to z}}は履歴${h}に到達した状態から、各プレイヤーが
            戦略の組${\sigma}に従って履歴${z}に到達する確率を表します。
            また、${\app{u_i}{\sigma}}をゲーム開始時におけるプレイヤー${i}の利得の期待値として
            定義します。
            すなわち、${\emptyset}をゲーム開始時の履歴としたとき、
            ${\app{u_i}{\sigma} \coloneq \app{u^\sigma_i}{\emptyset}}．
        }

        +subsection{ナッシュ均衡の定義} <
          +p{
            記号をちゃんと定義したので、ナッシュ均衡についてまずは定義を確認してみましょう。
            戦略の組${\sigma^*}が\dfn{ナッシュ均衡}であるとは、すべてのプレイヤー${i}に対して

            \eqn?:(AZMathEquation.notag)(${
              \app{u_i}{\sigma^*} = \max_{\sigma_i \in \Sigma_i} \app{u_i}{\sigma_i,\ \sigma_{-i}^*}
            });

            が成り立つことを言います。
            前章での説明の再掲となりますが、「どのプレイヤーも、その他のプレイヤーがその
            ナッシュ均衡に従って戦略を選択している限りは、自分の戦略を変更することによって
            利得の期待値をこれ以上増やすことができないような戦略の組」であるということを
            数式で記述するとこのようになります。
          }

          +p{
            また、ナッシュ均衡の近似である\dfn{${\epsilon}-ナッシュ均衡}についても定義して
            しまうことにしましょう。
            戦略の組${\sigma^*}が\dfn{${\epsilon}-ナッシュ均衡}であるとは、すべてのプレイヤー${i}に
            対して

            \eqn?:(AZMathEquation.notag)(${
              \app{u_i}{\sigma^*} + \epsilon \geq \max_{\sigma_i \in \Sigma_i} \app{u_i}{\sigma_i,\ \sigma_{-i}^*}
            });

            が成り立つことを言います。
            つまり、こちらは自分の戦略を変更することでちょっと${\p{\leq \epsilon}}だけ利得の期待値を
            増やすことが可能なような戦略の組を許容しています。
            CFRアルゴリズムによって得られるのは正確にはこちらです。
          }

          +subsubsection{可搾取量} <
            +p{
              さて、CFRアルゴリズムによって得られるのは${\epsilon}-ナッシュ均衡の方であると
              述べましたが、アルゴリズムを実行して得られた計算結果に対応する${\epsilon}の値を
              実際に知るにはどうしたら良いでしょうか。
              その答えを教えてくれるのが\dfn{可搾取量(exploitability)}と呼ばれる値です。
            }

            +p{
              可搾取量を定義する前に、まず\dfn{最適反応戦略}を定義しましょう。
              プレイヤー${i}の最適反応戦略とは、プレイヤー${i}以外の戦略の組${\sigma_{-i}}が
              与えられたときに、次のように定められる戦略です：

              \eqn?:(AZMathEquation.notag)(${
                \app{b_i}{\sigma_{-i}} \coloneq \argmax_{\sigma_i \in \Sigma_i} \app{u_i}{\sigma_i,\ \sigma_{-i}}.
              });

              つまり、最適反応戦略とはその名の通り、${\sigma_{-i}}が定まっているもとで
              プレイヤー${i}の利得を最大化するような戦略のことです。
              最適反応戦略は複数存在する場合もありますが、そのような場合はどの戦略を取ってきても
              良いものとします。
              このように最適反応戦略を定義すると、戦略の組${\sigma^*}がナッシュ均衡であるとき、
              すべてのプレイヤー${i}に対して

              \eqn?:(AZMathEquation.notag)(${
                \sigma_i^* = \app{b_i}{\sigma_{-i}^*}
              });

              が成り立つように${\app{b_i}{\sigma_{-i}^*}}を取ることができます。
            }

            +p{
              続いて、戦略の組${\sigma}に対する\dfn{プレイヤー${i}の可搾取量${\app{e_i}{\sigma}}}を
              次のように定義します：

              \eqn?:(AZMathEquation.notag)(${
                \app{e_i}{\sigma} \coloneq \app{u_i}{\app{b_i}{\sigma_{-i}},\ \sigma_{-i}} - \app{u_i}{\sigma}.
              });

              つまり、プレイヤー${i}の可搾取量は、現在の戦略の組からプレイヤー${i}が自分だけ
              最適反応戦略に変更したときの利得の期待値の差を表します。
              この利得の期待値の差を、相手から搾取することができる量と考えて可搾取量と呼ぶわけです。
            }

            +p{
              最後に、戦略の組${\sigma}に対して定まる\dfn{可搾取量${\app{e}{\sigma}}}を次のように
              定義します：

              \eqn?:(AZMathEquation.notag)(${
                \app{e}{\sigma} \coloneq \sum_{i \in N} \app{e_i}{\sigma} = \sum_{i \in N} \app{u_i}{\app{b_i}{\sigma_{-i}},\ \sigma_{-i}}.
              });

              つまり、戦略の組${\sigma}の可搾取量は各プレイヤーの可搾取量の総和として定義されます。
              なお、右側の等号ではゲームがゼロサムであること、すなわち
              ${\sum_{i \in N} \app{u_i}{\sigma} = 0} であることを利用しています。
              このように可搾取量を定義すると、${\app{e}{\sigma} \leq \epsilon} のとき、戦略の組
              ${\sigma}は${\epsilon}-ナッシュ均衡の条件を満たします。
              また、戦略の組${\sigma^*}がナッシュ均衡のときは${\app{e}{\sigma^*} = 0}が成り立ちます。
            }
          >
        >

        +subsection{CFRアルゴリズムの定義} <
          +p{
            文章だけの説明ではいまいち良く分からなかったCFRアルゴリズム周りの定義についても引き続き
            確認していきましょう。
            先ほどの説明において、戦略${\sigma}が定まっているもとでの、プレイヤー${i}の手番で
            あるような履歴${h}への\dfn{counterfactual-到達確率}と呼んでいたものは、まさに先ほど
            定義した${\app{\pi_{-i}^\sigma}{h}}のことです。
            続いて、\dfn{counterfactual value}はこのcounterfactual-到達確率と利得の期待値の積のこと
            でしたから、次のように定義されます：

            \eqn?:(AZMathEquation.notag)(${
              \app{v_i}{\sigma,\ h} \coloneq \app{\pi_{-i}^\sigma}{h} \cdot \app{u^\sigma_i}{h}.
            });

            履歴${h}におけるアクション${a}に対する\dfn{counterfactual regret}は、
            次のように定義されます:

            \eqn?:(AZMathEquation.notag)(${
              \app{r_i}{h,\ a} \coloneq \app{v_i}{\sigma_{I \to a},\ h} - \app{v_i}{\sigma,\ h}.
            });

            ここで${\sigma_{I \to a}}は、履歴${h}を含むような情報集合${I}を受け取った場合のみ、
            必ずアクション${a}を取るように戦略${\sigma}を変更したものを表すこととします。
            さらに、情報集合${I}におけるアクション${a}に対する\dfn{counterfactual regret}を
            次のように定義します：

            \eqn?:(AZMathEquation.notag)(${
              \app{r_i}{I,\ a} \coloneq \sum_{h \in I} \app{r_i}{h,\ a}.
            });

            CFRアルゴリズムは自己対戦を繰り返し行うというものでしたから、時刻${t}における
            戦略${\sigma^t}のもとでのcounterfactual regretを${\app{r_i^t}{I,\ a}}で表すことにしましょう。
            このとき、counterfactual regretの累積値${\app{R_i^T}{I,\ a}}は単に次のように与えられます：

            \eqn?:(AZMathEquation.notag)(${
              \app{R_i^T}{I,\ a} \coloneq \sum_{t = 1}^{T} \app{r_i^t}{I,\ a}.
            });

            各時刻における戦略${\sigma^t}は、regret-matchingアルゴリズムによって、これまでの
            リグレットの総和に比例するように与えられることを思い出すと、各時刻における戦略は
            次のように表すことができます：

            \eqn?:(AZMathEquation.notag)(${
              \app{\sigma_i^{T + 1}}{I,\ a} \coloneq \cases{
                | \frac{\app{R_i^{T,+}}{I,\ a}}{\sum_{a' \in \app{A}{I}} \app{R_i^{T,+}}{I,\ a'}} | \text!{if} \sum_{a' \in \app{A}{I}} \app{R_i^{T,+}}{I,\ a'} > 0
                | \frac{1}{\pabs{\app{A}{I}}} | \text!{otherwise.}
              |}
            });

            なお、ここで${\app{R_i^{T,+}}{I,\ a} = \app{\max}{\app{R_i^T}{I,\ a},\ 0}}です。
            最後に、二人ゼロサムゲームにおいてナッシュ均衡に収束するのは、各時刻の戦略を
            ${\app{\pi_i^{\sigma^t}}{I}}で重み付けした場合の平均戦略です：

            \eqn?:(AZMathEquation.notag)(${
              \app{\bar{\sigma}_i^T}{I} \coloneq \frac
                {\sum_{t = 1}^T \p{\app{\pi_i^{\sigma^t}}{I} \cdot \app{\sigma_i^t}{I}}}
                {\sum_{t = 1}^T \app{\pi_i^{\sigma^t}}{I}}.
            });
          }

          +subsubsection{CFRアルゴリズムの改良} <
            +p{
              以上が「バニラ」なCFRアルゴリズムの定義ですが、収束を実用上早めるためにさまざまな
              変種が提案されてもいます。
              ここでは、実装が比較的簡単でかつ効果の高い\dfn{Discounted CFR (DCFR) アルゴリズム}
              \footnote{N. Brown and T. Sandholm. \emph{Solving imperfect-information games via
              discounted regret minimization.} In AAAI, pages 1829–1836. 2019.}
              を紹介することにしましょう。
            }

            +p{
              DCFRアルゴリズムは３つの実数パラメータ${\alpha,\ \beta,\ \gamma}を取り、先ほどの
              ${\app{R_i^T}{I,\ a}}と${\app{\bar{\sigma}_i^T}{I}}を次のように再定義します：

              \gather?:(AZMathEquation.notag)(${
                | \app{R_i^{T + 1}}{I,\ a} \coloneq \cases{
                  | \frac{T^\alpha}{T^\alpha + 1} \app{R_i^T}{I,\ a} + \app{r_i^{T + 1}}{I,\ a} | \text!{if}\ \app{R_i^T}{I,\ a} \geq 0
                  | \frac{T^\beta}{T^\beta + 1} \app{R_i^T}{I,\ a} + \app{r_i^{T + 1}}{I,\ a} | \text!{otherwise},
                |}
                | \app{\bar{\sigma}_i^T}{I} \coloneq \frac
                    {\sum_{t = 1}^T \p{t^\gamma \cdot \app{\pi_i^{\sigma^t}}{I} \cdot \app{\sigma_i^t}{I}}}
                    {\sum_{t = 1}^T \p{t^\gamma \cdot \app{\pi_i^{\sigma^t}}{I}}}.
              |});

              ${\p{\alpha,\ \beta,\ \gamma} = \p{\infty,\ \infty,\ 0}}のとき、バニラな
              CFRアルゴリズムと一致します
              （${T + 1 = 2}のときに変なことが起こりますがそれは無視することとして）。
              提案論文では、さまざまなパラメータを実験的に試した結果
              ${\p{\alpha,\ \beta,\ \gamma} = \p{1.5,\ 0,\ 2}}が一貫して良いパフォーマンスを
              示すとされており、今後の実装においてもこのパラメータを採用することとします
              （特に、${\p{\alpha,\ \beta,\ \gamma} = \p{\infty,\ -\infty,\ 1}}の場合に相当する
              \dfn{CFR+アルゴリズム}
              \footnote{O. Tammelin, N. Burch, M. Johanson, and M. Bowling. \emph{Solving heads-up
              limit texas hold'em.} In IJCAI, pages 645–652. 2015.}
              がそれまでのstate-of-the-artとされていたのですが、このCFR+に対して一貫してより
              優れた結果をもたらしています）。
            }

            +p{
              非常にアドホックな感がある改変ではありますが、なぜこれで上手くいくようになるのかを
              かなりざっくり説明すると、リグレットの累積値${\app{R_i^T}{I,\ a}}および平均戦略
              ${\app{\bar{\sigma}_i^T}{I}}の計算において、各時刻における値を均一に重み付けするの
              ではなく、後の時刻ほど重みが大きくなるように改変をしている点が本質です
              （最初の方の時刻の寄与が“discount”されているわけです）。
              時刻が後になるほど徐々に賢い戦略を獲得していくアルゴリズムですから、後の時刻ほど重みを
              大きく設定することで収束を早くできる、というのは直感的にも正しいように思いますが、
              いかがでしょうか。
            }
          >
        >
      >

      +section{CFRアルゴリズムの実装} <
        +p{
          さて、ようやく本章からは皆さんお待ちかねの実装パートです。
          プログラムの記述言語には実行速度と実装のエレガントさを重視して\dfn{Rust}を採用することと
          します。
          yabaitechを手に取っていただけるような方々にはRustもお茶の子さいさいでしょう、
          ${\cdots\cdots}というのは半分冗談にしても、いくつかの言語に触れたことがある方ならば
          プログラムを読むのにそれほど困りはしないと思います。
        }

        +p{
          これから紹介するプログラムは、\url(`https://github.com/b-inary/yabai-vol7-src`); に
          アクセスすることで完全な形を確認することもできます。
        }

        +subsection{ゲームのインターフェース定義} <
          +p{
            まずはCFRアルゴリズム本体の実装に移る前に、ゲームを表現するインターフェースを
            先に定義することにしましょう。
            CFRアルゴリズムをこのインターフェースのもとで実装することで、具体的なゲームを
            考える際にはインスタンス化を行うだけでCFRアルゴリズムを適用できるようになります。
          }

          +p{
            本記事では主にポーカーを念頭に置いているので、ゲームの構造を制限してポーカーに特化した
            最適化を行えるようにします。
            具体的には、手札を配るといったプライベートな情報を与えるイベントはゲームの最初にしか
            発生しないという条件を加えます。
            例えばテキサスホールデムを考えると、最初に全員に二枚の手札が配られた後はコミュニティ
            カードの公開も含めて全員が正しくすべてのアクションを把握できるという構造になっている
            ことが分かります。
            また、実装を簡単にするためにゲームは二人ゲームに限定することとし、各プレイヤーが持つ
            手札の構造は対称であるものとします。
          }

          +p{
            ゲームにこのような条件を加えるとどのような最適化が行えるかというと、本来ならば
            ゲームの最初に偶然手番がアクションを行うため、その分だけゲーム木も枝分かれするはず
            なのですが、これを飛ばしてパブリックな履歴のみを追跡し、終端履歴に到達したときに初めて
            最初に遡ってすべての手札の可能性をまとめて計算する、ということが行えるようになります。
            ただし、このような最適化を施すことで実装の読みやすさは犠牲になる部分があるので、
            読みづらいと思われる点はその都度解説していこうと思います。
          }

          +p{
            それでは具体的な実装に移っていきますが、まずはゲームのインスタンスが満たすべき
            振る舞いを定めるトレイト（インターフェース）を次のように定義します：
          }

          +code(```
            /// ゲームの定義を表すインターフェース
            pub trait Game {
                /// ゲーム木のノードを表す型
                type Node: GameNode;

                /// ゲーム木の根、すなわちゲームの初期履歴を返す
                fn root() -> Self::Node;

                /// プライベートな手札の組合せの個数を返す
                fn num_private_hands() -> usize;

                /// 終端履歴 `node` において、偶然手番の寄与を含まない counterfactual-到達確率が
                /// `pmi` のときの `player` の counterfactual value を計算する
                fn evaluate(&self, node: &Self::Node, player: usize, pmi: &Vec<f64>) -> Vec<f64>;
            }
          ```);

          +p{
            この `Game` トレイトは、そのインスタンスに対して
            `Node, root(), num_private_hands(), evaluate()` の４点の定義を要求していることが読み取れます。
            このうち、 `Node` は `GameNode` トレイトを満たす型で、その `GameNode` トレイトの
            詳細については後述します。
            また、`root()` および `num_private_hands()` は、それぞれゲームの定義に関わる関数です。
          }

          +p{
            最後の `evaluate()` については、いきなり結構な難所なので詳しめに説明します。
            まず、`evaluate()` は `&self` のほかに３つの引数 `node, player, pmi` を受け取っており、
            このメソッドは何をするのかというと、コメントにもあるように「終端履歴 `node` において、
            偶然手番の寄与を含まないcounterfactual-到達確率が `pmi` のときの、 `player` の
            counterfactual value を計算する」というものとなっています。
            ここで、counterfactual value はcounterfactual-到達確率と利得の期待値の積

            \eqn?:(AZMathEquation.notag)(${
              \app{v_i}{\sigma,\ h} \coloneq \app{\pi_{-i}^\sigma}{h} \cdot \app{u^\sigma_i}{h}
            });

            であったことを思い出すと、終端履歴においては利得の期待値は単に利得の値そのものと
            一致するので、このメソッドは `node` における `player` の利得を計算し、さらに引数として
            受け取った `pmi` を掛け合わせた結果を返すことを期待されていることが分かります。
          }

          +p{
            ここまでの説明で `evaluate()` の役割は何となく理解していただけたのではと思うのですが、
            関数の返り値と引数 `pmi` の型が実数の「配列」となっているのが、このメソッドの定義を
            理解する上でもう一つ厄介な点です。
            この点についても説明していくことにすると、これらの配列の要素数は `num_private_hands()`
            の値と一致しており、返り値の配列の各要素は `player` が特定の手札を持っているときの
            counterfactual value であることが期待されています。
            また、counterfactual-到達確率 `pmi` は、相手（正確には自分以外）のプレイヤーが持っている
            手札に応じて値が定まるものなので、引数の配列 `pmi` の各要素には相手のプレイヤーが特定の
            手札を持っているときの counterfactual-到達確率が入っています。
          }

          +p{
            引き続いて、各アクションを ${0,\ 1,\ \cdots,\ \pabs{\app{A}{h}} - 1} の整数に対応させて
            表現することとし、パブリックな履歴をアクションの配列で表現することにしましょう。
          }

          +code(```
            /// アクションを表す型
            pub type Action = usize;

            /// パブリックな履歴を表す型
            pub type PublicHistory = Vec<Action>;
          ```);

          +p{
            さらに、先ほど出現した、ゲーム木のノードを表現するインターフェースである `GameNode`
            トレイトを次のように定義します：
          }

          +code(```
            /// ゲーム木のノードを表すインターフェース
            pub trait GameNode {
                /// 現在のパブリックな履歴を返す
                fn public_history(&self) -> &PublicHistory;

                /// 現在のノードが終端履歴かどうかを返す
                fn is_terminal(&self) -> bool;

                /// 現在の手番のプレイヤーを返す
                fn current_player(&self) -> usize;

                /// 着手可能なアクションの個数を返す
                fn num_actions(&self) -> Action;

                /// 着手可能なアクションの一覧を返す
                fn actions(&self) -> std::ops::Range<Action> {
                    0..self.num_actions()
                }

                /// `action` を行った後のノードを返す
                fn play(&self, action: Action) -> Self;
            }
          ```);

          +p{
            この `GameNode` トレイトのメソッドシグネチャは素直なものばかりで、特に理解が難しい点は
            無いと思います。
          }
        >

        +subsection{CFRアルゴリズムの実装} <
          +p{
            それではゲームを表現するインターフェースが定まったところで、続いてCFRアルゴリズム本体の
            実装にいよいよ取り掛かっていきましょう。
            トップダウンに大枠から細部の順に実装を紹介していくことにします。
          }

          +subsubsection{CFRアルゴリズムを管理する構造体の定義} <
            +p{
              まずは、CFRアルゴリズムを管理する構造体の定義から見ていきましょう。
            }

            +code(```
              use std::collections::HashMap;

              pub struct CFRMinimizer<'a, T: Game> {
                  /// ゲーム定義のインスタンス
                  game: &'a T,

                  /// リグレットの累積値
                  cum_regret: HashMap<PublicHistory, Vec<Vec<f64>>>,

                  /// 各時刻の戦略の和
                  cum_strategy: HashMap<PublicHistory, Vec<Vec<f64>>>,

                  /// Discounted CFR のパラメータ
                  alpha_t: f64,

                  /// Discounted CFR のパラメータ
                  beta_t: f64,

                  /// Discounted CFR のパラメータ
                  gamma_t: f64,
              }
            ```);

            +p{
              Rustでは構造体の定義にはフィールドのみが含まれ、メソッドは `impl` ブロックに記述
              することになるので、とりあえずはこのフィールドたちを眺めることにしましょう。
              Rustの大きな特徴の一つでもある参照のライフタイムの管理に必要な注釈 `'a` が
              ちょっとうるさいですが、 まず `game` は `Game` トレイトを満たすインスタンスへの参照を
              保持するフィールドで、この `game` に対してCFRアルゴリズムが実行されます。
              後ろの３つのフィールド `alpha_t, beta_t, gamma_t` については非本質的なので省略します。
              残った `cum_regret` と `cum_strategy` についてはもう少し詳しい説明をしましょう。
            }

            +p{
              これら２つのフィールドの型は `HashMap<PublicHistory, Vec<Vec<f64>>>` となっています。
              それぞれリグレットの累積値と各時刻の戦略の和を管理している変数ですが、パブリックな
              履歴がハッシュマップのキーとなっているのは良いものの、それで返ってくる値が
              二次元配列となっているのはどういうことなのでしょうか。
              まず外側の添字は分かりやすくて、リグレットや戦略はアクションに対して定まる
              ものですから、単に各アクションを受け取ります。
              さて、それで残った方の配列ですが、これは先ほどと同じで、手番となっているプレイヤーが
              特定の手札を持っているときの値が要素となっています。
              つまり、`cum_regret[public_history][action][private_hand]` のようにアクセスすると
              値を引くことができます。
              `private_hand` が最後の添字となっているのが気持ち悪いかもしれませんが、このように
              することでプログラムを高速化することができます。
            }

            +p{
              さて、ここからは `impl` ブロック内の記述に移っていくことになります。
              これも非本質的ですが、定数 `ALPHA, BETA, GAMMA` とコンストラクタを今のうちに
              定めてしまうことにします：
            }

            +code(```
              impl<T: GameNode> CFRMinimizer<T> {
                  const ALPHA: f64 = 1.5;
                  const BETA: f64 = 0.0;
                  const GAMMA: f64 = 2.0;

                  /// コンストラクタ
                  pub fn new(game: &'a T) -> Self {
                      Self {
                          game,
                          cum_regret: HashMap::new(),
                          cum_strategy: HashMap::new(),
                          alpha_t: 1.0,
                          beta_t: 1.0,
                          gamma_t: 1.0,
                      }
                  }

                  ...
            ```);
          >

          +subsubsection{CFRアルゴリズムの枠組みの実装} <
            +p{
              続いて、CFRアルゴリズムを実行する本体となるメソッドを書いていきます。
              アルゴリズムの大枠は自己対戦を繰り返すというものですが、各時刻における
              複雑な処理の部分はとりあえず `cfr_recursive()` に任せてしまうことにすると、
              実装はおおよそ次のようになるでしょう：
            }

            +code(```
              /// CFRアルゴリズムによる学習を行い、平均戦略を返す
              pub fn compute(
                  &mut self,
                  num_iterations: i32,
              ) -> HashMap<PublicHistory, Vec<Vec<f64>>> {
                  // ゲームの初期履歴を取得
                  let root = T::root();

                  // ゲーム木を構築して累積値を0で初期化
                  Self::build_tree(&root, &mut self.cum_regret);
                  Self::build_tree(&root, &mut self.cum_strategy);

                  let ones = vec![1.0; T::num_private_hands()];

                  // 自己対戦を繰り返す
                  for t in 0..num_iterations {
                      let t_f64 = t as f64;
                      self.alpha_t = t_f64.powf(Self::ALPHA) / (t_f64.powf(Self::ALPHA) + 1.0);
                      self.beta_t = t_f64.powf(Self::BETA) / (t_f64.powf(Self::BETA) + 1.0);
                      self.gamma_t = (t_f64 + 1.0).powf(Self::GAMMA);

                      // プレイヤー毎に処理を行う
                      for player in 0..2 {
                          self.cfr_recursive(&root, player, &ones, &ones);
                      }
                  }

                  self.compute_average_strategy()
              }
            ```);

            +p{
              まず、`build_tree()` はゲーム木を構築して `cum_regret` および `cum_strategy` を
              初期化する関数で、次のような特に工夫の要らない再帰関数として定義されます：
            }

            +code(```
              /// ゲーム木を構築する
              fn build_tree(node: &T::Node, tree: &mut HashMap<PublicHistory, Vec<Vec<f64>>>) {
                  if node.is_terminal() {
                      return;
                  }

                  tree.insert(
                      node.public_history().clone(),
                      vec![vec![0.0; T::num_private_hands()]; node.num_actions()],
                  );

                  for action in node.actions() {
                      Self::build_tree(&node.play(action), tree);
                  }
              }
            ```);

            +p{
              さて、 `cum_regret` および `cum_strategy` を初期化したらいよいよ自己対戦を繰り返す
              ことになります。
              ここで、何となくさらっと流されやすい点ですが、各時刻において「プレイヤー毎に」
              関数 `cfr_recursive()` による処理を行っている点は注意が必要です。
            }

            +p{
              関数 `cfr_recursive()` の仕事は、後ほど詳しく見ていくことになりますが、 `cum_regret`
              および `cum_strategy` のうち、手番が `player` であるような部分の更新を行うことです。
              逆に言えば、これらのフィールドのうち、手番が `player` でないような部分は更新が
              行われません。
              これは一見するとおかしな実装で、バニラなCFRアルゴリズムはすべてのプレイヤーの
              リグレットや戦略を\dfn{同時更新(simultaneous update)}することを前提にしていた
              はずなのに、この実装ではリグレットや戦略がプレイヤー毎に
              \dfn{交互更新(alternating update)}されることになります。
              果たしてそれで大丈夫なのかと思われるかもしれませんが、実は実用上は交互更新の方が
              収束が早いことが知られており、そのため今回の実装でも交互更新をあえて採用しています。
              理論的にも、解析は難しくなるものの収束は引き続き保証されるようです
              \footnote{N. Burch, M. Moravcik, and M. Schmid. \emph{Revisiting CFR+ and alternating
              updates.} Journal of Artificial Intelligence Research, 64:429–443. 2019.}。
            }

            +p{
              最後に、`compute_average_strategy()` を呼び出して各時刻の重み付き平均戦略を返します。
              `compute_average_strategy()` も愚直に書き下せばよく、実装は次のようになるでしょう：
            }

            +code(```
              /// フィールド `cum_strategy` を参照して平均戦略を返す
              fn compute_average_strategy(&self) -> HashMap<PublicHistory, Vec<Vec<f64>>> {
                  let num_private_hands = T::num_private_hands();
                  let mut average_strategy = self.cum_strategy.clone();

                  for strategy in average_strategy.values_mut() {
                      let mut denom = vec![0.0; num_private_hands];
                      strategy.iter().for_each(|strategy_action| {
                          add_assign_vec(&mut denom, &strategy_action);
                      });

                      strategy.iter_mut().for_each(|strategy_action| {
                          div_assign_vec(strategy_action, &denom, 0.0);
                      });
                  }

                  average_strategy
              }
            ```);

            +p{
              ここで、 `add_assign_vec` といった関数がいくつか出現していますが、これは配列の
              各要素を加算代入したりする関数で、次のような定義を持ちます。
              処理の内容は何となく分かると思いますので、いちいち全部紹介はしないことにします。
            }

            +code(```
              fn add_assign_vec(lhs: &mut Vec<f64>, rhs: &Vec<f64>) {
                  lhs.iter_mut().zip(rhs).for_each(|(l, r)| *l += *r);
              }
            ```);
          >

          +subsubsection{Counterfactual value の再帰的な計算} <
            +p{
              それでは、先ほど後回しにしてしまった `cfr_recursive()` の実装に移っていきます。
              先述したように、 `cfr_recursive()` の仕事は `cum_regret` と `cum_strategy` の
              更新ですが、関数そのものの返り値は counterfactual value となっていて、関数名の通り
              これを再帰的に計算します。
            }

            +p{
              さすがにこの関数はそれなりの実装量になってしまうため、紙面で読むのはややタフかも
              しれませんが、頑張って見ていきましょう。
            }

            +code(```
              /// `player` の counterfactual value を再帰的に計算する
              fn cfr_recursive(
                  &mut self,
                  node: &T::Node,
                  player: usize,
                  pi: &Vec<f64>,
                  pmi: &Vec<f64>,
              ) -> Vec<f64> {
                  // 終端履歴なら単に counterfactual value を返す
                  if node.is_terminal() {
                      return self.game.evaluate(node, player, pmi);
                  }

                  // 現在のパブリックな履歴を取得
                  let public_history = node.public_history();

                  // 現時刻の戦略を regret-matching アルゴリズムによって求める
                  let mut strategy = Self::regret_matching(&self.cum_regret[public_history]);

                  // 返り値となる counterfactual value を0で初期化
                  let mut cfvalue = vec![0.0; T::num_private_hands()];

                  if node.current_player() == player {
                      let mut cfvalue_action_vec = Vec::with_capacity(node.num_actions());

                      // 各アクションに対する counterfactual value を計算する
                      for action in node.actions() {
                          let pi = mul_vec(&pi, &strategy[action]);
                          let mut cfvalue_action =
                              self.cfr_recursive(&node.play(action), player, &pi, pmi);
                          cfvalue_action_vec.push(cfvalue_action.clone());
                          mul_assign_vec(&mut cfvalue_action, &strategy[action]);
                          add_assign_vec(&mut cfvalue, &cfvalue_action);
                      }

                      // リグレットの累積値と戦略の和を更新
                      for action in node.actions() {
                          let cum_regret: &mut Vec<f64> =
                              &mut self.cum_regret.get_mut(public_history).unwrap()[action];
                          let cum_strategy: &mut Vec<f64> =
                              &mut self.cum_strategy.get_mut(public_history).unwrap()[action];

                          cum_regret.iter_mut().for_each(|el| {
                              *el *= if *el >= 0.0 {
                                  self.alpha_t
                              } else {
                                  self.beta_t
                              }
                          });

                          add_assign_vec(cum_regret, &cfvalue_action_vec[action]);
                          sub_assign_vec(cum_regret, &cfvalue);

                          mul_assign_scalar(&mut strategy[action], self.gamma_t);
                          mul_assign_vec(&mut strategy[action], &pi);
                          add_assign_vec(cum_strategy, &strategy[action]);
                      }
                  } else {
                      for action in node.actions() {
                          let pmi = mul_vec(&pmi, &strategy[action]);
                          add_assign_vec(
                              &mut cfvalue,
                              &self.cfr_recursive(&node.play(action), player, pi, &pmi),
                          );
                      }
                  }

                  cfvalue
              }
            ```);

            +p{
              まず、はじめに関数 `cfr_recursive` の引数についてですが、 `node` と `player` に
              ついては特に説明は要らないでしょう。
              `pi` と `pmi` は、現在の引数 `node` への到達確率のうち、 `player` の寄与と `player`
              以外の寄与を表しています。
              また、返り値は counterfactual value となっていますが、`pi, pmi` および返り値の型が
              実数の配列となっているのはどういうことかというと、例によってこれらの配列の各要素は
              `player` （`pmi` については相手のプレイヤー）が特定の手札を持っているときの値を
              表しています。
            }

            +p{
              この再帰関数の終了条件は、引数 `node` が終端履歴に達することです。
              終端履歴に達したら、ゲームの定義に基づいて counterfactual value を計算し、その値を
              返します。
              引数 `node` が終端履歴でなかった場合、再帰的に子ノードの計算を行うことになります。
            }

            +p{
              それでは引数 `node` が終端履歴でなかった場合の処理についてですが、まず現時刻における
              戦略が regret-matching アルゴリズムによって求められます。
              この regret-matching アルゴリズムの実装は簡単ですね：
            }

            +code(```
              /// regret-matching アルゴリズム
              fn regret_matching(regrets: &Vec<Vec<f64>>) -> Vec<Vec<f64>> {
                  let num_actions = regrets.len();
                  let num_private_hands = T::num_private_hands();
                  let mut strategy = regrets.clone();

                  let mut denom = vec![0.0; num_private_hands];
                  strategy.iter_mut().for_each(|strategy_action| {
                      nonneg_assign_vec(strategy_action);
                      add_assign_vec(&mut denom, strategy_action);
                  });

                  strategy.iter_mut().for_each(|strategy_action| {
                      div_assign_vec(strategy_action, &denom, 1.0 / num_actions as f64);
                  });

                  strategy
              }
            ```);

            +p{
              あとは、counterfactual value の計算を再帰的に行いながら、現在の手番が `player` で
              ある場合は `cum_regret` と `cum_strategy` も併せて適切に更新していくだけです。
              ${\cdots\cdots}と、文章で書くのは簡単ですが、足し算や掛け算などをバグ無く正確に
              記述するのは結構大変な作業です。
              行数で言ってしまえば60行程度と、それほど長いというわけではない関数ですが、
              デバッグも容易ではありませんし、実装の与えられていない状態から書き起こそうと思うと
              なかなか骨が折れるでしょう。
            }

            +p{
              以上でCFRアルゴリズムの実装は完成です。
              ここまで出来てしまえば、`Game` トレイトを満たすようにゲームの定義を行ってあげれば
              CFRアルゴリズムを簡単に適用することができます。
              次章以降では、ゲームのインスタンスをいくつか定義して、実際にCFRアルゴリズムを
              動かしていきます。
            }
          >
        >

        +subsection{ユーティリティ関数の実装} <
          +p{
            と、ゲームの各インスタンスを見に行く前に、得られた戦略を解析するユーティリティ関数も
            ついでに実装してしまいましょう。
            具体的には、戦略の組が与えられたときに利得の期待値を計算する関数と、可搾取量を計算する
            関数を実装していきます。
            CFRアルゴリズムの実行結果が早く見たいところかもしれませんが、もう少しだけお付き合い
            くださいね。
          }

          +subsubsection{利得の期待値の計算} <
            +p{
              まず利得の期待値を計算する関数の方から見ていきます。
              と言っても、再帰関数に抵抗が無い方ならば利得の期待値を計算する処理は素直に書けるでしょう。
              問題は `Game` トレイトの `evaluate()` メソッドが厄介な型をしているという点ですが、
              まず求める利得の期待値${\app{u_i}{\sigma}}は

              \eqn?:(AZMathEquation.notag)(${
                \app{u_i}{\sigma} = \sum_{z \in Z} \app{\pi^\sigma}{z} \cdot \app{u_i}{z}
              });

              であり、メソッド `evaluate()` は counterfactual value

              \eqn?:(AZMathEquation.notag)(${
                \app{v_i}{\sigma,\ z} = \app{\pi_{-i}^\sigma}{z} \cdot \app{u_i}{z}
              });

              を計算してくれるので、あとは
              ${\app{\pi^\sigma}{h} = \app{\pi_i^\sigma}{h} \cdot \app{\pi_{-i}^\sigma}{h}} で
              あるということを思い出せば、終端履歴における処理の内容も理解できると思います。
            }

            +code(```
              /// 戦略 `strategy` のもとでの `player` の利得の期待値を返す
              pub fn compute_ev<T: Game>(
                  game: &T,
                  player: usize,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> f64 {
                  let ones = vec![1.0; T::num_private_hands()];
                  compute_ev_rec(game, &T::root(), player, &ones, &ones, strategy)
              }

              /// 利得の期待値を再帰的に計算するヘルパー
              fn compute_ev_rec<T: Game>(
                  game: &T,
                  node: &T::Node,
                  player: usize,
                  pi: &Vec<f64>,
                  pmi: &Vec<f64>,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> f64 {
                  if node.is_terminal() {
                      // `dot` は内積を計算する関数
                      return dot(&game.evaluate(node, player, pmi), &pi);
                  }

                  let current_strategy = &strategy[node.public_history()];
                  if node.current_player() == player {
                      node.actions()
                          .map(|action| {
                              let pi = mul_vec(&current_strategy[action], &pi);
                              compute_ev_rec(game, &node.play(action), player, &pi, pmi, strategy)
                          })
                          .sum()
                  } else {
                      node.actions()
                          .map(|action| {
                              let pmi = mul_vec(&current_strategy[action], &pmi);
                              compute_ev_rec(game, &node.play(action), player, pi, &pmi, strategy)
                          })
                          .sum()
                  }
              }
            ```);
          >

          +subsubsection{可搾取量の計算} <
            +p{
              さて、残った方の可搾取量を計算する関数ですが、こちらはちょっとマジカルな感じもある
              実装となっています。
              実装は簡潔だが、なぜこれで正しく動くのかがなかなか分からないという類のやつですね。
              何はともあれ実装を先に見てみることにしましょう。
            }

            +code(```
              /// 戦略の組 `strategy` の可搾取量を返す
              pub fn compute_exploitability<T: Game>(
                  game: &T,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> f64 {
                  let ones = vec![1.0; T::num_private_hands()];
                  let br0 = best_cfvalues_rec(game, &T::root(), 0, &ones, strategy);
                  let br1 = best_cfvalues_rec(game, &T::root(), 1, &ones, strategy);
                  br0.iter().sum::<f64>() + br1.iter().sum::<f64>()
              }

              /// 最適反応戦略の counterfactual value を再帰的に計算するヘルパー
              fn best_cfvalues_rec<T: Game>(
                  game: &T,
                  node: &T::Node,
                  player: usize,
                  pmi: &Vec<f64>,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> Vec<f64> {
                  if node.is_terminal() {
                      return game.evaluate(node, player, pmi);
                  }

                  if node.current_player() == player {
                      node.actions()
                          .map(|action| {
                              best_cfvalues_rec(game, &node.play(action), player, pmi, strategy)
                          })
                          .reduce(|v, w| max_vec(&v, &w))
                  } else {
                      let current_strategy = &strategy[node.public_history()];
                      node.actions()
                          .map(|action| {
                              let pmi = mul_vec(&pmi, &current_strategy[action]);
                              best_cfvalues_rec(game, &node.play(action), player, &pmi, strategy)
                          })
                          .reduce(|v, w| add_vec(&v, &w))
                  }
                  .unwrap()
              }
            ```);

            +p{
              ヘルパー関数である `best_cfvalues_rec()` は、コメントにも書いてあるように、
              `player` が最適反応戦略を取ったときの counterfactual value を再帰的に求める関数です。
              実際、終端履歴に到達したら単に `evaluate()` を呼んでいるだけであることが分かります。
            }

            +p{
              さて、実はこの counterfactual value は利得の期待値への寄与とイコールに
              なっているのですが、ここで「最適反応戦略には必ず純粋戦略が含まれている」という
              重要な事実を利用しています。
              というのも、`player` 以外の戦略は固定されているものとしているのですから、
              自分が相手に搾取されてしまう可能性は考える必要がなく、従って単に利得の期待値が最も
              大きくなるようなアクションを常に選べば利得を最大化できるためです。
              よって、各履歴${h}における ${\app{\pi_i^\sigma}{h}} は0か1のどちらかであるとしてよく、
              ${\app{\pi_i^\sigma}{h}} を追跡したりする必要が無いのです。
            }

            +p{
              よって、ヘルパー関数 `best_cfvalues_rec()` は ${\app{\pi_{-i}^\sigma}{h}} だけを
              引数にとって追跡します。
              現在の `node` の手番が `player` であるなら、counterfactual value、すなわち利得の
              期待値への寄与が最大となるアクションを選択し、手番が `player` でないなら単に
              counterfactual value を加算します。
            }
          >
        >
      >

      +section{CFRの具体例1: Kuhn Poker} <
        +p {
          それでは、本章からはいくつかのゲームの定義を実装し、CFRアルゴリズムを実際に動かして
          いきます。
          まずは\dfn{Kuhn poker}と呼ばれる非常に単純化されたポーカーの変種を扱っていくことに
          しましょう。
        }

        +subsection{Kuhn Pokerのルール} <
          +p{
            Kuhn poker は２人向けのポーカーで、デッキは３枚のカードのみから構成されます。
            ここでは、キング(K)、クイーン(Q)、ジャック(J)の３枚を用いることとし、先に挙げた
            カードほど強いものとします。
          }

          +p{
            ここからはゲームの進行について説明していきますが、まず２人のプレイヤーは相手から
            見えないようにカードをデッキから１枚ずつ引き、残った１枚については伏せたままにしておきます。
            続いて、各プレイヤーは１点の\dfn{アンティ}（参加費）を場に供託します。
            どちらかのプレイヤーが\dfn{ベット}することでもう１点を追加で供託することもできますが、
            この手順については後ほど説明します。
            供託する点数について両者の合意が取れた場合、各プレイヤーは自分の持つカードを公開し、
            より強いカードを持っていた方がそれまでに供託されていた点数を総取りします
            （\dfn{ショーダウン}）。
          }

          +p{
            アンティを供託した後は、プレイヤーは先手と後手に分かれて、次のような手順を経て
            供託する点数についての合意を取ろうとします：
          }

          +listing{
            * 先手は手番をパスする（\dfn{チェック}）か、１点を追加で供託する（\dfn{ベット}）かを
              選択します。

            ** 先手がチェックした場合、後手は同様に手番をパスしてショーダウンに進む（チェック）か、
               １点を追加で供託する（ベット）かを選択します。

            *** 後手がベットした場合、先手はすでに供託した１点を相手に与えることにして勝負から降りる
                （\dfn{フォールド}）か、後手と同様に１点を追加で供託してショーダウンに進む
                （\dfn{コール}）かを選択します。

            ** 先手がベットした場合、後手はすでに供託した１点を相手に与えることにして勝負から降りる
               （フォールド）か、先手と同様に１点を追加で供託してショーダウンに進む（コール）かを
               選択します。
          }

          +p{
            以上が Kuhn poker のルールですが、非常に単純化されたポーカーであるとは述べたものの、
            最適な戦略がどのようなものであるかを思いつくのはそう簡単ではないでしょう。
            キングを持っていて相手にベットされた場合は常にコールする、またジャックを持っていて
            相手にベットされた場合は常にフォールドする、さらに後手がキングを持っていて先手が
            チェックした場合は常にベットする、くらいのことは分かるでしょうが、それ以上のことは
            少なくとも筆者にはまったく明らかではありません。
          }

          +p{
            この Kuhn poker は発案者の Harold W. Kuhn によって解析もなされており、ナッシュ均衡が
            数学的に求まっています。
            それぞれの具体的な最適戦略は次に挙げる通りですが、ここで先手は最適戦略を無限に
            持っており、パラメータ ${\alpha \in \pB{0,\ 1\ /\ 3}} を自由に選択することができます：
          }

          +p{
            【先手】
          }

          +listing{
            * 手札がキング: 確率 ${3 \alpha} でベットし、自分のチェックに対して後手がベットした場合は
              常にコール
            * 手札がクイーン: 常にチェックし、後手がベットした場合は ${\alpha + 1\ /\ 3} の確率で
              コール
            * 手札がジャック: 確率 ${\alpha} でベットし、自分のチェックに対して後手がベットした場合は
              常にフォールド
          }

          +p{
            【後手】
          }

          +listing{
            * 手札がキング: 常にベットかコール
            * 手札がクイーン: 先手がチェックした場合はチェック、先手がベットした場合は確率
              ${1\ /\ 3} でコール
            * 手札がジャック: 先手がチェックした場合は確率 ${1\ /\ 3} でベット、先手がベットした
              場合は常にフォールド
          }

          +p{
            なお、両プレイヤーがこのナッシュ均衡に基づく戦略を取った場合、後手が平均して
            ${1\ /\ 18} 点だけ勝つことが知られています。
          }

          +p{
            この最適戦略を眺めて個人的に面白いと思うのは、先手はパラメータ ${\alpha} の値にも
            よりますが、両プレイヤーとも最弱のカードであるジャックを持っているときにも一定の確率で
            ベットを行い、ブラフを仕掛けるのが最適とされている点です。
            特にポーカーの楽しさの大部分は相手がブラフをしているかどうかの心理的な読み合いに
            あると思うのですが、適切な確率でブラフを仕掛けることはナッシュ均衡の観点からも
            ちゃんと正当化されるのだということが、このように非常に単純化されたルールのもとでも
            確認することができます。
          }
        >

        +subsection{ゲーム定義の実装} <
          +p{
            Kuhn poker のルールについての説明が済んだので、続いてゲーム定義の実装に移って
            いきましょう。
            まず、Kuhn poker には特に設定を変更できる部分がありませんので、構造体 `KuhnGame` には
            空の定義を持たせます。
            `KuhnGame` のノードを表す型である `KuhnNode` はパブリックな履歴を保持すれば良いので、
            唯一のフィールド `public_history` を持ちます。
            また、ノードはクローンできると便利なので、 `KuhnNode` は `Clone` トレイトを
            継承しておきます。
          }

          +code(```
            pub struct KuhnGame {}

            #[derive(Clone)]
            pub struct KuhnNode {
                public_history: PublicHistory,
            }
          ```);

          +p{
            それでは `KuhnGame` の実装に移っていきますが、`Node, root(), num_private_hands()` の
            実装は明らかで、特に考える必要はないでしょう。
          }

          +code(```
            impl Game for KuhnGame {
                type Node = KuhnNode;

                fn root() -> KuhnNode {
                    KuhnNode {
                        public_history: Vec::new(),
                    }
                }

                fn num_private_hands() -> usize {
                    3
                }

                fn evaluate(&self, node: &KuhnNode, player: usize, pmi: &Vec<f64>) -> Vec<f64> {
                    ...
                }
            }
          ```);

          +p{
            ここで、やはり問題となるのが `evaluate()` の実装です。
            `evaluate()` がどのようなメソッドだったかを思い出すと、「終端履歴 `node` において、
            偶然手番の寄与を含まない counterfactual-到達確率が `pmi` のときの、 `player` の
            counterfactual value を計算する」というものでした。
            また、 counterfactual value は終端履歴においてはcounterfactual-到達確率と利得の積で
            与えられるのでしたね。
          }

          +p{
            一つ注意したいのは、返り値となる counterfactual value の計算においては偶然手番に
            よる寄与を含むcounterfactual-到達確率の値を用いるべきですが、引数 `pmi` には偶然手番に
            よる寄与が含まれていないという点です。
            つまり、偶然手番によってどのような確率でどのようなハンドが各プレイヤーに与えられるのかは、
            この `evaluate()` メソッド内で定義を与えなければならないということです。
            `evaluate()` を `GameNode` ではなく `Game` トレイトのメソッドとしている理由は
            このためでもありますが、今回の Kuhn poker の例では両プレイヤーへのカードの配り方は
            全部で６通りあり、それらが等確率に発生するということをこの `evaluate()` メソッドに
            組み込む必要があります。
          }

          +p{
            以上の考察を経ると、利得を計算する関数 `payoff()` を用いて `evaluate()` が次のように
            書き下せることが分かるでしょう：
          }

          +code(```
            fn evaluate(&self, node: &KuhnNode, player: usize, pmi: &Vec<f64>) -> Vec<f64> {
                let mut cfvalue = vec![0.0; Self::num_private_hands()];

                for my_card in 0..Self::num_private_hands() {
                    for opp_card in 0..Self::num_private_hands() {
                        if my_card == opp_card {
                            continue;
                        }
                        cfvalue[my_card] +=
                            Self::payoff(node, player, my_card, opp_card) * pmi[opp_card] / 6.0;
                    }
                }

                cfvalue
            }
          ```);

          +p{
            さらに、以下の実装を加えて構造体 `KuhnGame` の定義を完成させます：
          }

          +code(```
            const CHECK_FOLD: usize = 0;
            const BET_CALL: usize = 1;

            impl KuhnGame {
                /// コンストラクタ
                pub fn new() -> Self {
                    Self {}
                }

                fn payoff(
                    node: &KuhnNode,
                    player: usize,
                    my_card: usize,
                    opp_card: usize,
                ) -> f64 {
                    match (node.public_history.as_slice(), node.public_history.last()) {
                        ([CHECK_FOLD, CHECK_FOLD], _) if my_card > opp_card => 1.0,
                        ([CHECK_FOLD, CHECK_FOLD], _) => -1.0,
                        (_, Some(&CHECK_FOLD)) if node.current_player() == player => 1.0,
                        (_, Some(&CHECK_FOLD)) => -1.0,
                        _ if my_card > opp_card => 2.0,
                        _ => -2.0,
                    }
                }
            }
          ```);

          +p{
            最後に、ゲーム木のノードを表す型である `KuhnNode` の実装を以下のように与えます。
            こちらも特に難しい点は無く、素直に定義を書き下しているだけであることが分かると思います。
          }

          +code(```
            impl GameNode for KuhnNode {
                fn public_history(&self) -> &PublicHistory {
                    &self.public_history
                }

                fn is_terminal(&self) -> bool {
                    match self.public_history.as_slice() {
                        [CHECK_FOLD, BET_CALL] => false,
                        [_, _] => true,
                        [_, _, _] => true,
                        _ => false,
                    }
                }

                fn current_player(&self) -> usize {
                    self.public_history.len() % 2
                }

                fn num_actions(&self) -> usize {
                    2
                }

                fn play(&self, action: Action) -> Self {
                    let mut ret = self.clone();
                    ret.public_history.push(action);
                    ret
                }
            }
          ```);
        >

        +subsection{プログラムの実行} <
          +p{
            ここまで実装パートが長かったですが、いよいよプログラムを実際に実行していきましょう。
            `main()` 関数を次のように記述することで、これまでに作ってきたコンポーネントや関数を
            簡単に組み合わせて実行することができます：
          }

          +code(```
            fn main() {
                // CFRの反復回数を指定
                let num_iterations = 10000;

                // Kuhn poker のゲーム定義のインスタンスを作成
                let kuhn_game = KuhnGame::new();

                // CFRを管理する構造体のインスタンスを作成
                let mut cfr = CFRMinimizer::new(&kuhn_game);

                // CFRを実行して最適戦略を得る
                let strategy = cfr.compute(num_iterations);

                // 利得の期待値と可搾取量を計算
                let ev = compute_ev(&kuhn_game, 0, &strategy);
                let exploitability = compute_exploitability(&kuhn_game, &strategy);

                ...
          ```);

          +p{
            さて、今回はCFRの反復回数を10000回に指定しましたが、これで正しい結果はちゃんと
            得られるのでしょうか。
            出力を適当に整形すると、手元のマシンでは0.1秒も掛からずに次のような出力が得られました：
          }

          +console(```
            - Exploitability: +4.774e-5

            [First player]
            - EV: -0.0556
            - Bet%                          - Call% (Check => Bet => ?)
                J: 18.90%                       J: 0.00%
                Q: 0.00%                        Q: 52.25%
                K: 56.72%                       K: 100.00%

            [Second player]
            - EV: +0.0556
            - Bet% (Check => ?)             - Call% (Bet => ?)
                J: 33.33%                       J: 0.00%
                Q: 0.00%                        Q: 33.33%
                K: 100.00%                      K: 100.00%
          ```);

          +p{
            まず、可搾取量は${4.8 \times 10^{-5}}となっており、実用上は十分収束していることが
            分かります。
            これだけ収束していれば、得られた戦略もほぼナッシュ均衡となっているはずで、数学的に
            求まっている最適戦略と実際に比較してみると${\alpha \approx 18.91\%}のときの最適戦略と
            ほぼ一致していることが見て取れます。
            先手および後手の利得の期待値もちゃんと${\mp 1\ /\ 18}になっていますね。
          }

          +p{
            ここまで理論も実装もそれなりに重たかったかと思いますが、実際にプログラムが動いたときの
            喜びもその分大きいものです。
            完全情報ゲームであれば、例えば三目並べのような比較的単純なゲームの最適戦略を求めたければ
            単なる深さ優先探索をするだけですが、不完全情報ゲームは、この Kuhn poker のように例え
            非常に単純化されたルールのものであっても、プログラムによって最適戦略を求めようと思うと
            これだけの準備が必要となってしまうのです。
            逆に言えば、不完全情報ゲームの魅力や奥の深さはこのような性質によるところが大きいとも
            言えるでしょう。
          }
        >
      >

      +section{CFRの具体例2: Push/Fold Hold'em} <
        +subsection{Push/Fold Hold'em のルール} <
        >

        +subsection{ゲーム定義の実装} <
        >

        +subsection{プログラムの実行} <
        >
      >

      +section{まとめ} <
      >
    >
  >

end
