@require: class-yabaitech/yabaitech
@require: azmath/azmath
@require: easytable/easytable
@require: latexcmds/latexcmds

@import: lib
@import: ../lib/code

module Binary : sig

  val article : block-text

end = struct

  open BinaryCommands
  open Code2
  open EasyTableAlias

  let vop-scheme charf s =
    let mop = charf MathOp s in
      math-pull-in-scripts MathOp MathOp
        (fun moptS moptT -> (
          let m =
            match moptS with
            | None     -> mop
            | Some(mS) -> math-lower mop mS
          in
            match moptT with
            | None     -> m
            | Some(mT) -> math-upper m mT
        ))

  let vop = vop-scheme math-char

  let-math \argmax = vop `arg max`

  let-math \empty = ${}

  let-math \choose m n = ${\empty_{#m}\mathrm{C}_{#n}}

  let article = '<
    +chapter(|
      bibliography = [];
      title = {不完全情報ゲームのナッシュ均衡を CFR (Counterfactual Regret Minimization) アルゴリズムで求めよう};
      title-for-toc = Option.none;
      subtitle = Option.none;
      author = {ばいなり};
    |)
    <
      +section{はじめに} <
        +p{
          yabaitech.tokyo vol.7 を手に取ってくださりありがとうございます。
          前号の vol.6 では、世界的に見て最もポピュラーなポーカーの変種と言える
          \dfn{テキサスホールデム}の役判定を高速に行う話を書かせていただきましたが、
          今回も引き続いてポーカーを念頭に置いた記事を投稿したいと思います。
        }

        +p{
          具体的には、ポーカーを含む\dfn{不完全情報ゲーム}において\dfn{最適解}とも言える
          \dfn{ナッシュ均衡}を\dfn{CFRアルゴリズム}によって求めるプログラムの実装を目指します。
          もちろん、実際にナッシュ均衡が現実的な時間内に求まる（より正確には計算結果が十分収束する）
          かどうかはゲームの複雑さによるため、複雑性の非常に高いテキサスホールデムの完全解析を
          行おうと思うと本記事の内容ではまったく不可能ですが、それほど複雑でないゲームであれば
          最適解をプログラムによって求めることができるようになります。
        }

        +p{
          ゲーム理論周りにあまり馴染みの無い方には、タイトルには仰々しい単語しか並んどらんし
          ナッシュ均衡とか言われてもさっぱり分からんと思われてしまっているかもしれません。
          ただ、そういった方にもせめて雰囲気は伝わるような記事となるよう心掛けたので、
          ややタフな内容かもしれませんが各々の楽しみ方で読み進めていただけると幸いです\footnote{
            と言うよりむしろ、筆者も特にゲーム理論の専門家でも何でもなくて、ゲーム理論を齧っただけの
            素人なりに初学者に興味を持ってもらうことも目的の一つにして記事を書きました。
          }。
        }
      >

      +section{用語の説明} <
        +p{
          本章では、本記事のタイトルを構成する主なキーワードとなっている「不完全情報ゲーム」
          「ナッシュ均衡」「CFRアルゴリズム」の三点について、それぞれ説明をしていきます。
        }

        +subsection{不完全情報ゲーム} <
          +p{
            まず、タイトルの最初の単語である\dfn{不完全情報ゲーム}とは何ぞや？
            \ というところから見ていきましょう。
            簡単に言ってしまえば、ボードゲームなどのゲームにおいて、
            各々のプレイヤーがすべての情報にアクセスできるならばそのゲームは\dfn{完全情報ゲーム}、
            そうでないなら\dfn{不完全情報ゲーム}であると呼ばれます。
          }

          +p{
            具体的な例を挙げると、\dfn{オセロ}や\dfn{将棋}といったゲームはお互いのプレイヤーが
            同じ盤面の情報（より正確には棋譜の情報）を共有していて、特定のプレイヤーからしか
            アクセスできないような情報は存在しないため、完全情報ゲームに分類されます。
            逆に、\dfn{ポーカー}や\dfn{麻雀}といったゲームは各々のプレイヤーが自分にしか
            分からない手札を持っていて、各プレイヤーがアクセスできる情報は同じではないため、
            不完全情報ゲームに分類されます。
            さらに、\dfn{じゃんけん}といった複数のプレイヤーが同時にアクションを起こす必要のある
            ゲームも不完全情報ゲームです。
            \dfn{バックギャモン}や\dfn{すごろく}といったゲームはどちらに分類すべきか微妙なところで、
            盤面がプレイヤーにすべて公開されているという観点では完全情報ゲームと言えますが、
            サイコロなどによるランダム性の解釈次第で不完全情報ゲームに分類されることもあります。
            これについては不確定完全情報ゲームという分類の仕方もあり、そちらの方がより正確とも
            言えますが、本記事ではこれらは不完全情報ゲームとして扱うことにしておきます。
          }

          +p{
            完全情報ゲームでは、盤面が与えられると神の視点からはすでに勝敗（または引き分け）が
            定まっており、例えば ${6 \times 6} マスのオセロは初期盤面からお互いが最善を尽くすと
            後手が４石差で勝つことが知られています。
            この完全情報ゲームの攻略に関しては、完全読み切りが極めて困難であるような複雑なゲームに
            対しても、\dfn{AlphaGo}や\dfn{AlphaZero}といったプログラムがディープラーニングと
            強化学習を組み合わせた\dfn{深層強化学習}を用いて、盤面評価の精度の面でブレークスルーを
            最近引き起こしたことをご存じの方も多いでしょう。
          }

          +p{
            不完全情報ゲームは、完全情報ゲームが持つこのような性質を持っていません。
            すべてのプレイヤーがそれぞれに与えられた情報をもとに「最善手」を取り続けても、さまざまな
            要因によって勝敗は変化しますし、そもそも「最善手」というのが特定の一手に定まらずに複数の
            候補から確率的にアクションを選択するべき状況となることも少なくありません。
            また不完全情報ゲームにおいては、評価が可能なのは戦略であってアクションではないという点も
            非常に重要です。
            つまり、例えばじゃんけんにおいて「グーとチョキとパーを等確率で出す戦略」の良し悪しを
            評価することはできますが、「実際にグーを出した」というアクションを単体で評価することは
            できないということです。
            この性質がプログラムによる精度の高い盤面評価を難しいものにしています。
          }
        >

        +subsection{ナッシュ均衡} <
          +p{
            不完全情報ゲームについて説明したところで、次のキーワードである\dfn{ナッシュ均衡}に
            ついても説明していきましょう。
            ナッシュ均衡とは、ざっくり言ってしまえば「どのプレイヤーも利得（の期待値）を
            これ以上増やすことができない状態」のことを指します。
            はじめにナッシュ均衡のことを「最適解」と呼びましたが、具体的にはこのような意味だった
            わけですね。
          }

          +p{
            厳密な定義については記号などをちゃんと導入した後で再度確認しますが、もう少し正確に
            文章で記述すると「どのプレイヤーも、その他のプレイヤーがそのナッシュ均衡に従って戦略を
            選択している限りは、自分の戦略を変更することによって利得（の期待値）をこれ以上
            増やすことができないような戦略の組」のことをナッシュ均衡と言います。
          }

          +subsubsection{男女の争い} <
            +p{
              簡単な例として\dfn{男女の争い}\footnote{
                ポリティカルコレクトネス的に不適切な感のある名称・設定ですが、一般的に使用されている
                名称なのでその点についてはご容赦ください。
                ちなみに英語では“Battle of the sexes”となりますが、調べてみたところやはり
                ポリティカルコレクトネスに配慮して“Bach or Stravinsky”ゲームと名称を変えている
                文献も少なくないようです。
              }
              と呼ばれるゲームを見てみましょう。
              このゲームはある男女の組がデートに行こうとしており、一緒に行動したいという点では
              同意しているものの、お互いが行きたい場所が異なるという状況をモデル化したものです。
              ここでは男性はスポーツ観戦に、女性は映画鑑賞に行きたいということにしておきましょう。
              このとき、それぞれ男性と女性が得る利得は次のように定められるものとします：
            }

            +centering{
              \easytable?:[t; b; m 1; v 1][c; c; c] {
                | 男性 ＼ 女性 | スポーツ観戦 | 映画鑑賞
                | スポーツ観戦 | (2, 1) | (0, 0)
                | 映画鑑賞 | (0, 0) | (1, 2)
              |}
            }

            +p{
              つまり、男性側は女性とともにスポーツ観戦に行った場合に最も利得を多く得ますが（２点）、
              次いで利得を多く得られるのは女性とともに映画鑑賞に行った場合で（１点）、
              一緒に行動できなかった場合は利得を得ることができません（０点）。
            }

            +p{
              このような設定下におけるナッシュ均衡は、当たり前のようですが
              （男性 ${\to} スポーツ鑑賞, 女性 ${\to} スポーツ鑑賞）と
              （男性 ${\to} 映画鑑賞, 女性 ${\to} 映画鑑賞）の二組となります
              \footnote{後述する\dfn{混合戦略}を考慮すると本当はこの二組だけではないのですが、
              それに関してはまた後ほど。}。
              男性側にとっては女性とともに映画鑑賞に行くというのは次善策なわけですが、女性側が
              映画鑑賞を選択している場合は、男性側の利得を最大化するためには男性側も映画鑑賞を
              選択する必要があるため、これが均衡の一つとなります。
            }

            +p{
              この単純な例からナッシュ均衡の重要な性質を二点見出すことができます。
              一つは、一般にナッシュ均衡は必ずしも一組とは限らないということです。
              もう一つは、ナッシュ均衡はすべてのプレイヤーの戦略の組として与えられるため、
              ある一人のプレイヤーにとっての「相手の戦略によらない良い戦略」を与えてくれるような
              ものではないということです。
            }
          >

          +subsubsection{二人ゼロサムゲームとしてのじゃんけん} <
            +p{
              ところで、本記事では主にポーカーの戦略を考えたいわけですから、ゲームの条件として
              各プレイヤーの利得を足し合わせるとゼロになる、すなわちゲームが\dfn{ゼロサム}である
              ことも追加しましょう。
              先ほどの男女の争いの例はゼロサムではありませんでしたので、ゼロサムであるゲームとして
              \dfn{じゃんけん}\footnote{
                知人が豪語していたじゃんけんの「必勝法」を紹介しましょう。曰く、
                「『最初はグー』の状態から遷移に最も時間が掛かるのはパーに切り替える場合だ。
                相手がパーに切り替えようとしているかどうかを見極めるには、相手の小指に注目すれば良い。
                よって、基本はこちらはグーのままだが、相手の小指が動いたらこちらはチョキに切り替えろ。
                グーからチョキに切り替えるのは、パーに切り替えるよりは素早く行えるから、
                優れた動体視力と反射神経があれば間に合わせることができる」。
                全員がこの「必勝法」を採用するとグーを出し続ける無限ループに陥ることになるので、
                そもそも「必勝法」の定義を満たしていないように筆者的には思うのですが、
                額面通りに事を運ばせられるのならば確かにまあ負けることは無いので、
                動体視力と反射神経に自信のある方は試してみると良いのではないでしょうか。
              }
              を考えてみることにしましょう。
              特に解説は要らないと思いますが、じゃんけんの利得表は次のようになります：
            }

            +centering{
              \easytable?:[t; b; m 1; v 1][c; c; c; c] {
                | | グー | チョキ | パー
                | グー | (0, 0) | (1, -1) | (-1, 1)
                | チョキ | (-1, 1) | (0, 0) | (1, -1)
                | パー | (1, -1) | (-1, 1) | (0, 0)
              |}
            }

            +p{
              このようなゲームにおいてもナッシュ均衡は存在するのでしょうか？
              \ 例えばプレイヤーAがグーを出した場合を想定すると、プレイヤーBはパーを出すのが最善で、
              そうするとプレイヤーAはチョキを出すのが最善で${\cdots\cdots}と循環してしまい、お互いが
              妥協できる均衡状態は一見して存在せず、よってナッシュ均衡も存在しないように思えます。
            }

            +p{
              ここで登場する概念が\dfn{混合戦略}というものです。
              混合戦略とは、例えばグーを50\%、チョキを30\%、パーを20\%で出すといったように、
              確率的にアクションを決定する戦略のことです。
              なおこれに対して、例えば決まってグーを出すといったように確率によらない戦略は
              \dfn{純粋戦略}と呼ばれ、男女の争いの例では純粋戦略のみによるナッシュ均衡を紹介
              しました。
              今回のじゃんけんの例では、三種類の手をそれぞれ等確率に出す混合戦略を両者が採用する、
              というのがこのゲームにおける唯一のナッシュ均衡となります。
            }

            +p{
              このナッシュ均衡の解釈は一筋縄ではいかないものです。
              三種類の手をそれぞれ等確率に出す混合戦略というのは、必ずグーを出すような非常に
              弱い相手に対しても利得の期待値がゼロにしかならない戦略です。
              ナッシュ均衡とはどのプレイヤーも利得をこれ以上増やすことができない状態のことで、
              言わば「最適解」であると説明されたはずなのに、そのような混合戦略を両者ともに
              取ることがそのナッシュ均衡なんです、と言われても納得できない方もいると思います。
              この事態を直感的に捉えられるようにするには、少なくとも筆者にとっては
              次のような言い換えを経る必要がありました：
            }

            +with-param(AZMathEquation.vmargin-between-eqn)(fun ctx -> get-font-size ctx) <
              +align?:(AZMathEquation.notag)(${
                |\text!{（} |\text!{\dfn{二人ゼロサムゲーム}において一人がナッシュ均衡から外れたと仮定）}
                |           |\text!{ナッシュ均衡ではどのプレイヤーも利得をこれ以上増やすことができない}
                |\Rightarrow|\text!{ナッシュ均衡から外れたプレイヤーは利得が増えることはない}
                |\Rightarrow|\text!{\dfn{ゼロサム性}よりナッシュ均衡の戦略を取り続けたプレイヤーは利得が減ることはない}
                |\Rightarrow|\text!{\emph{ナッシュ均衡の戦略を取り続けることで利得の最低値が保証される}}
              |});
            >

            +p{
              つまり、\dfn{二人ゼロサムゲーム}においては、ナッシュ均衡の戦略を取ることは
              「相手の戦略によらず保証される利得を最大化できる」という意味で「最適解」なのであり、
              「相手の特定の戦略に対して利得を最大化できる」というようなものではないのです。
              なお後者のように特定の戦略を狙い撃ちするような戦略は\dfn{搾取戦略}と呼ばれ、
              例えば必ずグーを出す相手に対して必ずパーを出すといった戦略はこれに該当します。
              ただし、このように必ずパーを出すという戦略は、必ずチョキを出すという
              \dfn{対抗搾取戦略}に搾取されてしまいます。
              ナッシュ均衡とは、言わばすべてのプレイヤーがお互いを最大限搾取している理想的な状態で、
              達人どうしが戦ったらこうなりますという状態なわけですから、必ずパーを出すというような
              簡単に搾取されてしまう戦略は、その理想からはかけ離れてしまっているわけですね。
            }

            +p{
              このような考察を経ると、二人ゼロサムゲームにおいてはナッシュ均衡はまさに
              \dfn{「最善手」}の組であると言えることが分かります。
              ナッシュ均衡はあくまで戦略の組ですから、男女の争いの例を思い出していただくと
              分かるように、ナッシュ均衡に基づく戦略は一般のゲームにおいては「相手の戦略によらない
              良い戦略」であるとは限らなかったわけです。
              ところが、二人ゼロサムゲームにおいてはナッシュ均衡に基づく戦略を取ることで
              「相手の戦略によらず利得の最低値が保証される」わけですから、これはまさしく「最善手」で
              あると言って良いでしょう。
            }

            +p{
              なお、今回のじゃんけんの例で得られた「最善手」は三種類の手をそれぞれ等確率に出すと
              いうもので、これはどのような相手に対しても利得がゼロになってしまうというものでしたが、
              これはどちらかと言うと具体例が悪いという類のものです。
              よくあるポーカーの状況などでは、こちらがナッシュ均衡に基づく最善手を取っていて
              相手が最善手から離れているという場合、相手も最善手を取っている場合と比べて
              得られる利得は増加することがほとんどです。
            }
          >

          +subsubsection{補足} <
            +p{
              ところで、二人ゼロサムゲームにおけるナッシュ均衡の嬉しい性質については納得したが、
              三人以上が関与するゼロサムゲームではどうなのか、という点が気になっている方も
              いらっしゃるかもしれません。
              特に、テキサスホールデムなどは通常三人以上でテーブルを囲みますから、ポーカーを念頭に
              置いている本記事ではこの話題に触れておく必要があるでしょう。
              この点については、ふわっとした結論として「ナッシュ均衡に従うのは実用上はそれなりに
              強い場合が多いが、理論的な保証は無くなってしまう」と言うくらいが実は精々です。
            }

            +p{
              ここで理論的な保証が無くなってしまうというのは、再び男女の争いの例と同じく
              ナッシュ均衡に基づく戦略が「相手の戦略によらない良い戦略」とは限らないという意味です。
              これは三人ゼロサムゲームは二人非ゼロサムゲームの一般化である（ゼロサムとなるように
              ダミープレイヤーを一人追加すれば良い）ことを考えると分かるかもしれません。
              ただし、理論的な保証は失われると言っても、実用上はナッシュ均衡に従ってプレイすると
              それなりに強いことが多く、三人以上が関与するゲームにおいてもナッシュ均衡を求めることが
              実際にはよく目標とされています。
            }

            +p{
              また、これは理論寄りの話題となりますが、ほとんどすべての有限ゲームにはナッシュ均衡が
              奇数個存在することが知られています。
              じゃんけんについてはナッシュ均衡は一つだったので良いとして、男女の争いについては
              中途半端に二つしか見つけられていませんが、どういうことなのでしょうか。
              実は、これは混合戦略を考慮していなかったことが原因で、混合戦略までちゃんと考えると
              （男性 ${\to} ${2\ /\ 3} の確率でスポーツ観戦,
                女性 ${\to} ${2\ /\ 3} の確率で映画鑑賞）
              という戦略の組もナッシュ均衡となります。
            }

            +p{
              ここまで説明がやや長くなってしまいましたが、以上がナッシュ均衡についての説明でした。
              本記事ではナッシュ均衡を求めるプログラムをこれから語っていくことになるので、
              「ナッシュ均衡を求めることにどのような意味があるのか」についてあやふやなままだと
              記事としての説得力に欠けるかなあということで詳しめに説明してみましたが、
              いかがでしたでしょうか。
            }
          >
        >

        +subsection{CFR (Counterfactual Regret Minimization) アルゴリズム} <
          +p{
            さて、ナッシュ均衡を乗り越えたと思ったらまたしても仰々しいキーワードが出てきて
            しまいましたね。
            CFRアルゴリズムとは、一言で言えば「展開型の二人ゼロサムゲームにおいてナッシュ均衡に収束
            することが保証されているアルゴリズム」なのですが、それがどのようなアルゴリズムなのか、
            まずは数式を使わずに文章で雰囲気を掴んでいただければと思います。
          }

          +subsubsection{標準型ゲームと展開型ゲーム} <
            +p{
              CFRアルゴリズムの説明に入る前に、まずはゲームの分類についての話をしましょう。
              これまでに見てきた男女の争いやじゃんけんは、各プレイヤーが同時にアクションを起こすと
              それぞれの利得が定まるという構造になっており、このようなゲームは\dfn{標準型ゲーム}
              （または\dfn{戦略型ゲーム}）と呼ばれます。
              これに対して、テキサスホールデムのように\dfn{手番}が存在するゲームは\dfn{展開型ゲーム}
              と呼ばれます。
              展開型ゲームは、ゲームの状態を表現する頂点およびアクションによって遷移できることを示す
              有向辺からなる\dfn{ゲーム木}を用いて、グラフ形式で数学的に表現されます。
              また不完全情報ゲームにおいては、実際には異なる状態だがあるプレイヤーからはその区別が
              つかないという状況がよく発生するでしょう。
              このような状況に対応するために、展開型ゲームでは各プレイヤーが区別できない状態について
              定義する\dfn{情報分割}も与えられます。
            }

            +p{
              なお、すべての標準型ゲームは展開型ゲームとして表現することもできます。
              具体的には、便宜上の手番を順番に割り振って、手番が後のプレイヤーはそれまでになされた
              アクションの区別がつかない、という設定にすれば良いだけです。
              例えばじゃんけんならば、あるプレイヤーがグーを出すことを予め決定しているものの、
              他のプレイヤーからはそれが観測できないというような状況に対応しています。
            }
          >

          +subsubsection{リグレットとregret-matchingアルゴリズム} <
            +p{
              それでは、続いてキーワード中の\dfn{リグレット(regret)}の部分の説明をしていきましょう。
              リグレットとは直訳すると\dfn{後悔}のことで、日本語訳する際にリグレットと呼ばずに
              後悔という語がそのまま使われることもありますが、これは過去の自分のアクションに対して
              「あのときああすれば良かったのに」という後悔の度合いを定量化したものです。
            }

            +p{
              標準型ゲームであるじゃんけんを再び例にとって考えます。
              自分はグーを出し、相手にパーを出されて負けた（１点を失った）としましょう。
              このとき、もし自分もパーを出していれば引き分けとなって１点を失わずに済んだはず
              だったので、パーに対するリグレットは１点となります。
              もっと言えば、自分がチョキを出せていれば勝ちとなり、むしろ１点を得ることが出来ていた
              はずだったので、チョキに対するリグレットは差し引き２点となります。
              なお、もし逆に自分がパーを出し、相手はグーを出して勝負に勝った（１点を得た）場合、
              グーとチョキに対するリグレットはそれぞれ－１点と－２点になり、リグレットは負の値を
              取ることになります。
            }

            +p{
              さて、じゃんけんも何戦かこなすと、これまでのリグレットの総和を計算することで「過去に
              どのようなアクションを取るべきだったのか」がだんだんと見えてくることになるでしょう。
              このリグレットの総和（ただし総和が負の値になった場合はゼロとして扱う）に比例するように
              混合戦略を立てて将来的なリグレットを最小化しよう、という単純かつ強力な手法が
              \dfn{regret-matchingアルゴリズム}と呼ばれるものです。
              具体的には、例えばグー、チョキ、パーそれぞれに対する現在のリグレットの総和が
              ２点、－１点、８点であるとしましょう。
              このとき、負の値となっているチョキに対するリグレットの総和は０点として扱うこととし、
              ２点、０点、８点に比例するように、グー、チョキ、パーをそれぞれ20\%、0\%、80\%の確率で
              出す混合戦略を立てるということになります。
            }

            +p{
              このregret-matchingアルゴリズムを用いて\dfn{自己対戦}を繰り返し行うと、各時刻の
              混合戦略の平均\footnote{
                時刻が無限大のときの混合戦略そのものではない点に注意。
              }が\dfn{相関均衡}に収束することが知られています\footnote{
                S. Hart and A. Mas-Colell. \emph{A simple adaptive procedure leading to correlated
                equilibrium.} Econometrica, 68(5):1127–1150. 2000.
              }。
              なお、ここで相関均衡とかいう用語が突然出てきてしまいましたが、これはナッシュ均衡を
              一般化した概念で、今回の二人じゃんけんの例ではナッシュ均衡と一致するので特に
              気にしないことにしましょう。
              この枠組みを展開型ゲームに拡張したのが続いて見ていくCFRアルゴリズムになります。
            }
          >

          +subsubsection{Counterfactual regret} <
            +p{
              さて、ここまで説明してようやく本題の\dfn{CFR (Counterfactual Regret Minimization)
              アルゴリズム}の説明に移ることができます。
              と言っても実はすでに説明の大半は終わっていて、先ほどのregret-matchingアルゴリズムに
              おいて\dfn{counterfactual regret}と呼ばれるリグレットを用いるというだけなのですが、
              このcounterfactual regretが曲者なので、その定義に必要なcounterfactual到達確率および
              counterfactual valueと併せて軽く説明します。
            }

            +p{
              ここでの目標は、「事実に反する」「反事実的」という意味の“counterfactual”という語が
              なぜ用いられているのかを説明することに留めます。
              というのも、counterfactual regret をちゃんと理解しようと思うと数式を避けて通れない
              ためで、正確な定義は次章を参照してください。
            }

            +p{
              まず、各プレイヤーの戦略が固定されているものとします。
              このとき、アクションの\dfn{履歴}${h}に対するプレイヤー${i}の
              \dfn{counterfactual到達確率}を「プレイヤー${i}の戦略の寄与を無視して計算された
              アクションの履歴${h}の実現確率」として定義します。
              つまり、プレイヤー${i}が行ってきた確率的選択はcounterfactual到達確率には一切影響せず、
              プレイヤー${i}以外の寄与のみを取り出して計算するということです。
              このように事実に反した計算を行うので、“counterfactual”であると呼ばれるわけですね。
            }

            +p{
              いま、利得の値は本来はゲームの終了状態においてのみ定まる値ですが、各プレイヤーの戦略が
              固定されているので、現在の履歴${h}におけるプレイヤー${i}の利得の期待値をボトムアップに
              計算することができます。
              この利得の期待値とcounterfactual到達確率の積を\dfn{counterfactual value}と呼びます。
              最後に、現在の履歴${h}における\dfn{counterfactual regret}とは、利得の値を直接使う
              代わりにこのcounterfactual valueを用いて計算されたリグレットのことです。
            }

            +p{
              このcounterfactual regretによるregret-matchingアルゴリズムを用いて自己対戦を繰り返し
              行うと、展開型の二人ゼロサムゲームにおいては各時刻の戦略の重み付き平均がナッシュ均衡に
              収束します\footnote{
                M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. \emph{Regret minimization in
                games with incomplete information.} In NIPS, pages 1729–1736. 2007.
              }\footnote{
                ナッシュ均衡の説明に引き続いてまたしても「二人ゼロサム」の条件が加えられて
                しまいましたが、今回も三人以上の設定でも多くの場合は「実用上それなりに強い」解を
                求められることが知られています。
              }。
              今回も説明が長くなってしまいましたが、これがCFRアルゴリズムと呼ばれるものの正体で、
              ポーカーを含む不完全情報ゲームの近年の解析における基礎的なアルゴリズムの一つと
              なっています。
            }
          >

          +subsubsection{補足：ナッシュ均衡を厳密に求めるアルゴリズム} <
            +p{
              さて、ここまでのCFRアルゴリズムの説明をよく読むと、ナッシュ均衡に「収束」するという
              表現が用いられていて、ナッシュ均衡解が「求まる」とは表現されていませんでした。
              これは、CFRアルゴリズムは\dfn{反復解法}であるため、数学的な意味でナッシュ均衡を厳密に
              満たす戦略の組を有限の時間で求められるようなアルゴリズムではないためです。
              なお、反復解法とは定められた処理を繰り返すことで求めたい解の近似を徐々に改善していく
              手法のことです。
            }

            +p{
              ナッシュ均衡を\dfn{解析的}に求めたい、すなわち数学的にナッシュ均衡を厳密に満たす
              戦略の組を求めたいという場合は、\dfn{直接解法}を用いる必要があります。
              実際、二人ゼロサムゲームならば\dfn{線形計画法}によって多項式時間でナッシュ均衡解を
              厳密に求めることができます\footnote{
                ただし、展開型ゲームにおいては\dfn{完全記憶(perfect recall)}という性質がゲームに
                備わっている必要があります。
                完全記憶でない展開型ゲームのナッシュ均衡を求めるのはNP困難です。
              }。
              二人非ゼロサムゲームおよびその一般化の三人ゼロサムゲームにおいては、多項式時間の
              アルゴリズムが存在するかどうかは未解決問題で、もしそのようなアルゴリズムが存在した
              場合は計算量理論において大きな意味を持つようです\footnote{
                PPADという計算量クラスがあるようで、これらの問題はPPAD完全であるとのことです。
              }（ので存在しなそうということになります）。
            }

            +p{
              それならば、二人ゼロサムゲームに対してはCFRアルゴリズムを使わずに線形計画法で解いて
              しまえば良いのではと思われるかもしれませんが、実際には線形計画法は多項式時間で
              動作すると言っても解ける問題の大きさは限られています。
              CFRアルゴリズムを本記事で紹介しているのはそのためで、実用上はこちらの方がより
              大きい問題に対応することができます。
            }
          >
        >
      >

      +section{数学的な定義} <
        +p{
          本記事のタイトルを構成するキーワードについての大雑把な説明が済んだところで、
          数学的な議論を行えるようにいよいよ記号を導入していきましょう。
          定義が突然大量に出てきて混乱してしまうかもしれませんが、どれも今後の議論で必要となる
          定義ですので、必要に応じてここに立ち返ってきてください。
        }

        +listing{
          * ${N \coloneq \set{0,\ 1,\ \cdots,\ n - 1}}:
            ${n}人のプレイヤー全体の集合。

          * ${c}:
            \dfn{偶然手番}。
            アクションを予め定められた確率に基づいて実行する仮想的なプレイヤー。
            サイコロを振る、カードをシャッフルする、といった行為はこの偶然手番が行っているものと
            見なします。

          * ${h \in H}:
            \dfn{履歴}${h}および履歴全体の集合${H}．
            履歴とはこれまでになされたすべてのアクションの列のことで、ゲーム木における頂点に
            対応します。
            盤面などが同じであっても履歴が異なる場合はそれらを区別するということを明示するためにも、
            ゲームの「状態」などとは呼ばずに「履歴」という語を用います。

          * ${Z \subseteq H}:
            \dfn{終端履歴}の集合。
            終端履歴${z \in Z}まで到達するとゲームは終了し、各プレイヤーに対する利得が定まります。

          * ${P: H \setminus Z \to N \cup \set{c}}:
            \dfn{手番関数}。
            ${\app{P}{h}}は非終端履歴${h \in H \setminus Z}において次にアクションを行うプレイヤーを
            表します。

          * ${\app{A}{h} \coloneq \set{a \mid \p{h,\ a} \in H}}:
            \dfn{アクション集合}。
            ${\app{A}{h}}は非終端履歴${h \in H \setminus Z}においてプレイヤー${\app{P}{h}}が
            行うことが可能なアクションの集合を表します。
            ここで、${\p{h,\ a}}は履歴${h}の後にアクション${a}を行ったときの履歴を表します。

          * ${u_i: Z \to \mathbb{R}}:
            プレイヤー${i}の\dfn{利得関数}。
            ${\app{u_i}{z}}は、終端履歴${z \in Z}における、プレイヤー${i}にとっての利得の実数値を
            表します。
            二人ゼロサムゲームの場合${\app{u_0}{z} = -\app{u_1}{z}}が成り立ちます。

          * ${I_i \in \mathcal{I}_i}:
            プレイヤー${i}に関する\dfn{情報集合}${I_i}および\dfn{情報分割}${\mathcal{I}_i}．
            なおプレイヤーに関して興味が無い場合は、添字の${i}を省略することがあります。
            プレイヤー${i}に関する情報集合${I_i}は、手番がプレイヤー${i}であるような履歴の集合の
            ことで、${h,\ h' \in I_i}のとき、つまり履歴${h}と${h'}がともに同じ情報集合${I_i}に
            属するとき、プレイヤー${i}からはこれらの履歴を区別することができないことを意味します。
            また、すべての履歴は必ずただ一つの情報集合に属します。
            さらに、手番関数${P}やアクション集合${A}について、${h,\ h' \in I}ならば
            ${\app{P}{h} = \app{P}{h'}}および${\app{A}{h} = \app{A}{h'}}が成り立つので、
            これらを${\app{P}{I},\ \app{A}{I}}と型を混同して記述することにします。

          * ${\sigma_i \in \Sigma_i}:
            プレイヤー${i}の\dfn{戦略}${\sigma_i}および戦略全体の集合${\Sigma_i}．
            戦略${\sigma_i}は、情報集合${I_i}を受け取って次に行うアクション候補の集合
            ${\app{A}{I_i}}上の確率分布を返す関数です。
            また、戦略${\sigma_i}のもとで情報集合${I_i}においてアクション${a}を行う確率を
            ${\app{\sigma_i}{I_i,\ a}}と表記します。
            なお、${\app{\sigma_i}{I_i}}は確率分布なので
            ${\sum_{a \in \app{A}{I_i}} \app{\sigma_i}{I_i,\ a} = 1}が成り立ちます。
            さらに、各プレイヤーの戦略の組${\set{\sigma_0,\ \sigma_1,\ \cdots,\ \sigma_{n - 1}}}を
            ${\sigma}と表記し、${\sigma}から${\sigma_i}を除いたものを${\sigma_{-i}}と表記します。

          * ${\pi^\sigma: H \to \pB{0,\ 1}}:
            ${\app{\pi^\sigma}{h}}は、各プレイヤーが戦略の組${\sigma}に従ったときの履歴${h}への
            \dfn{到達確率}を表します。
            また、${\app{\pi_i^\sigma}{h}}を${\app{\pi^\sigma}{h}}におけるプレイヤー${i}の
            寄与として定めます。
            すなわち、${\app{\pi^\sigma}{h} = \prod_{i \in N \cup \set{c}} \app{\pi_i^\sigma}{h}}．
            さらに、${\app{\pi_{-i}^\sigma}{h}}を${\app{\pi^\sigma}{h}}からプレイヤー${i}の寄与
            ${\app{\pi_i^\sigma}{h}}を除外した到達確率として定めます。
            すなわち、${\app{\pi_{-i}^\sigma}{h} \coloneq \app{\pi^\sigma}{h}\ /\ \app{\pi_i^\sigma}{h}}．

          * ${\pi^\sigma: \mathcal{I} \to \pB{0,\ 1}}:
            ${\app{\pi^\sigma}{I}}は、各プレイヤーが戦略の組${\sigma}に従ったときの、情報集合${I}に
            含まれるいずれかの履歴への到達確率を表します。
            すなわち、${\app{\pi^\sigma}{I} \coloneq \sum_{h \in I} \app{\pi^\sigma}{h}}．
            また、${\app{\pi_i^\sigma}{I}}および${\app{\pi_{-i}^\sigma}{I}}もこれまでと同様に
            定義します。

          * ${\app{u^\sigma_i}{h} \coloneq \sum_{z \in Z} \app{\pi^\sigma}{h \to z} \app{u_i}{z}}:
            各プレイヤーが戦略の組${\sigma}に従ったときの、履歴${h}におけるプレイヤー${i}の
            \dfn{利得の期待値}。
            ここで、${\app{\pi^\sigma}{h \to z}}は履歴${h}に到達した状態から、各プレイヤーが
            戦略の組${\sigma}に従って履歴${z}に到達する確率を表します。
            また、${\app{u_i}{\sigma}}をゲーム開始時におけるプレイヤー${i}の利得の期待値として
            定義します。
            すなわち、${\emptyset}をゲーム開始時の履歴としたとき、
            ${\app{u_i}{\sigma} \coloneq \app{u^\sigma_i}{\emptyset}}．
        }

        +subsection{ナッシュ均衡の定義} <
          +p{
            記号をちゃんと定義したので、ナッシュ均衡についてまずは定義を確認してみましょう。
            前章では、ナッシュ均衡を「どのプレイヤーも、その他のプレイヤーがそのナッシュ均衡に従って
            戦略を選択している限りは、自分の戦略を変更することによって利得の期待値をこれ以上増やす
            ことができないような戦略の組」であると説明しました。
            これを数式を用いて記述すると、戦略の組${\sigma^*}が\dfn{ナッシュ均衡}であるとは、
            すべてのプレイヤー${i}に対して

            \eqn?:(AZMathEquation.notag)(${
              \app{u_i}{\sigma^*} = \max_{\sigma_i \in \Sigma_i} \app{u_i}{\sigma_i,\ \sigma_{-i}^*}
            });

            が成り立つことであると表現できます。
            ここで、${\app{u_i}{\sigma_i,\ \sigma_{-i}^*}}は
            ${\app{u_i}{\set{\sigma_i} \cup \sigma_{-i}^*}}の略記とします。
          }

          +p{
            また、ナッシュ均衡を緩和した\dfn{${\mathbf{\epsilon}}-ナッシュ均衡}を導入します。
            前章で説明したようにCFRアルゴリズムは反復解法であり、ナッシュ均衡を厳密に満たす
            戦略の組を有限の時間で求められるものではないため、ナッシュ均衡の近似という概念を
            正確に定義したいためです。
            ${\epsilon}-ナッシュ均衡を以下のように定義することで、これまで「ナッシュ均衡に収束する」
            とぼんやり表現していたものが、「${\epsilon}が0に収束する」という意味に正確に定まります。
          }

          +p{
            それでは${\epsilon}-ナッシュ均衡の定義ですが、${\epsilon \geq 0}に対して、戦略の組
            ${\sigma^*}が\dfn{${\mathbf{\epsilon}}-ナッシュ均衡}であるとは、すべてのプレイヤー${i}に
            対して

            \eqn?:(AZMathEquation.notag)(${
              \app{u_i}{\sigma^*} + \epsilon \geq \max_{\sigma_i \in \Sigma_i} \app{u_i}{\sigma_i,\ \sigma_{-i}^*}
            });

            が成り立つことを言います。
            つまり、こちらは自分の戦略を変更することでちょっと${\p{\leq \epsilon}}だけ利得の期待値を
            増やせる余地が残っているような戦略の組を許容しています。
            ${\epsilon}が小さいほどより良いナッシュ均衡の近似であると言え、特に${\epsilon = 0}の
            ときはナッシュ均衡と一致します。
          }

          +subsubsection{可搾取量} <
            +p{
              さて、それでは実際にCFRアルゴリズムを実行して得られた戦略の組に対応する${\epsilon}の
              値を見積もるにはどうしたら良いでしょうか。
              その上界を与えてくれるのが\dfn{可搾取量(exploitability)}と呼ばれる値です。
            }

            +p{
              可搾取量を定義する前に、まず\dfn{最適反応戦略}を定義しましょう。
              プレイヤー${i}以外の戦略の組${\sigma_{-i}}が与えられたときのプレイヤー${i}の
              最適反応戦略${\app{b_i}{\sigma_{-i}}}は次のように定められます：

              \eqn?:(AZMathEquation.notag)(${
                \app{b_i}{\sigma_{-i}} \coloneq \argmax_{\sigma_i \in \Sigma_i} \app{u_i}{\sigma_i,\ \sigma_{-i}}.
              });

              つまり、最適反応戦略とはその名の通り、${\sigma_{-i}}が定まっているもとで
              プレイヤー${i}の利得を最大化するような戦略のことです。
              最適反応戦略は複数存在する場合もありますが、そのような場合はどの戦略を取ってきても
              良いものとします。
              このように最適反応戦略を定義すると、戦略の組${\sigma^*}がナッシュ均衡であるとき、
              すべてのプレイヤー${i}に対して

              \eqn?:(AZMathEquation.notag)(${
                \sigma_i^* = \app{b_i}{\sigma_{-i}^*}
              });

              が成り立つように${\app{b_i}{\sigma_{-i}^*}}を取ることができます。
            }

            +p{
              ここで、ゲームを二人ゲームに再び限定することにします。
              このとき、プレイヤー${i}の戦略${\sigma_i}の\dfn{可搾取量}${\app{\epsilon_i}{\sigma_i}}
              を次のように定義します：

              \eqn?:(AZMathEquation.notag)(${
                \app{\epsilon_i}{\sigma_i} \coloneq \app{u_i}{\sigma^*} - \app{u_i}{\sigma_i,\ \app{b_{-i}}{\sigma_i}}.
              });

              なお、ここで${\sigma^*}はナッシュ均衡を満たす戦略の組とします。
              つまり戦略${\sigma_i}の可搾取量とは、こちらの戦略が相手に筒抜けで常に最適反応戦略を
              取られてしまうような状況において、こちらがナッシュ均衡に基づく最適戦略を取っていた
              場合と比べて、現在の戦略${\sigma_i}が相手にどれだけ搾取されてしまうかを表す値の
              ことです。
            }

            +p{
              最後に、戦略の組${\sigma}に対して定まる\dfn{可搾取量}${\app{\epsilon}{\sigma}}を
              次のように定義します：

              \eqn?:(AZMathEquation.notag)(${
                \app{\epsilon}{\sigma} \coloneq \app{\epsilon_0}{\sigma_0} + \app{\epsilon_1}{\sigma_1} = \app{u_0}{\app{b_0}{\sigma_1},\ \sigma_1} + \app{u_1}{\sigma_0,\ \app{b_1}{\sigma_0}}.
              });

              つまり、戦略の組${\sigma}の可搾取量は二人ゲームにおいては両プレイヤーの戦略の
              可搾取量の和として定義されます。
              なお、右側の等号ではゼロサム性より${\app{u_0}{\sigma} = -\app{u_1}{\sigma}}が
              成り立つことを利用しており、特に${\app{u_i}{\sigma^*}}の値は一般には未知ですが
              キャンセルされます。
              特に証明は行いませんが、このように可搾取量を定義すると
              ${\app{\epsilon}{\sigma} \leq \epsilon}のとき、戦略の組${\sigma}は少なくとも
              ${\epsilon}-ナッシュ均衡であると言えます。
              特に、戦略の組${\sigma^*}がナッシュ均衡のときは${\app{\epsilon}{\sigma^*} = 0}が
              成り立ちます。
            }
          >
        >

        +subsection{CFRアルゴリズムの定義} <
          +p{
            文章だけの説明ではいまいち良く分からなかったCFRアルゴリズム周りの定義についても引き続き
            確認していきましょう。
            前章の説明において\dfn{counterfactual到達確率}と呼んでいたものは、まさに先ほど定義した
            ${\app{\pi_{-i}^\sigma}{h}}のことです。
            続いて、\dfn{counterfactual value}はこのcounterfactual到達確率と利得の期待値の積のこと
            でしたから、次のように定義されます：

            \eqn?:(AZMathEquation.notag)(${
              \app{v_i}{\sigma,\ h} \coloneq \app{\pi_{-i}^\sigma}{h} \cdot \app{u^\sigma_i}{h}.
            });

            履歴${h}におけるアクション${a}に対する\dfn{counterfactual regret}は、
            次のように定義されます:

            \eqn?:(AZMathEquation.notag)(${
              \app{r_i}{h,\ a} \coloneq \app{v_i}{\sigma_{I \to a},\ h} - \app{v_i}{\sigma,\ h}.
            });

            ここで${\sigma_{I \to a}}は、履歴${h}を含むような情報集合${I}を受け取った場合のみ、
            必ずアクション${a}を取るように戦略${\sigma}を変更したものを表すこととします。
            さらに、情報集合${I}におけるアクション${a}に対する\dfn{counterfactual regret}を
            次のように定義します：

            \eqn?:(AZMathEquation.notag)(${
              \app{r_i}{I,\ a} \coloneq \sum_{h \in I} \app{r_i}{h,\ a}.
            });

            CFRアルゴリズムは自己対戦を繰り返し行うというものでしたから、時刻${t}における戦略
            ${\sigma^t}のもとでのcounterfactual regretを${\app{r_i^t}{I,\ a}}で表すことにしましょう。
            このとき、counterfactual regretの累積値${\app{R_i^T}{I,\ a}}は単に次のように与えられます：

            \eqn?:(AZMathEquation.notag)(${
              \app{R_i^T}{I,\ a} \coloneq \sum_{t = 1}^{T} \app{r_i^t}{I,\ a}.
            });

            各時刻における戦略${\sigma^t}は、regret-matchingアルゴリズムによってこれまでの
            リグレットの総和に比例するように与えられることを思い出すと、次のように表すことが
            できます：

            \eqn?:(AZMathEquation.notag)(${
              \app{\sigma_i^{T + 1}}{I,\ a} \coloneq \cases{
                | \ \frac{\pB{\app{R_i^T}{I,\ a}}^+}{\sum_{a' \in \app{A}{I}} \pB{\app{R_i^T}{I,\ a'}}^+} | \text!{if} \sum_{a' \in \app{A}{I}} \pB{\app{R_i^T}{I,\ a'}}^+ > 0
                | \ 1\ /\ \pabs{\app{A}{I}} | \text!{otherwise.}
              |}
            });

            なお、ここで${\pB{x}^+ = \app{\max}{x,\ 0}}です。
            最後に、二人ゼロサムゲームにおいてナッシュ均衡に収束するのは各時刻の戦略を
            ${\app{\pi_i^{\sigma^t}}{I}}で重み付けした場合の平均戦略です：

            \eqn?:(AZMathEquation.notag)(${
              \app{\bar{\sigma}_i^T}{I} \coloneq \frac
                {\sum_{t = 1}^T \p{\app{\pi_i^{\sigma^t}}{I} \cdot \app{\sigma_i^t}{I}}}
                {\sum_{t = 1}^T \app{\pi_i^{\sigma^t}}{I}}.
            });
          }

          +subsubsection{CFRアルゴリズムの改良} <
            +p{
              以上が「バニラ」なCFRアルゴリズムの定義ですが、収束を実用上早めるためにさまざまな
              変種が提案されてもいます。
              ここでは、実装が比較的簡単でかつ効果の高い
              \dfn{Discounted CFR (DCFR) アルゴリズム}\footnote{
                N. Brown and T. Sandholm. \emph{Solving imperfect-information games via discounted
                regret minimization.} In AAAI, pages 1829–1836. 2019.
              }を紹介することにしましょう。
            }

            +p{
              DCFRアルゴリズムは３つの実数パラメータ${\alpha,\ \beta,\ \gamma}を取り、先ほどの
              ${\app{R_i^T}{I,\ a}}と${\app{\bar{\sigma}_i^T}{I}}を次のように再定義します：

              \gather?:(AZMathEquation.notag)(${
                | \app{R_i^{T + 1}}{I,\ a} \coloneq \cases{
                  | \frac{T^\alpha}{T^\alpha + 1} \app{R_i^T}{I,\ a} + \app{r_i^{T + 1}}{I,\ a} | \text!{if}\ \app{R_i^T}{I,\ a} \geq 0
                  | \frac{T^\beta}{T^\beta + 1} \app{R_i^T}{I,\ a} + \app{r_i^{T + 1}}{I,\ a} | \text!{otherwise},
                |}
                | \app{\bar{\sigma}_i^T}{I} \coloneq \frac
                    {\sum_{t = 1}^T \p{t^\gamma \cdot \app{\pi_i^{\sigma^t}}{I} \cdot \app{\sigma_i^t}{I}}}
                    {\sum_{t = 1}^T \p{t^\gamma \cdot \app{\pi_i^{\sigma^t}}{I}}}.
              |});

              ${\p{\alpha,\ \beta,\ \gamma} = \p{\infty,\ \infty,\ 0}}のとき、バニラな
              CFRアルゴリズムと一致します
              （${T + 1 = 2}のときに変なことが起こりますがそれは無視することとして）。
              提案論文では、さまざまなパラメータを実験的に試した結果
              ${\p{\alpha,\ \beta,\ \gamma} = \p{1.5,\ 0,\ 2}}が一貫して良いパフォーマンスを
              示すとされており、今後の実装においてもこのパラメータを採用することとします
              （特に、${\p{\alpha,\ \beta,\ \gamma} = \p{\infty,\ -\infty,\ 1}}の場合に相当する
              \dfn{CFR+アルゴリズム}\footnote{
                O. Tammelin, N. Burch, M. Johanson, and M. Bowling. \emph{Solving heads-up limit
                texas hold'em.} In IJCAI, pages 645–652. 2015.
              }
              がそれまでのstate-of-the-artとされていたのですが、このCFR+に対して一貫してより
              優れた結果をもたらしています）。
            }

            +p{
              非常にアドホックな感がある改変ではありますが、なぜこれで上手くいくようになるのかを
              かなりざっくり説明すると、リグレットの累積値${\app{R_i^T}{I,\ a}}および平均戦略
              ${\app{\bar{\sigma}_i^T}{I}}の計算において、各時刻における値を均一に重み付けするの
              ではなく、後の時刻ほど重みが大きくなるように改変をしている点が本質です。
              つまり、最初の方の時刻の寄与が“discount”されているわけですね。
              時刻が後になるほど徐々に賢い戦略を獲得していくアルゴリズムですから、後の時刻ほど重みを
              大きく設定することで収束を早くできる、というのは直感的にも正しいように思いますが、
              いかがでしょうか。
            }
          >
        >
      >

      +section{CFRアルゴリズムの実装} <
        +p{
          さて、ようやく本章からは皆さんお待ちかねの実装パートです。
          プログラムの記述言語には実行速度と実装のエレガントさを重視して\dfn{Rust}を採用することと
          します。
          yabaitechを手に取っていただけるような方々にはRustもお茶の子さいさいでしょう、
          ${\cdots\cdots}というのは半分冗談にしても、いくつかの言語に触れたことがある方ならば
          プログラムを読むのにそれほど困りはしないと思います。
        }

        +p{
          これから紹介するプログラムは、\url(`https://github.com/b-inary/yabai-vol7-src`); に
          アクセスすることで完全な形を確認することもできます。
        }

        +subsection{ゲームのインターフェース定義} <
          +p{
            まずはCFRアルゴリズム本体の実装に移る前に、ゲームを表現するインターフェースを
            先に定義することにしましょう。
            CFRアルゴリズムをこのインターフェースのもとで実装することで、具体的なゲームを
            考える際にはインスタンス化を行うだけでCFRアルゴリズムを適用できるようになります。
          }

          +p{
            本記事では主にポーカーを念頭に置いているので、ゲームの構造を制限してポーカーに特化した
            最適化を行えるようにします。
            具体的には、手札を配るといったプライベートな情報を与えるイベントはゲームの最初にしか
            発生しないという条件を加えます。
            例えばテキサスホールデムを考えると、最初に全員に二枚の手札が配られた後はコミュニティ
            カードの公開も含めて全員が正しくすべてのアクションを把握できるという構造になっている
            ことが分かります。
            本記事ではこのように全員が把握できるアクションは\dfn{パブリック}であると呼ぶことに
            しましょう。
            また、実装を簡単にするためにゲームは二人ゲームに限定することとし、各プレイヤーが持つ
            手札の構造は対称であるものとします。
          }

          +p{
            ゲームにこのような条件を加えるとどのような最適化が行えるかというと、本来ならば
            ゲームの最初に偶然手番がアクションを行うため、その分だけゲーム木も枝分かれするはず
            なのですが、これを飛ばしてパブリックなアクションの履歴のみを追跡し、終端履歴に
            到達したときに初めて最初に遡ってすべての手札の可能性をまとめて計算する、
            ということが行えるようになります。
            ただし、このような最適化を施すことで実装の読みやすさは犠牲になる部分があるので、
            読みづらいと思われる点はその都度解説していこうと思います。
          }

          +p{
            それでは具体的な実装に移っていきますが、まずはゲームのインスタンスが満たすべき
            振る舞いを定めるトレイト（インターフェース）を次のように定義します：
          }

          +code(```
            /// ゲームの定義を表すインターフェース
            pub trait Game {
                /// ゲーム木のノードを表す型
                type Node: GameNode;

                /// ゲーム木の根、すなわちゲームの初期履歴を返す
                fn root() -> Self::Node;

                /// プライベートな手札の組み合わせの個数を返す
                fn num_private_hands() -> usize;

                /// 終端履歴 `node` において、最初の偶然手番の寄与を含まないcounterfactual到達確率が
                /// `pmi` のときの `player` の counterfactual value を計算する
                fn evaluate(&self, node: &Self::Node, player: usize, pmi: &Vec<f64>) -> Vec<f64>;
            }
          ```);

          +p{
            この `Game` トレイトは、そのインスタンスに対して
            `Node, root(), num_private_hands(), evaluate()` の四点の定義を要求していることが
            読み取れます。
            このうち、 `Node` は `GameNode` トレイトを満たす型で、その `GameNode` トレイトの
            詳細については後述します。
            また、`root()` および `num_private_hands()` は、それぞれゲームの定義に関わる関数です。
          }

          +p{
            最後の `evaluate()` については、いきなり結構な難所なので詳しめに説明します。
            コードとコメントを読むと、`evaluate()` は `&self` の他に三個の引数
            `node, player, pmi` を受け取って「終端履歴 `node` において、最初の偶然手番の寄与を
            含まないcounterfactual到達確率が `pmi` のときの `player` のcounterfactual valueを
            計算する」というメソッドであることが分かります。
            ここで、counterfactual value はcounterfactual到達確率と利得の期待値の積

            \eqn?:(AZMathEquation.notag)(${
              \app{v_i}{\sigma,\ h} \coloneq \app{\pi_{-i}^\sigma}{h} \cdot \app{u^\sigma_i}{h}
            });

            で与えられるのでした。
            また終端履歴においては、利得の期待値は単に利得の値そのものと一致します。
            よって、このメソッドは `node` における `player` の利得を計算し、さらに引数として
            受け取った `pmi` を掛け合わせた結果を返すことを期待されていることが分かります。
          }

          +p{
            `evaluate()` が counterfactual value を返すのは良いとしても、関数の返り値と引数
            `pmi` の型が実数の配列となっているのが、このメソッドの定義を理解する上でもう一つ厄介な
            点となっています。
            この点についても説明していくことにすると、まずこれらの配列の要素数は
            `num_private_hands()` の値と一致することが期待されています。
            また、返り値の配列の各要素は `player` が特定の手札を持っているときの
            counterfactual value を表します。
            引数のcounterfactual到達確率 `pmi` については、相手（正確には自分以外）のプレイヤーが
            持っている手札に応じて値が定まるものなので、`pmi` の各要素には相手のプレイヤーが特定の
            手札を持っているときの counterfactual到達確率が入っています。
          }

          +p{
            引き続いて、各アクションを ${0,\ 1,\ \cdots,\ \pabs{\app{A}{h}} - 1} の整数に対応させて
            表現することとし、パブリックな履歴をアクションの配列で表現することにしましょう。
          }

          +code(```
            /// アクションを表す型
            pub type Action = usize;

            /// パブリックな履歴を表す型
            pub type PublicHistory = Vec<Action>;
          ```);

          +p{
            さらに、先ほど出現した、ゲーム木のノードを表現するインターフェースである `GameNode`
            トレイトを次のように定義します：
          }

          +code(```
            /// ゲーム木のノードを表すインターフェース
            pub trait GameNode {
                /// 現在のパブリックな履歴を返す
                fn public_history(&self) -> &PublicHistory;

                /// 現在のノードが終端履歴かどうかを返す
                fn is_terminal(&self) -> bool;

                /// 現在の手番のプレイヤーを返す
                fn current_player(&self) -> usize;

                /// 着手可能なアクションの個数を返す
                fn num_actions(&self) -> Action;

                /// 着手可能なアクションの一覧を返す
                fn actions(&self) -> std::ops::Range<Action> {
                    0..self.num_actions()
                }

                /// `action` を行った後のノードを返す
                fn play(&self, action: Action) -> Self;
            }
          ```);

          +p{
            この `GameNode` トレイトのメソッドシグネチャは素直なものばかりで、特に理解が難しい点は
            無いと思います。
          }
        >

        +subsection{CFRアルゴリズムの実装} <
          +p{
            それではゲームを表現するインターフェースが定まったところで、続いてCFRアルゴリズム本体の
            実装にいよいよ取り掛かっていきましょう。
            トップダウンに大枠から細部の順に実装を紹介していくことにします。
          }

          +subsubsection{CFRアルゴリズムを管理する構造体の定義} <
            +p{
              まずは、CFRアルゴリズムを管理する構造体の定義から見ていきましょう。
            }

            +code(```
              use std::collections::HashMap;

              pub struct CFRMinimizer<'a, T: Game> {
                  /// ゲーム定義のインスタンス
                  game: &'a T,

                  /// リグレットの累積値
                  cum_regret: HashMap<PublicHistory, Vec<Vec<f64>>>,

                  /// 各時刻の戦略の和
                  cum_strategy: HashMap<PublicHistory, Vec<Vec<f64>>>,

                  /// Discounted CFR のパラメータ
                  alpha_t: f64,

                  /// Discounted CFR のパラメータ
                  beta_t: f64,

                  /// Discounted CFR のパラメータ
                  gamma_t: f64,
              }
            ```);

            +p{
              Rustでは構造体の定義にはフィールドのみが含まれ、メソッドは `impl` ブロックに記述
              することになるので、とりあえずはこのフィールドたちを眺めることにしましょう。
              Rustの大きな特徴の一つでもある参照のライフタイムの管理に必要な注釈 `'a` が
              ちょっとうるさいですが、 まず `game` は先ほどの `Game` トレイトを満たすインスタンス
              への参照を保持するフィールドで、この `game` に対してCFRアルゴリズムが実行されます。
              後ろのフィールド `alpha_t, beta_t, gamma_t` については非本質的なので省略します。
              残った `cum_regret` と `cum_strategy` についてはもう少し詳しい説明をしましょう。
            }

            +p{
              これら二個のフィールドの型は `HashMap<PublicHistory, Vec<Vec<f64>>>` となっています。
              それぞれリグレットの累積値と各時刻の戦略の和を管理している変数ですが、パブリックな
              履歴がハッシュマップのキーとなっているのは良いものの、それで返ってくる値が二次元配列と
              なっているのはどういうことなのでしょうか。
              まず外側の配列は分かりやすくて、リグレットや戦略はアクションに対して定まるもの
              ですから、単に各アクションを受け取ります。
              それで残った方の配列ですが、これは先ほどと同じで手番となっているプレイヤーが特定の
              手札を持っているときの値が要素となっています。
              つまり、`cum_regret[public_history][action][private_hand]` のようにアクセスすると
              値を引くことができます。
            }

            +p{
              さて、ここからは `impl` ブロック内の記述に移っていくことになります。
              これも非本質的ですが、定数 `ALPHA, BETA, GAMMA` とコンストラクタを今のうちに定めて
              しまうことにします：
            }

            +code(```
              impl<T: GameNode> CFRMinimizer<T> {
                  const ALPHA: f64 = 1.5;
                  const BETA: f64 = 0.0;
                  const GAMMA: f64 = 2.0;
                  /// コンストラクタ
                  pub fn new(game: &'a T) -> Self {
                      Self {
                          game,
                          cum_regret: HashMap::new(),
                          cum_strategy: HashMap::new(),
                          alpha_t: 1.0,
                          beta_t: 1.0,
                          gamma_t: 1.0,
                      }
                  }

                  ...
            ```);
          >

          +subsubsection{CFRアルゴリズムの枠組みの実装} <
            +p{
              続いて、CFRアルゴリズムを実行する本体となるメソッドを書いていきます。
              アルゴリズムの大枠は自己対戦を繰り返すというものですが、各時刻における複雑な処理の
              部分はとりあえず `cfr_recursive()` に任せてしまうことにすると、実装はおおよそ
              次のようになるでしょう：
            }

            +code(```
              /// CFRアルゴリズムによる学習を行い、平均戦略を返す
              pub fn compute(
                  &mut self,
                  num_iterations: i32,
              ) -> HashMap<PublicHistory, Vec<Vec<f64>>> {
                  // ゲームの初期履歴を取得
                  let root = T::root();

                  // ゲーム木を構築して累積値を0で初期化
                  Self::build_tree(&root, &mut self.cum_regret);
                  Self::build_tree(&root, &mut self.cum_strategy);

                  // 到達確率を1で初期化
                  let ones = vec![1.0; T::num_private_hands()];

                  // 自己対戦を繰り返す
                  for t in 0..num_iterations {
                      let t_f64 = t as f64;
                      self.alpha_t = t_f64.powf(Self::ALPHA) / (t_f64.powf(Self::ALPHA) + 1.0);
                      self.beta_t = t_f64.powf(Self::BETA) / (t_f64.powf(Self::BETA) + 1.0);
                      self.gamma_t = (t_f64 + 1.0).powf(Self::GAMMA);

                      // プレイヤー毎に処理を行う
                      for player in 0..2 {
                          self.cfr_recursive(&root, player, &ones, &ones);
                      }
                  }

                  self.compute_average_strategy()
              }
            ```);

            +p{
              まず、`build_tree()` はゲーム木を構築して `cum_regret` および `cum_strategy` を
              初期化する関数で、次のような特に工夫の要らない再帰関数として定義されます：
            }

            +code(```
              /// ゲーム木を構築する
              fn build_tree(node: &T::Node, tree: &mut HashMap<PublicHistory, Vec<Vec<f64>>>) {
                  if node.is_terminal() {
                      return;
                  }
                  tree.insert(
                      node.public_history().clone(),
                      vec![vec![0.0; T::num_private_hands()]; node.num_actions()],
                  );
                  for action in node.actions() {
                      Self::build_tree(&node.play(action), tree);
                  }
              }
            ```);

            +p{
              さて、 `cum_regret` および `cum_strategy` を初期化したらいよいよ自己対戦を繰り返す
              ことになります。
              ここで、何となくさらっと流されやすい点ですが、各時刻においてプレイヤー毎に関数
              `cfr_recursive()` による処理を行っている点は注意が必要です。
            }

            +p{
              関数 `cfr_recursive()` の仕事は、後ほど詳しく見ていくことになりますが `cum_regret`
              および `cum_strategy` のうち、手番が `player` であるような部分の更新を行うことです。
              逆に言えば、これらのフィールドのうちの手番が `player` でないような部分は更新が
              行われません。
              これは一見するとおかしな実装で、バニラなCFRアルゴリズムはすべてのプレイヤーの
              リグレットや戦略を\dfn{同時更新(simultaneous update)}することを前提にしていた
              はずなのに、この実装ではリグレットや戦略がプレイヤー毎に
              \dfn{交互更新(alternating update)}されることになります。
              果たしてそれで大丈夫なのかと思われるかもしれませんが、実は実用上は交互更新の方が
              収束が早いことが知られており、そのため今回の実装でも交互更新をあえて採用しています。
              理論的にも、解析は難しくなるものの収束は引き続き保証されるようです\footnote{
                N. Burch, M. Moravcik, and M. Schmid. \emph{Revisiting CFR+ and alternating
                updates.} Journal of Artificial Intelligence Research, 64:429–443. 2019.
              }。
            }

            +p{
              最後に、`compute_average_strategy()` を呼び出して各時刻の重み付き平均戦略を返します。
              `compute_average_strategy()` も愚直に書き下せばよく、実装は次のようになるでしょう：
            }

            +code(```
              /// フィールド `cum_strategy` を参照して平均戦略を返す
              fn compute_average_strategy(&self) -> HashMap<PublicHistory, Vec<Vec<f64>>> {
                  let num_private_hands = T::num_private_hands();
                  let mut average_strategy = self.cum_strategy.clone();

                  for strategy in average_strategy.values_mut() {
                      let mut denom = vec![0.0; num_private_hands];
                      strategy.iter().for_each(|strategy_action| {
                          add_assign_vec(&mut denom, &strategy_action);
                      });

                      strategy.iter_mut().for_each(|strategy_action| {
                          div_assign_vec(strategy_action, &denom, 0.0);
                      });
                  }

                  average_strategy
              }
            ```);

            +p{
              ここで、 `add_assign_vec` といった関数がいくつか出現していますが、これは配列の
              各要素を加算代入したりする関数で、次のような定義を持ちます。
              処理の内容は何となく分かると思いますので、いちいち全部紹介はしないことにします。
            }

            +code(```
              fn add_assign_vec(lhs: &mut Vec<f64>, rhs: &Vec<f64>) {
                  lhs.iter_mut().zip(rhs).for_each(|(l, r)| *l += *r);
              }
            ```);
          >

          +subsubsection{Counterfactual value の再帰的な計算} <
            +p{
              それでは、先ほど後回しにしてしまった `cfr_recursive()` の実装に移っていきます。
              先述したように、 `cfr_recursive()` の仕事は `cum_regret` と `cum_strategy` の
              更新ですが、関数そのものの返り値は counterfactual value となっていて、関数名の通り
              これを再帰的に計算します。
            }

            +p{
              さすがにこの関数はそれなりの実装量になってしまうため、紙面で読むのはややタフかも
              しれませんが、頑張って見ていきましょう。
            }

            +code(```
              /// `player` の counterfactual value を再帰的に計算する
              fn cfr_recursive(
                  &mut self,
                  node: &T::Node,
                  player: usize,
                  pi: &Vec<f64>,
                  pmi: &Vec<f64>,
              ) -> Vec<f64> {
                  // 終端履歴なら単に counterfactual value を返す
                  if node.is_terminal() {
                      return self.game.evaluate(node, player, pmi);
                  }

                  // 現在のパブリックな履歴を取得
                  let public_history = node.public_history();

                  // 現時刻の戦略を regret-matching アルゴリズムによって求める
                  let mut strategy = Self::regret_matching(&self.cum_regret[public_history]);
                  // 返り値となる counterfactual value を0で初期化
                  let mut cfvalue = vec![0.0; T::num_private_hands()];

                  // 手番が `player` の場合
                  if node.current_player() == player {
                      let mut cfvalue_action_vec = Vec::with_capacity(node.num_actions());

                      // 各アクションに対する counterfactual value を計算する
                      for action in node.actions() {
                          let pi = mul_vec(&pi, &strategy[action]);
                          let mut cfvalue_action =
                              self.cfr_recursive(&node.play(action), player, &pi, pmi);
                          cfvalue_action_vec.push(cfvalue_action.clone());
                          mul_assign_vec(&mut cfvalue_action, &strategy[action]);
                          add_assign_vec(&mut cfvalue, &cfvalue_action);
                      }

                      // リグレットの累積値と戦略の和を更新
                      for action in node.actions() {
                          let cum_regret: &mut Vec<f64> =
                              &mut self.cum_regret.get_mut(public_history).unwrap()[action];
                          let cum_strategy: &mut Vec<f64> =
                              &mut self.cum_strategy.get_mut(public_history).unwrap()[action];

                          cum_regret.iter_mut().for_each(|el| {
                              *el *= if *el >= 0.0 {
                                  self.alpha_t
                              } else {
                                  self.beta_t
                              }
                          });

                          add_assign_vec(cum_regret, &cfvalue_action_vec[action]);
                          sub_assign_vec(cum_regret, &cfvalue);

                          mul_assign_scalar(&mut strategy[action], self.gamma_t);
                          mul_assign_vec(&mut strategy[action], &pi);
                          add_assign_vec(cum_strategy, &strategy[action]);
                      }
                  }
                  // 手番が `player` でない場合
                  else {
                      for action in node.actions() {
                          let pmi = mul_vec(&pmi, &strategy[action]);
                          add_assign_vec(
                              &mut cfvalue,
                              &self.cfr_recursive(&node.play(action), player, pi, &pmi),
                          );
                      }
                  }

                  cfvalue
              }
            ```);

            +p{
              まず、はじめに関数 `cfr_recursive()` の引数についてですが、 `node` と `player` に
              ついては特に説明は要らないでしょう。
              `pi` と `pmi` は、現在の引数 `node` への到達確率のうち `player` の寄与と `player`
              以外の寄与を表しています。
              また、返り値は counterfactual value となっています。
              ここで `pi, pmi` および返り値の型が実数の配列となっているのはどういうことかというと、
              例によってこれらの配列の各要素は `player` （`pmi` については相手のプレイヤー）が
              特定の手札を持っているときの値を表しています。
            }

            +p{
              この再帰関数の終了条件は、引数 `node` が終端履歴に達することです。
              終端履歴に達したら、ゲームの定義に基づいて counterfactual value を計算し、その値を
              返します。
              引数 `node` が終端履歴でなかった場合、再帰的に子ノードの計算を行うことになります。
            }

            +p{
              それでは引数 `node` が終端履歴でなかった場合の処理についてですが、まず現時刻における
              戦略が regret-matching アルゴリズムによって求められます。
              この regret-matching アルゴリズムの実装は簡単ですね：
            }

            +code(```
              /// regret-matching アルゴリズム
              fn regret_matching(regrets: &Vec<Vec<f64>>) -> Vec<Vec<f64>> {
                  let num_actions = regrets.len();
                  let num_private_hands = T::num_private_hands();
                  let mut strategy = regrets.clone();

                  let mut denom = vec![0.0; num_private_hands];
                  strategy.iter_mut().for_each(|strategy_action| {
                      nonneg_assign_vec(strategy_action);
                      add_assign_vec(&mut denom, strategy_action);
                  });

                  strategy.iter_mut().for_each(|strategy_action| {
                      div_assign_vec(strategy_action, &denom, 1.0 / num_actions as f64);
                  });

                  strategy
              }
            ```);

            +p{
              あとはcounterfactual valueの計算を再帰的に行いながら、現在の手番が `player` である
              場合は `cum_regret` と `cum_strategy` も併せて適切に更新していくだけです。
              ${\cdots\cdots}と、文章で書くのは簡単ですが、足し算や掛け算などをバグ無く正確に
              記述するのは結構大変な作業です。
              行数で言ってしまえば60行程度とそれほど長いというわけではない関数ですが、デバッグも
              容易ではありませんし、実装の与えられていない状態から書き起こそうと思うとなかなか
              骨が折れるでしょう。
            }

            +p{
              以上でCFRアルゴリズムの実装は完成です。
              ここまで出来てしまえば `Game` トレイトを満たすようにゲームの定義を行ってあげれば
              CFRアルゴリズムを簡単に適用することができます。
              次章以降では、ゲームのインスタンスをいくつか定義して実際にCFRアルゴリズムを動かして
              いきます。
            }
          >
        >

        +subsection{ユーティリティ関数の実装} <
          +p{
            と、ゲームの各インスタンスを見に行く前に、得られた戦略を解析するユーティリティ関数も
            ついでに実装してしまいましょう。
            具体的には、戦略の組が与えられたときに利得の期待値を計算する関数と、可搾取量を計算する
            関数を実装していきます。
          }

          +subsubsection{利得の期待値の計算} <
            +p{
              まず利得の期待値を計算する関数の方から見ていきます。
              と言っても、再帰関数に抵抗が無い方ならば利得の期待値を計算する処理は素直に書けるでしょう。
              問題は `Game` トレイトの `evaluate()` メソッドが厄介な型をしているという点ですが、
              まず求める利得の期待値${\app{u_i}{\sigma}}は

              \eqn?:(AZMathEquation.notag)(${
                \app{u_i}{\sigma} = \sum_{z \in Z} \app{\pi^\sigma}{z} \cdot \app{u_i}{z}
              });

              であり、メソッド `evaluate()` は counterfactual value

              \eqn?:(AZMathEquation.notag)(${
                \app{v_i}{\sigma,\ z} = \app{\pi_{-i}^\sigma}{z} \cdot \app{u_i}{z}
              });

              を計算してくれるので、あとは
              ${\app{\pi^\sigma}{h} = \app{\pi_i^\sigma}{h} \cdot \app{\pi_{-i}^\sigma}{h}} で
              あるということを思い出せば、終端履歴における処理の内容も理解できると思います。
            }

            +code(```
              /// 戦略 `strategy` のもとでの `player` の利得の期待値を返す
              pub fn compute_ev<T: Game>(
                  game: &T,
                  player: usize,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> f64 {
                  let ones = vec![1.0; T::num_private_hands()];
                  compute_ev_rec(game, &T::root(), player, &ones, &ones, strategy)
              }

              /// 利得の期待値を再帰的に計算するヘルパー
              fn compute_ev_rec<T: Game>(
                  game: &T,
                  node: &T::Node,
                  player: usize,
                  pi: &Vec<f64>,
                  pmi: &Vec<f64>,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> f64 {
                  if node.is_terminal() {
                      // `dot` は内積を計算する関数
                      return dot(&game.evaluate(node, player, pmi), &pi);
                  }
                  let current_strategy = &strategy[node.public_history()];
                  if node.current_player() == player {
                      node.actions()
                          .map(|action| {
                              let pi = mul_vec(&current_strategy[action], &pi);
                              compute_ev_rec(game, &node.play(action), player, &pi, pmi, strategy)
                          })
                          .sum()
                  } else {
                      node.actions()
                          .map(|action| {
                              let pmi = mul_vec(&current_strategy[action], &pmi);
                              compute_ev_rec(game, &node.play(action), player, pi, &pmi, strategy)
                          })
                          .sum()
                  }
              }
            ```);
          >

          +subsubsection{可搾取量の計算} <
            +p{
              さて、残った方の可搾取量を計算する関数ですが、こちらはちょっとマジカルな感じもある
              実装となっています。
              実装は簡潔だが、なぜこれで正しく動くのかがなかなか分からないという類のやつですね。
              何はともあれ実装を先に見てみることにしましょう。
            }

            +code(```
              /// 戦略の組 `strategy` の可搾取量を返す
              pub fn compute_exploitability<T: Game>(
                  game: &T,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> f64 {
                  let ones = vec![1.0; T::num_private_hands()];
                  let br0 = best_cfvalues_rec(game, &T::root(), 0, &ones, strategy);
                  let br1 = best_cfvalues_rec(game, &T::root(), 1, &ones, strategy);
                  br0.iter().sum::<f64>() + br1.iter().sum::<f64>()
              }
            ```);

            +code(```
              /// 最適反応戦略の counterfactual value を再帰的に計算するヘルパー
              fn best_cfvalues_rec<T: Game>(
                  game: &T,
                  node: &T::Node,
                  player: usize,
                  pmi: &Vec<f64>,
                  strategy: &HashMap<PublicHistory, Vec<Vec<f64>>>,
              ) -> Vec<f64> {
                  if node.is_terminal() {
                      return game.evaluate(node, player, pmi);
                  }

                  if node.current_player() == player {
                      node.actions()
                          .map(|action| {
                              best_cfvalues_rec(game, &node.play(action), player, pmi, strategy)
                          })
                          .reduce(|v, w| max_vec(&v, &w))
                  } else {
                      let current_strategy = &strategy[node.public_history()];
                      node.actions()
                          .map(|action| {
                              let pmi = mul_vec(&pmi, &current_strategy[action]);
                              best_cfvalues_rec(game, &node.play(action), player, &pmi, strategy)
                          })
                          .reduce(|v, w| add_vec(&v, &w))
                  }
                  .unwrap()
              }
            ```);

            +p{
              ヘルパー関数である `best_cfvalues_rec()` は、コメントにも書いてあるように、
              `player` が最適反応戦略を取ったときの counterfactual value を再帰的に求める関数です。
              実際、終端履歴に到達したら単に `evaluate()` を呼んでいるだけであることが分かります。
            }

            +p{
              さて、実はこの counterfactual value は利得の期待値への寄与とイコールに
              なっているのですが、ここで「最適反応戦略には必ず純粋戦略が含まれている」という
              重要な事実を利用しています。
              というのも、`player` 以外の戦略は固定されているものとしているのですから、
              自分が相手に搾取されてしまう可能性は考える必要がなく、従って単に利得の期待値が最も
              大きくなるようなアクションを常に選べば利得を最大化できるためです。
              よって、各履歴${h}における ${\app{\pi_i^\sigma}{h}} は0か1のどちらかであるとしてよく、
              ${\app{\pi_i^\sigma}{h}} を追跡したりする必要が無いのです。
            }

            +p{
              よって、`best_cfvalues_rec()` は ${\app{\pi_{-i}^\sigma}{h}} だけを引数にとって
              追跡します。
              現在の `node` の手番が `player` であるなら、counterfactual value、すなわち利得の
              期待値への寄与が最大となるアクションを選択し、手番が `player` でないなら単に
              counterfactual value を加算します。
            }
          >
        >
      >

      +section{CFRの具体例1: Kuhn Poker} <
        +p {
          それでは、本章からはいくつかのゲームの定義を実装し、CFRアルゴリズムを実際に動かして
          いきます。
          まずは\dfn{Kuhn poker}と呼ばれる非常に単純化されたポーカーの変種を扱っていくことに
          しましょう。
        }

        +subsection{Kuhn Pokerのルール} <
          +p{
            Kuhn poker は２人向けのポーカーで、デッキは３枚のカードのみから構成されます。
            ここでは、キング(K)、クイーン(Q)、ジャック(J)の３枚を用いることとし、先に挙げた
            カードほど強いものとします。
          }

          +p{
            ここからはゲームの進行について説明していきますが、まず２人のプレイヤーは相手から
            見えないようにカードをデッキから１枚ずつ引き、残った１枚については伏せたままにしておきます。
            続いて、各プレイヤーは１点の\dfn{アンティ}（参加費）を場に供託します。
            どちらかのプレイヤーが\dfn{ベット}することでもう１点を追加で供託することもできますが、
            この手順については後ほど説明します。
            供託する点数について両者の合意が取れた場合、各プレイヤーは自分の持つカードを公開し、
            より強いカードを持っていた方がそれまでに供託されていた点数を総取りします
            （\dfn{ショーダウン}）。
          }

          +p{
            アンティを供託した後は、プレイヤーは先手と後手に分かれて、次のような手順を経て
            供託する点数についての合意を取ろうとします：
          }

          +listing{
            * 先手は手番をパスする（\dfn{チェック}）か、１点を追加で供託する（\dfn{ベット}）かを
              選択します。

            ** 先手がチェックした場合、後手は同様に手番をパスしてショーダウンに進む（チェック）か、
               １点を追加で供託する（ベット）かを選択します。

            *** 後手がベットした場合、先手はすでに供託した１点を相手に与えることにして勝負から降りる
                （\dfn{フォールド}）か、後手と同様に１点を追加で供託してショーダウンに進む
                （\dfn{コール}）かを選択します。

            ** 先手がベットした場合、後手はすでに供託した１点を相手に与えることにして勝負から降りる
               （フォールド）か、先手と同様に１点を追加で供託してショーダウンに進む（コール）かを
               選択します。
          }

          +p{
            以上が Kuhn poker のルールですが、非常に単純化されたポーカーであるとは述べたものの、
            最適な戦略がどのようなものであるかを思いつくのはそう簡単ではないでしょう。
            キングを持っていて相手にベットされた場合は常にコールする、またジャックを持っていて
            相手にベットされた場合は常にフォールドする、さらに後手がキングを持っていて先手が
            チェックした場合は常にベットする、くらいのことは分かるでしょうが、それ以上のことは
            少なくとも筆者にはまったく明らかではありません。
          }

          +p{
            この Kuhn poker は発案者の Harold W. Kuhn によって解析もなされており、ナッシュ均衡が
            数学的に求まっています。
            それぞれの具体的な最適戦略は次に挙げる通りですが、ここで先手は最適戦略を無限に
            持っており、パラメータ ${\alpha \in \pB{0,\ 1\ /\ 3}} を自由に選択することができます：
          }

          +p{
            【先手】
          }

          +listing{
            * 手札がキング: 確率 ${3 \alpha} でベットし、自分のチェックに対して後手がベットした場合は
              常にコール
            * 手札がクイーン: 常にチェックし、後手がベットした場合は ${\alpha + 1\ /\ 3} の確率で
              コール
            * 手札がジャック: 確率 ${\alpha} でベットし、自分のチェックに対して後手がベットした場合は
              常にフォールド
          }

          +p{
            【後手】
          }

          +listing{
            * 手札がキング: 常にベットかコール
            * 手札がクイーン: 先手がチェックした場合はチェック、先手がベットした場合は確率
              ${1\ /\ 3} でコール
            * 手札がジャック: 先手がチェックした場合は確率 ${1\ /\ 3} でベット、先手がベットした
              場合は常にフォールド
          }

          +p{
            なお、両プレイヤーがこのナッシュ均衡に基づく戦略を取った場合、後手が平均して
            ${1\ /\ 18} 点だけ勝つことが知られています。
          }

          +p{
            この最適戦略を眺めて個人的に面白いと思うのは、先手はパラメータ ${\alpha} の値にも
            よりますが、両プレイヤーとも最弱のカードであるジャックを持っているときにも一定の確率で
            ベットを行い、ブラフを仕掛けるのが最適とされている点です。
            特にポーカーの楽しさの大部分は相手がブラフをしているかどうかの心理的な読み合いに
            あると思うのですが、適切な確率でブラフを仕掛けることはナッシュ均衡の観点からも
            ちゃんと正当化されるのだということが、このように非常に単純化されたルールのもとでも
            確認することができます。
          }
        >

        +subsection{ゲーム定義の実装} <
          +p{
            Kuhn poker のルールについての説明が済んだので、続いてゲーム定義の実装に移って
            いきましょう。
            まず、Kuhn poker には特に設定を変更できる部分がありませんので、構造体 `KuhnGame` には
            空の定義を持たせます。
            `KuhnGame` のノードを表す型である `KuhnNode` はパブリックな履歴を保持すれば良いので、
            唯一のフィールド `public_history` を持ちます。
            また、ノードはクローンできると便利なので、 `KuhnNode` は `Clone` トレイトを
            継承しておきます。
          }

          +code(```
            pub struct KuhnGame {}

            #[derive(Clone)]
            pub struct KuhnNode {
                public_history: PublicHistory,
            }
          ```);

          +p{
            それでは `KuhnGame` の実装に移っていきますが、`Node, root(), num_private_hands()` の
            実装は明らかで、特に考える必要はないでしょう。
          }

          +code(```
            impl Game for KuhnGame {
                type Node = KuhnNode;

                fn root() -> KuhnNode {
                    KuhnNode {
                        public_history: Vec::new(),
                    }
                }

                fn num_private_hands() -> usize {
                    3
                }

                fn evaluate(&self, node: &KuhnNode, player: usize, pmi: &Vec<f64>) -> Vec<f64> {
                    ...
                }
            }
          ```);

          +p{
            ここで、やはり問題となるのが `evaluate()` の実装です。
            `evaluate()` がどのようなメソッドだったかを思い出すと、「終端履歴 `node` において、
            最初の偶然手番の寄与を含まない counterfactual到達確率が `pmi` のときの `player` の
            counterfactual value を計算する」というものでした。
            また、counterfactual valueは終端履歴においてはcounterfactual到達確率と利得の積で
            与えられるのでしたね。
          }

          +p{
            一つ注意したいのは、返り値となる counterfactual value の計算においては最初の偶然手番に
            よる寄与も含むcounterfactual到達確率の値を用いるべきですが、引数 `pmi` には最初の
            偶然手番による寄与が含まれていないという点です。
            つまり、最初の偶然手番によってどのような確率でどのようなハンドが各プレイヤーに
            与えられるのかは、この `evaluate()` メソッド内で定義を与えなければならないということです。
            `evaluate()` を `GameNode` ではなく `Game` トレイトのメソッドとしている理由は
            このためでもありますが、今回の Kuhn poker の例では両プレイヤーへのカードの配り方は
            全部で６通りあり、それらが等確率に発生するということをこの `evaluate()` メソッドに
            組み込む必要があります。
          }

          +p{
            以上の考察を経ると、利得を計算する関数 `payoff()` を用いて `evaluate()` が次のように
            書き下せることが分かるでしょう：
          }

          +code(```
            fn evaluate(&self, node: &KuhnNode, player: usize, pmi: &Vec<f64>) -> Vec<f64> {
                let mut cfvalue = vec![0.0; Self::num_private_hands()];

                for my_card in 0..Self::num_private_hands() {
                    for opp_card in 0..Self::num_private_hands() {
                        if my_card == opp_card {
                            continue;
                        }
                        cfvalue[my_card] +=
                            Self::payoff(node, player, my_card, opp_card) * pmi[opp_card] / 6.0;
                    }
                }

                cfvalue
            }
          ```);

          +p{
            さらに、以下の実装を加えて構造体 `KuhnGame` の定義を完成させます：
          }

          +code(```
            const CHECK_FOLD: usize = 0;
            const BET_CALL: usize = 1;

            impl KuhnGame {
                /// コンストラクタ
                pub fn new() -> Self {
                    Self {}
                }

                fn payoff(
                    node: &KuhnNode,
                    player: usize,
                    my_card: usize,
                    opp_card: usize,
                ) -> f64 {
                    match (node.public_history.as_slice(), node.public_history.last()) {
                        ([CHECK_FOLD, CHECK_FOLD], _) if my_card > opp_card => 1.0,
                        ([CHECK_FOLD, CHECK_FOLD], _) => -1.0,
                        (_, Some(&CHECK_FOLD)) if node.current_player() == player => 1.0,
                        (_, Some(&CHECK_FOLD)) => -1.0,
                        _ if my_card > opp_card => 2.0,
                        _ => -2.0,
                    }
                }
            }
          ```);

          +p{
            最後に、ゲーム木のノードを表す型である `KuhnNode` の実装を以下のように与えます。
            こちらも特に難しい点は無く、素直に定義を書き下しているだけであることが分かると思います。
          }

          +code(```
            impl GameNode for KuhnNode {
                fn public_history(&self) -> &PublicHistory {
                    &self.public_history
                }

                fn is_terminal(&self) -> bool {
                    match self.public_history.as_slice() {
                        [CHECK_FOLD, BET_CALL] => false,
                        [_, _] => true,
                        [_, _, _] => true,
                        _ => false,
                    }
                }

                fn current_player(&self) -> usize {
                    self.public_history.len() % 2
                }

                fn num_actions(&self) -> usize {
                    2
                }

                fn play(&self, action: Action) -> Self {
                    let mut ret = self.clone();
                    ret.public_history.push(action);
                    ret
                }
            }
          ```);
        >

        +subsection{プログラムの実行} <
          +p{
            ここまで実装パートが長かったですが、いよいよプログラムを実際に実行していきましょう。
            `main()` 関数を次のように記述することで、これまでに作ってきたコンポーネントや関数を
            簡単に組み合わせて実行することができます：
          }

          +code(```
            fn main() {
                // CFRの反復回数を指定
                let num_iterations = 10000;

                // Kuhn poker のゲーム定義のインスタンスを作成
                let kuhn_game = KuhnGame::new();

                // CFRを管理する構造体のインスタンスを作成
                let mut cfr = CFRMinimizer::new(&kuhn_game);

                // CFRを実行して最適戦略を得る
                let strategy = cfr.compute(num_iterations);

                // 利得の期待値と可搾取量を計算
                let ev = compute_ev(&kuhn_game, 0, &strategy);
                let exploitability = compute_exploitability(&kuhn_game, &strategy);

                ...
          ```);

          +p{
            さて、今回はCFRの反復回数を10000回に指定しましたが、これで正しい結果はちゃんと
            得られるのでしょうか。
            出力を適当に整形すると、手元のマシンでは0.1秒も掛からずに次のような出力が得られました：
          }

          +console(```
            [First player]
            - EV: -0.0556
            - Bet%                          - Call% (Check => Bet => ?)
                K: 56.72%                       K: 100.00%
                Q: 0.00%                        Q: 52.25%
                J: 18.90%                       J: 0.00%
            [Second player]
            - EV: +0.0556
            - Bet% (Check => ?)             - Call% (Bet => ?)
                K: 100.00%                      K: 100.00%
                Q: 0.00%                        Q: 33.33%
                J: 33.33%                       J: 0.00%
          ```);

          +console(```
            - Exploitability: +4.774e-5
          ```);

          +p{
            まず、可搾取量は${4.8 \times 10^{-5}}となっており、実用上は十分収束していることが
            分かります。
            これだけ収束していれば、得られた戦略もほぼナッシュ均衡となっているはずで、数学的に
            求まっている最適戦略と実際に比較してみると${\alpha \approx 18.91\%}のときの最適戦略と
            ほぼ一致していることが見て取れます。
            先手および後手の利得の期待値もちゃんと${\mp 1\ /\ 18}になっていますね。
          }

          +p{
            ここまで理論も実装もそれなりに重たかったかと思いますが、実際にプログラムが動いたときの
            喜びもその分大きいものです。
            完全情報ゲームであれば、例えば三目並べのような比較的単純なゲームの最適戦略を求めたければ
            単なる深さ優先探索をするだけですが、不完全情報ゲームは、この Kuhn poker のように例え
            非常に単純化されたルールのものであっても、プログラムによって最適戦略を求めようと思うと
            これだけの準備が必要となってしまうのです。
            逆に言えば、不完全情報ゲームの魅力や奥の深さはこのような性質によるところが大きいとも
            言えるでしょう。
          }
        >
      >

      +section{CFRの具体例2: プッシュ/フォールド} <
        +p{
          さて、前章で見てきたKuhn pokerはルールが非常に単純化されているもので、ナッシュ均衡解が
          解析的に求まっているような言わばおもちゃに過ぎませんでした。
          おもちゃであってもきちんと解析を行うことは大事なことではありますが、具体例がそれだけでは
          物足りなさもあるでしょう。
          そこで本章では、実際のテキサスホールデムにおいてしばしば発生する単純な状況をモデル化した
          \dfn{プッシュ/フォールド}ゲームについて解析を行っていきます。
        }

        +subsection{プッシュ/フォールドのルール} <
          +p{
            本章で扱うプッシュ/フォールドゲームは、テキサスホールデムの用語が分かる方には
            「\dfn{スモールブラインド}が\dfn{プリフロップ}で\dfn{オールイン}か\dfn{フォールド}しか
            しない\dfn{ヘッズアップ}のテキサスホールデム」で通じるのですが、そうでない方には
            暗号か何かにしか見えないでしょう。
            そういったテキサスホールデムに馴染みが無い方に向けて、ここからは最低限のルールの説明を
            していきます。
          }

          +p{
            まず、\dfn{ヘッズアップ}というのは一対一、すなわち二人で行われるポーカーゲームのことを
            指します。
            ヘッズアップでは、プレイヤーは\dfn{スモールブラインド}と\dfn{ビッグブラインド}に
            分かれます。
            ここで、両者とも点数を強制的に供託させられますが、スモールブラインドが供託しなければ
            ならない点数はビッグブラインドの半分となっています。
            ビッグブラインドが強制的に供託させられる点数は\dfn{bb}という単位として扱われます。
          }

          +p{
            続いて、テキサスホールデムでは両プレイヤーに二枚の手札が配られますが、この手札が配られた
            直後のラウンドを\dfn{プリフロップ}と呼びます。
            プリフロップでは、スモールブラインドが先にアクションを起こします。
            ここで、プッシュ/フォールドでは手持ちの点数をすべて供託する（\dfn{オールイン}）か、
            すでに供託した0.5[bb]を相手に与えることにして勝負から降りる（\dfn{フォールド}）かの
            どちらかのアクションのみを取ることができます。
            スモールブラインドがオールインした場合、ビッグブラインドは供託する点数を相手に合わせて
            勝負に乗る（\dfn{コール}）か、すでに供託した1[bb]を相手に与えることにして勝負から降りる
            （フォールド）かを選択することになります。
          }

          +p{
            オールインに対してビッグブラインドがコールした場合は\dfn{ショーダウン}へと進むことに
            なり、より強い役を作った方がそれまでに供託された点数を総取りします。
            テキサスホールデムでは二枚の手札に加えて、コミュニティカードと呼ばれる全員が使用できる
            カードが五枚公開され、これらの計七枚のうちもっとも強い五枚の組み合わせを用いて役の強さを
            競います。
          }

          +p{
            役については、多くの方に馴染みのあるであろうポーカーと同じなので特に説明はしませんが、
            同一の役であってもカードの\dfn{ランク}によってタイブレークがなされる点は注意が必要かも
            しれません。
            例えば、エースのワンペアはキングのワンペアよりも強く、また同じエースのワンペアどうしで
            あっても持っているカードのうち次に強いカードがキングの場合はそれがクイーンの場合よりも
            強いものとして扱われます。
            なお、スペードやハートといったカードの\dfn{スート}は役の強さに影響しません。
          }

          +p{
            さて、ルールの最低限の説明は以上になりますが、なぜこのようなゲームを考えているのかに
            ついても簡単に説明することにしましょう。
            通常のテキサスホールデムではスモールブラインドはオールインとフォールドの他にも、
            供託する点数を合わせる\dfn{コール}や、オールインまでは行かないが供託する点数を増やす
            \dfn{レイズ}というアクションが行えます。
            このコールやレイズといったアクションを除外してしまったモデルに意味があるのかというと、
            両プレイヤーの持ち点の最小値（\dfn{エフェクティブスタック}）が小さい場合は、選択肢を
            オールインとフォールドのみに限ってもそれなりに良い戦略となることが知られています。
            というのも、中途半端なコールやレイズはビッグブラインド側のオールインを簡単に誘発して
            しまうためで、それならば最初から選択肢をオールインかフォールドに限ってしまうのは合理的と
            言えます\footnote{
              エフェクティブスタックが小さい状況では中途半端なレイズが悪手なのは間違いないのですが、
              最近の解析ではコールは実は強力なことが分かりつつあります。
              ただし、コールを戦略に組み込む場合は相手のオールインに対するディフェンスや、
              プリフロップ以降の戦略についても考えなければなりません。
              よって、コールは上級者に向いたアクションであると言え、初心者にはやはりオールインか
              フォールドに限った戦略の方が扱いやすいでしょう。
            }。
          }
        >

        +subsection{ゲーム定義の実装} <
          +p{
            ルールについての説明が済んだので、ゲーム定義の実装の説明に移っていきましょう。
            なお Kuhn pokerの例では実装をすべて紹介しましたが、今回は実装の紹介は省略します。
          }

          +p{
            ここまで読んでこられた方には予想がついているかと思いますが、ゲーム定義の実装の鬼門は
            やはり `evaluate()` メソッドです。
            今回は手札が二枚あるため `num_private_hands()` の値は${\choose{52}{2} = 1,326}と
            なっており、相手も同じく二枚の手札を持っていますから、ショーダウンまで進んだ場合は
            ${\choose{52}{2} \times\ \choose{50}{2} = 1,624,350}個のパターンの計算を `evaluate()` で
            行う必要があります。
          }

          +p{
            この時点ですでにやや暗雲が立ち込めている気がしますが、さらに問題となるのは
            ショーダウン時の勝率を計算しようと思うと${\choose{48}{5} = 1,712,304}通りの
            コミュニティカードの組み合わせを総当たりする必要が生じることです。
            ${\choose{52}{2} \times\ \choose{50}{2} \times\ \choose{48}{5} \approx 2.8 \times 10^{12}}
            ですから、このオーダーの計算を繰り返すのはまったく現実的ではありません。
          }

          +p{
            ただ幸いなことに、自分と相手の手札が与えられたときの勝率は一定ですので、今回の状況では
            それを\dfn{前計算}してしまうことが可能です。
            およそ${2.8 \times 10^{12}}通りの総当たりはやはり必要になってしまいますが、一度その計算
            さえできてしまえば結果を使い回すことができます。
            前号のvol.6で紹介した高速役判定プログラムを用いることで、筆者の16スレッドのマシンでは
            20分掛からず前計算を完了できました（自画自賛ですがこれは非常に高速だと思います）。
          }

          +p{
            あとは、自分の手札と相手の手札でカードが重複しないように気をつけながら、それぞれの場合の
            counterfactual valueを計算する処理を書き下せば良いことになります。
            これも文章で書くのは簡単だが実際に実装するのはハマると大変という類のものですが、
            今回の場合は完成した実装がありますので、気になる方は４章で紹介したURLからリポジトリを
            参照ください。
          }

          +p{
            ところで、手札の組み合わせは${\choose{52}{2} = 1,326}通りであると述べましたが、これを
            ${169}通りの同値類に潰せるのではないかと思われた賢い方もいるかもしれません。
            というのも、例えば \spade{A}\heart{A} のペアと \diamond{A}\club{A} のペアは異なる手札
            ですが、勝率の観点からは同じであるため、これらをまとめて扱うことが一見可能なように
            思えるためです。
          }

          +p{
            しかし実際にはこれは横着というもので、相手の手札を特定のものに固定して考えると勝率は
            異なるため、このようなことを行うとプログラムが上手く動かなくなってしまいます。
            例えば相手の手札を \spade{K}\heart{K} に固定したとすると、 \spade{A}\heart{A} のペアは
            フラッシュで負ける可能性が無いのに対し、 \diamond{A}\club{A} のペアはスペードかハートの
            フラッシュを相手に作られると負け得るため、勝率が異なることが分かります。
            従って、`evaluate()` では${\choose{52}{2} = 1,326}通りのすべての組み合わせをちゃんと
            試す必要があるのです\footnote{
              計算結果が厳密でなくなることを承知の上で、あえてこういった「横着」をするという方針も
              あり得ます。
              このようなテクニックは\dfn{抽象化}と呼ばれ、最先端のソルバーなどは多かれ少なかれ
              何らかの抽象化を行っています。
            }。
          }
        >

        +subsection{プログラムの実行} <
          +p{
            さて、それではゲーム定義が実装できたものとしてプログラムを実際に実行していきましょう。
            エフェクティブスタックは10[bb]、反復回数は1000回に指定することにすると、`main()` 関数は
            次のようになります：
          }

          +code(```
            fn main() {
                // エフェクティブスタックを指定
                let effective_stack = 10.0;

                // CFRの反復回数を指定
                let num_iterations = 1000;

                // プッシュ/フォールドゲームの定義のインスタンスを作成
                let push_fold_game = PushFoldGame::new(effective_stack);

                // CFRを管理する構造体のインスタンスを作製
                let mut cfr = CFRMinimizer::new(&push_fold_game);
                // CFRを実行して最適戦略を得る
                let strategy = cfr.compute(num_iterations);

                // 利得の期待値と可搾取量を計算
                let ev = compute_ev(&push_fold_game, 0, &strategy);
                let exploitability = compute_exploitability(&push_fold_game, &strategy);

                ...
          ```);

          +p{
            手元のマシンでは実行に10秒程度を要しましたが、まずは可搾取量を見てみましょう：
          }

          +console(```
            - Exploitability: +1.447e-7[bb]
          ```);

          +p{
            可搾取量は${1.5 \times 10^{-7}} [bb]未満となっており、これなら十分収束していると
            言えそうですね。
            続いてスモールブラインドの最適戦略はどうでしょうか：
          }

          +console(```
            [Pusher (Small blind)]
            - EV: -0.0454[bb]
            - Overall push rate: 58.29%
             |   A     K     Q     J     T     9     8     7     6     5     4     3     2
            -+------------------------------------------------------------------------------
            A| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%
            K| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%
            Q| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%
            J| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%   -
            T| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%   -     -
            9| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%   -     -     -
            8| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%   -     -
            7| 100.% 100.% 100.%   -     -   100.% 100.% 100.% 100.% 100.% 100.%   -     -
            6| 100.% 100.%   -     -     -     -     -   100.% 100.% 100.% 100.%   -     -
            5| 100.% 100.%   -     -     -     -     -     -     -   100.% 100.% 100.%   -
            4| 100.% 100.%   -     -     -     -     -     -     -     -   100.% 71.6%   -
            3| 100.% 100.%   -     -     -     -     -     -     -     -     -   100.%   -
            2| 100.% 100.%   -     -     -     -     -     -     -     -     -     -   100.%
          ```);

          +p{
            まず、スモールブラインドの利得の期待値は-0.0454[bb]となっており、このゲームは
            スモールブラインドが若干不利であることが分かります。
            続いて、全体の58.29\%の手札でオールインをするのが良いと言っており、具体的にどの手札で
            オールインすべきかが続いて表形式で示されています。
            この表の見方ですが、対角線の右上は手札のスートが揃っている場合を、対角線の左下は手札の
            スートが揃っていない場合を表しています。
            確率的にオールインを選択すべき場面はスートが揃っている4と3で手札が構成されているとき
            のみで、この場合は71.6\%の確率でオールインを選択すべきという結果になっています。
          }

          +p{
            続いてビッグブラインドの最適戦略についても見ていきましょう：
          }

          +console(```
            [Caller (Big blind)]
            - EV = +0.0454[bb]
            - Overall call rate: 37.38%
             |   A     K     Q     J     T     9     8     7     6     5     4     3     2
            -+------------------------------------------------------------------------------
            A| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%
            K| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.%
            Q| 100.% 100.% 100.% 100.% 100.% 100.% 100.% 100.% 40.0%   -     -     -     -
            J| 100.% 100.% 100.% 100.% 100.% 100.% 100.%   -     -     -     -     -     -
            T| 100.% 100.% 100.% 100.% 100.% 100.%   -     -     -     -     -     -     -
            9| 100.% 100.% 100.%   -     -   100.%   -     -     -     -     -     -     -
            8| 100.% 100.%   -     -     -     -   100.%   -     -     -     -     -     -
            7| 100.% 100.%   -     -     -     -     -   100.%   -     -     -     -     -
            6| 100.% 100.%   -     -     -     -     -     -   100.%   -     -     -     -
            5| 100.% 100.%   -     -     -     -     -     -     -   100.%   -     -     -
            4| 100.%   -     -     -     -     -     -     -     -     -   100.%   -     -
            3| 100.%   -     -     -     -     -     -     -     -     -     -   100.%   -
            2| 100.%   -     -     -     -     -     -     -     -     -     -     -   100.%
          ```);

          +p{
            スモールブラインド側が上位58.29\%の手札でオールインしてきているので、それに対抗する
            ビッグブラインド側はそれよりタイトな上位37.38\%の手札でのみコールします。
            コールすべきかどうかの境界となっているのはスートが揃っているQと6で手札が構成されている
            場合で、このとき40.0\%の確率でコールを選択すべきという結果になっています。
          }

          +p{
            以上がプッシュ/フォールドゲームの解析になりますが、実はこの内容が行えるプログラムに
            ユーザーインターフェースを整えた程度のものが世間では有償で提供されていたりもします。
            説明は割とあっさり済ませてしまいましたが、実装が公開されていてかつきちんと高速に動作する
            プログラムというのは案外貴重だったりするのではないでしょうか。
          }
        >
      >

      +section{まとめと参考文献} <
        +p{
          以前にCFRアルゴリズムを用いた趣味プログラミングを書いたことがあったので、「せっかくだし
          記事にしてみるか～」くらいのノリで書き始めたら40ページを超える内容となってしまいました。
          記事にするからには不正確なことは書けませんし、これまで適当に理解したつもりだった内容も
          論文にちゃんとあたって勉強する必要がたびたび生じ、筆者としても学びの多いプロセスと
          なりました。
        }

        +p{
          さて本記事はポーカーを念頭に置いたものでしたが、具体例として出てきたのは非常に限られた
          ゲームであって、テキサスホールデムの解析には程遠いものでもありました。
          実際には2017年のほぼ同時期に\dfn{DeepStack}と\dfn{Libratus}という二つのプログラムが
          ヘッズアップのゲームにおいてプロを破っているなど、プログラムの開発者は何らかの方法で
          テキサスホールデムの複雑さに向き合っていることが分かります（使われている計算資源は
          一般人からするとしばしば膨大すぎるものではありますが）。
        }

        +p{
          そこで、ポーカーAIの最新の動向を知りたいという方のためにDeepStackとLibratusおよび
          それ以降の代表的なポーカーAIを独断でチョイスして、その名前と対応する論文のリストを
          時系列順に紹介しようと思います。
          開発者（研究者）がテキサスホールデムなどの非常に複雑なゲームに対してどう向き合って
          いるのか、気になる方はこれらの論文にぜひあたってみてください。
          CFRアルゴリズムの基礎を本記事で学んだ後ならば、これらの論文もだいぶ読みやすくなっている
          ことでしょう。
        }

        +listing{
          * \dfn{DeepStack}\footnote{M. Moravčík et al. \emph{DeepStack: Expert-level artificial
            intelligence in heads-up no-limit poker.} Science, 356 (6337):508–513. 2017.}:
            ヘッズアップテキサスホールデムで初めてプロを破ったという内容の論文です。
            適切にある種の値を管理することで、ゲーム途中においてこれまでの戦略を忘れても正しく
            部分ゲームのCFRが再計算できるという\dfn{continual re-solving}と、探索の深さが一定以上に
            なったらcounterfactual valueの計算をディープラーニングで学習させた\dfn{deep CFVnet}の
            推測値で代用するというテクニックを用いています。
            ターンラウンドの学習には175 CPUコア年の計算資源を用いるなど、使用されている計算資源は
            膨大です。

          * \dfn{Libratus}\footnote{N. Brown and T. Sandholm. \emph{Superhuman AI for heads-up
            no-limit poker: Libratus beats top professionals.} Science, 359(6374):418–424. 2018.}:
            こちらはヘッズアップテキサスホールデムで「トップ」プロを破ったという内容の論文です。
            抽象化されたゲームに対する戦略を前計算しておいて、実行時には抽象度を下げてCFRを再計算
            するというもので、こちらも必要な計算資源は膨大なものでした。

          * \dfn{Modicum}\footnote{N. Brown, T. Sandholm, and B. Amos. \emph{Depth-limited solving
            for imperfect-information games.} In NeurIPS. 2018.}:
            非常にコンパクトな計算資源でそこそこ強いポーカーAIを作るという内容の論文です。
            具体的には前計算に700 CPUコア時間と16GBメモリのみを用い、実行時も4コアCPUで平均20秒
            程度の思考時間に収まります。
            用いられているアルゴリズムは、まずモンテカルロCFRによって\dfn{blueprint}と呼ばれる戦略を
            前計算し、実行時はそのblueprintをもとにさらに探索を行うというものです。
            この実行時の探索というのが肝なのですが、深さが一定以上になった場合は相手の戦略を
            いくつかに限定してしまいます。

          * \dfn{Pluribus}\footnote{N. Brown and T. Sandholm. \emph{Superhuman AI for multiplayer
            poker.} Science, 365(6456):885–890. 2019.}:
            これまでのポーカーAIはヘッズアップに限られたものでしたが、こちらは六人テーブルの
            テキサスホールデムにおいてトッププロを破ったという内容になっています。
            使われているアルゴリズムはModicumと類似しており、blueprint戦略を利用するというものです。
            前計算に用いた計算資源は64コアCPUを8日と512GBメモリと中程度です。

          * \dfn{Supremus}\footnote{R. Zarick, B. Pellegrino, N. Brown, and C. Banister.
            \emph{Unlocking the potential of deep counterfactual value networks.} arXiv preprint
            arXiv:2007.10442. 2020.}:
            NeurIPSに投稿したがrejectされたのであろう論文です。
            ただし論文中で実装されているプログラムは現時点で恐らく最強のもので、DeepStackで
            用いられていたCFVnetをいろいろ改良した上で計算資源で殴ってみた、という内容に
            なっています。

          * \dfn{ReBeL}\footnote{N. Brown, A. Bakhtin, A. Lerer, and Q. Gong. \emph{Combining deep
            reinforcement learning and search for imperfect-information games.} In NeurIPS. 2020.}:
            不完全情報ゲームの解釈を変更して「戦略を宣言するとレフェリーがアクションを決定する」
            ものとして表現すると（\dfn{belief representation}）、AlphaZeroに代表される
            「強化学習 + 探索」の枠組みが二人ゼロサムであるような不完全情報ゲームにも適用できる
            ようになるという論文です。
            ヘッズアップテキサスホールデムではLibratusと同様にトッププロを有意に破る実力があり、
            \dfn{Liar's Dice}というゲームに対しては実装も公開されています。
            また、ポーカーに対する実装では既存のプログラムよりも使われているドメイン知識が少なく、
            情報の抽象化などをあえて行っていない点も特徴です。

          * \dfn{Player of Games (PoG)}\footnote{M. Schmid et al. \emph{Player of Games.} arXiv
            preprint arXiv:2112.03178. 2021.}:
            記事を書いている途中で見つけたDeepMind社の最新論文です。
            完全情報ゲームと不完全情報ゲームの両方の探索が統一的に扱える\dfn{growing-tree CFR}と
            呼ばれるアルゴリズムを提案し、完全情報ゲームであるチェスと囲碁、不完全情報ゲームである
            ヘッズアップテキサスホールデムとスコットランドヤードという四つのゲームにおいて、
            state-of-the-artとまではいかないが強力な結果を残したという論文です。
        }

        +p{
          さて、長かった本記事もここまでで本当に終わりです。
          不完全情報ゲームの解析において今や基礎とも言えるCFRアルゴリズムについて、背景、数式、
          実装、具体例を網羅した日本語記事というのも我ながら貴重なのではないかと思うのですが、
          いかがでしたでしょうか。
          書く方もそれなりに大変でしたが、ここまでちゃんと読まれた方も大変だったのではないかと
          思います。
          この記事が読者の方々に何らかの刺激を与えるものとなっていれば嬉しいですね。
          それではまた機会がありましたら紙面なりでお会いしましょう。
        }
      >
    >
  >

end
