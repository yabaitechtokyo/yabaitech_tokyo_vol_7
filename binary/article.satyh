@require: class-yabaitech/yabaitech
@require: azmath/azmath
@require: easytable/easytable
@require: latexcmds/latexcmds

module Binary : sig

  val article : block-text

end = struct

  open EasyTableAlias

  let article = '<
    +chapter(|
      bibliography = [];
      title = {不完全情報ゲームのナッシュ均衡を CFR (Counterfactual Regret Minimization) アルゴリズムで求めよう};
      title-for-toc = Option.none;
      subtitle = Option.none;
      author = {ばいなり};
    |)
    <
      +section{はじめに} <
        +p{
          yabaitech.tokyo vol.7 を手に取ってくださりありがとうございます。
          前回の vol.6 ではテキサスホールデムポーカーの役判定を高速に行う話を
          書かせていただきましたが、今回も引き続いてポーカーを念頭に置いた記事を
          投稿したいと思います。
        }

        +p{
          具体的には、ポーカーを含む\dfn{不完全情報ゲーム}において\dfn{最適解}とも言える
          \dfn{「ナッシュ均衡」}を求めるプログラムの実装を目指します。
          もちろん、実際にナッシュ均衡が現実的な時間内に求まる（より正確には計算結果が十分収束する）
          かどうかはゲームの状態数の大きさに依るため、状態数の非常に大きい（ノーリミット）
          テキサスホールデムの完全解析を行おうと思うと本記事の内容ではまったく不可能ですが、
          状態数の小さいゲームであれば最適解をプログラムによって求めることができるようになります。
        }

        +p{
          ゲーム理論周りに馴染みの無い方の中には、タイトルには仰々しい単語しか並んどらんし
          ナッシュ均衡とか言われてもさっぱり分からん、という方もいらっしゃるかもしれませんが、
          そういった方にもせめて雰囲気は伝わるような記事にできればと思っているので、
          ややタフな内容かもしれませんが各々の楽しみ方で読み進めていただけると幸いです
          \footnote{と言うよりむしろ、筆者も特にゲーム理論の専門家でも何でもなくて、
          ゲーム理論を齧っただけの筆者なりに初学者に興味を持ってもらうのを目的に記事を書いています。}
          。
        }
      >

      +section{用語の説明} <
        +p{
          本章では、本記事のタイトルを構成する主なキーワードとなっている「不完全情報ゲーム」
          「ナッシュ均衡」「CFRアルゴリズム」の３点について、それぞれ説明をしていきます。
        }

        +subsection{不完全情報ゲーム} <
          +p{
            まずは、タイトルの最初の単語である\dfn{「不完全情報ゲーム」}とは何ぞや？　
            というところから見ていきましょう。
            簡単に言ってしまえば、ボードゲームなどのゲームにおいて、
            各々のプレイヤーがすべての情報にアクセスできるならばそのゲームは\dfn{「完全情報」}、
            そうでないなら\dfn{「不完全情報」}であると呼ばれます。
          }

          +p{
            具体的な例を挙げると、\dfn{オセロ}や\dfn{将棋}といったゲームはお互いのプレイヤーが
            同じ盤面を共有していて、特定のプレイヤーからしかアクセスできないような情報は
            存在しないため、「完全情報」に分類されます。
            逆に、\dfn{ポーカー}や\dfn{麻雀}といったゲームは、各々のプレイヤーが自分にしか分からない
            「手札」を持っているので、各々がアクセスできる情報は対称ではなく「不完全情報」に
            分類されることになります。
            さらに、\dfn{じゃんけん}といった複数のプレイヤーが「同時に」アクションを起こす
            必要のあるゲームも「不完全情報」です。
            \dfn{バックギャモン}や\dfn{すごろく}といったゲームはどちらに分類すべきか
            微妙なところで、これらにはポーカーや麻雀のように秘匿された情報は無いため、
            ある意味では「完全情報」ですが、サイコロなどによる確率的なアクションを
            「確率的に出目を決める仮想的なプレイヤー」のものとして見なすと、
            「不完全情報」であるとも解釈できます。
            本記事ではこれらのゲームは「不完全情報」として扱うことにしておきます
            （「不確定・完全情報」という分類とするのが正確ではあるのですが）。
          }

          +p{
            完全情報ゲームは、盤面が与えられると神の視点からはすでに勝敗（または引き分け）が
            定まっており、例えば ${6 \times 6} マスのオセロは初期盤面からお互いが最善を尽くすと
            後手が４石差で勝つことが知られています。
            この完全情報ゲームの攻略に関しては、完全読み切りが極めて困難であるような
            状態数の大きいゲームに対しても、ディープラーニングと強化学習を組み合わせた
            \dfn{深層強化学習}を用いて、\dfn{AlphaGo}や\dfn{AlphaZero}といったプログラムが
            盤面評価の精度の面でブレークスルーを最近引き起こしたことをご存じの方も多いでしょう。
          }

          +p{
            不完全情報ゲームは、完全情報ゲームが持つこのような性質を持っていません。
            すべてのプレイヤーがそれぞれに与えられた情報をもとに「最善手」を取り続けても、
            さまざまな要因によって勝敗は変化しますし、そもそも「最善手」というのが
            一手に定まらずに\dfn{混合戦略}（複数のアクション候補から確率的にアクションを
            選択する戦略）となることも少なくありません。
            最善手が混合戦略になり得るというのは、説明だけでは分かりにくいと思うので後ほど
            じゃんけんを例に具体的に見ていきますが、不完全情報ゲームにおいては、
            評価が可能なのは「戦略」であって「アクション」ではないという点が非常に重要です。
            つまり、例えばじゃんけんにおいて「グーとチョキとパーを等確率で出す戦略」の良し悪しを
            評価することはできますが、「実際にグーを出した」という「アクション」を単体で
            評価することはできないということです。
            この性質が、プログラムによる精度の高い盤面評価を難しいものにしています。
          }
        >

        +subsection{ナッシュ均衡} <
          +p{
            不完全情報ゲームについて説明したところで、タイトル中の次のキーワードである
            \dfn{「ナッシュ均衡」}についても続いて説明していきましょう。
            ナッシュ均衡とは、ざっくり言ってしまえば「どのプレイヤーも利得（の期待値）を
            これ以上増やすことができない状態」のことを指します。
            はじめにナッシュ均衡のことを「最適解」と呼びましたが、具体的にはこのような意味だった
            わけですね。
          }

          +p{
            厳密な定義については、記号などをちゃんと導入したあとで再度確認しようと思いますが、
            もうちょっとだけ正確に文章で記述すると、「どのプレイヤーも、他のプレイヤーが
            そのナッシュ均衡に従って戦略を選択している限りは、自分の戦略を変更することによって
            利得（の期待値）をこれ以上増やすことができないような戦略の組」のことを
            ナッシュ均衡と言います。
          }

          +subsubsection{男女の争い} <
            +p{
              簡単な例として\dfn{「男女の争い」}と呼ばれるゲームを見てみましょう。
              今どきナントカコレクトネスに反してそうな名称・設定ではありますが、
              そこには目を瞑っていただくとして、ある男女の組がデートに行こうとしており、
              一緒に行動したいという点では同意しているものの、
              お互いが行きたい場所が異なるという状況をモデル化したものです。
              ここでは男性はスポーツ観戦に、女性は映画鑑賞に行きたいということにしておきましょう。
              このとき、それぞれ男性と女性が得る利得は次のように定められるものとします。
            }

            +centering{
              \easytable?:[t; b; m 1; v 1][c; c; c] {
                | 男性 ＼ 女性 | スポーツ観戦 | 映画鑑賞
                | スポーツ観戦 | (2, 1) | (0, 0)
                | 映画鑑賞 | (0, 0) | (1, 2)
              |}
            }

            +p{
              つまり、男性側は女性とともにスポーツ観戦を行った場合に最も利得を多く得ますが（２点）、
              次いで利得を多く得られるのは女性とともに映画鑑賞を行った場合で（１点）、
              一緒に行動できなかった場合は利得を得ることができません（０点）。
            }

            +p{
              このような設定下における（純粋戦略のみによる）ナッシュ均衡は、
              当たり前のようですが、（男性 ${\to} スポーツ鑑賞, 女性 ${\to} スポーツ鑑賞）と
              （男性 ${\to} 映画鑑賞, 女性 ${\to} 映画鑑賞）の２組となります。
              男性側にとっては、女性とともに映画鑑賞に行くというのは次善策なわけですが、
              女性側が映画鑑賞を選択している限りは、男性側の利得を最大化するためには
              男性側も映画鑑賞を選択する必要があるため、これが均衡の一つとなります。
            }

            +p{
              この例から分かる重要な点は、一般にナッシュ均衡は必ずしも1組とは限らないということ、
              またナッシュ均衡はすべてのプレイヤーの戦略の「組」として与えられるため、
              ある1人のプレイヤーにとって「相手の戦略に関わらず最善」という戦略は
              存在しない場合があるということです。
            }
          >

          +subsubsection{じゃんけんと混合戦略} <
            +p{
              ところで、本記事では主にポーカーの戦略を考えたいわけですから、ゲームの条件として
              「各プレイヤーの利得はゼロサムである」（各プレイヤーの利得を足し合わせるとゼロになる）
              ことも追加しましょう。
              先ほどの「男女の争い」の例はゼロサムではありませんでしたので、
              ゼロサムであるゲームとして\dfn{じゃんけん}
              \footnote{
                知人が豪語していたじゃんけんの「必勝法」を紹介しましょう。曰く、
                「『最初はグー』の状態から遷移に最も時間が掛かるのは『パー』に切り替える場合だ。
                相手が『パー』に切り替えようとしているかどうかを見極めるには、
                相手の小指に注目すれば良い。よって、基本はこちらは『グー』のままだが、
                相手の小指が動いたらこちらは『チョキ』に切り替えろ。
                『グー』から『チョキ』に切り替えるのは、『パー』に切り替えるよりは素早く行えるから、
                優れた動体視力と反射神経があれば間に合わせることができる」。
                全員がこの「必勝法」を採用すると「グー」を出し続ける無限ループに陥ることになるので、
                そもそも「必勝法」の定義を満たしていないように筆者的には思うのですが、
                額面通りに事を運ばせられるのならば確かにまあ負けることは無いので、
                動体視力と反射神経に自信のある方は試してみると良いのではないでしょうか。
              }
              を考えてみることにしましょう。
              特に解説は要らないと思いますが、じゃんけんの利得表は次のようになります：
            }

            +centering{
              \easytable?:[t; b; m 1; v 1][c; c; c; c] {
                | | グー | チョキ | パー
                | グー | (0, 0) | (1, -1) | (-1, 1)
                | チョキ | (-1, 1) | (0, 0) | (1, -1)
                | パー | (1, -1) | (-1, 1) | (0, 0)
              |}
            }

            +p{
              このようなゲームにおいてもナッシュ均衡は存在するのでしょうか？　
              例えばプレイヤーAが「グー」を出した場合を想定すると、プレイヤーBは「パー」を出すのが
              最善で、そうするとプレイヤーAは「チョキ」を出すのが最善で…と循環してしまい、
              お互いが妥協できる均衡状態は一見して存在せず、よってナッシュ均衡も存在しないように
              思えます。
            }

            +p{
              ここで登場する概念が\dfn{「混合戦略」}というものです。
              混合戦略とは、例えばグーを50\%、チョキを30\%、パーを20\%で出すといったように、
              確率的にアクションを決定する戦略のことです（これに対して例えば決まってグーを
              出すといったような戦略は\dfn{「純粋戦略」}と呼ばれます）。
              今回のじゃんけんの例では、直感的か非直感的かは人に依ると思いますが、
              ３種類の手をそれぞれ等確率に出すという戦略を両者が採用するというのが、
              このゲームにおける唯一のナッシュ均衡となります。
            }

            +p{
              このナッシュ均衡の解釈は一筋縄ではいかないものです。
              ナッシュ均衡とは「どのプレイヤーも利得をこれ以上増やすことができない」という
              状態のことで、言わば「最適解」であると説明されたはずなのに、
              必ずグーを出すような非常に弱い相手に対しても利得の期待値がゼロにしかならない戦略
              （３種類の手をそれぞれ等確率に出す）をお互い取ることがそのナッシュ均衡なんです、
              と言われても納得できない方もいると思います。
              この事態を直感的に捉えられるようにするには、少なくとも筆者にとっては
              次のような言い換えを経る必要がありました：
            }

            +with-param(AZMathEquation.vmargin-between-eqn)(fun ctx -> get-font-size ctx) <
              +align?:(AZMathEquation.notag)(${
                |           |\text!{ナッシュ均衡ではどのプレイヤーも利得をこれ以上増やすことができない}
                |\Rightarrow|\text!{\dfn{２人ゼロサムゲーム}において１人がナッシュ均衡から外れたと仮定する}
                |\Rightarrow|\text!{ナッシュ均衡から外れたプレイヤーは利得が増えることはない}
                |\Rightarrow|\text!{\dfn{ゼロサム性}よりナッシュ均衡の戦略を取り続けたプレイヤーは利得が減ることはない}
                |\Rightarrow|\text!{\dfn{ナッシュ均衡の戦略を取り続けることで利得の最低値が保証される}}
              |});
            >

            +p{
              つまり、\dfn{２人ゼロサムゲーム}においては、ナッシュ均衡の戦略を取ることは
              「相手の戦略に関わらず保証される利得を最大化できる」という意味で「最適解」なのであり、
              「相手の特定の戦略に対して利得を最大化できる」というようなものではないのです。
              なお後者のような\dfn{「特定の戦略」}を狙い撃ちするような戦略は\dfn{搾取戦略}と呼ばれ、
              例えば必ずグーを出す相手に対して必ずパーを出すといった戦略はこれに該当します。
              ただし、このように「必ずパーを出す」という戦略は、「必ずチョキを出す」という
              \dfn{対抗搾取戦略}に搾取されてしまいます。
              ナッシュ均衡とは、言わばすべてのプレイヤーがお互いを最大限搾取している
              理想的な状態で、達人どうしが戦ったらこうなりますという状態なわけですから、
              「必ずパーを出す」というような簡単に搾取されてしまう戦略は、
              その理想からはかけ離れてしまっているわけですね。
            }

            +p{
              このような考察を経ると、２人ゼロサムゲームにおいてはナッシュ均衡はまさに
              \dfn{「最善手」}の組であると言えることが分かります。
              ナッシュ均衡は基本的にはあくまで戦略の「組」ですから、「男女の争い」の例を
              思い出していただくと分かるように、非ゼロサムゲームでは「相手の戦略に関わらず最善」
              という戦略は存在しない場合があったわけです。
              ところが、２人ゼロサムゲームにおいては、ナッシュ均衡に基づく戦略を取ることで
              「相手の戦略に関わらず利得の最低値が保証される」わけですから、これはまさしく
              「最善手」であると言って良いでしょう。
            }

            +p{
              なお今回のじゃんけんの例では、得られた「最善手」は３種類の手をそれぞれ等確率に出すと
              いうもので、これはどのような相手に対しても利得がゼロになってしまうというものでしたが、
              これはどちらかと言うと「具体例が悪い」という類のものです。
              よくあるポーカーの状況などでは、こちらがナッシュ均衡に基づく最善手を取っていて、
              相手が最善手から離れているという場合は、相手も最善手を取っている場合と比べて
              得られる利得は増加する場合がほとんどです。
            }

            +p{
              ところで、ある時点からゲームが突然に２人ゲームに限定されてしまいましたが、
              ３人以上が関与するゼロサムゲームではナッシュ均衡に意味はあるのでしょうか。
              この点については、ふわっとした結論として「ナッシュ均衡に従うのは実用上は
              それなりに強いが、理論的な保証は無くなってしまう」と言うくらいが精々です。
              理論的な保証が無くなってしまうというのは、３人ゼロサムゲームは２人非ゼロサムゲームの
              一般化である（ゼロサムとなるようにダミープレイヤーを１人追加すれば良い）ことを考えて
              もらえれば分かりやすいかと思いますが、「男女の争い」と同じく「相手の戦略に関わらず
              最善」というような戦略は存在しない場合があるという意味です。
              ただし、実用上はナッシュ均衡に従ってプレイするとそれなりに強いことが多く、
              理論的な保証が無くなることを理解していながらも、実用上の汎用的な強さを重視して、
              ３人以上が関与するゲームにおいてもナッシュ均衡を求めることが実際にはよく目標と
              されています。
            }

            +p{
              さて、説明がやや長くなってしまったものの、じゃんけんを例にゼロサムゲームにおける
              ナッシュ均衡の性質について深堀りしてみましたが、いかがでしたでしょうか。
              本記事ではナッシュ均衡を求めるプログラムをこれから語っていくことになるので、
              「ナッシュ均衡を求めることにどのような意味があるのか」についてあやふやなままだと、
              記事としての説得力に欠けるかなあということで詳しめに説明してみました。
            }

            +p{
              なおこれは余談ですが、ほとんどすべての有限ゲームにはナッシュ均衡が奇数個存在することが
              知られています（ここでの「ほとんどすべて」は数学用語です）。
              じゃんけんについてはナッシュ均衡は１つだったので良いとして、
              男女の争いについては中途半端に２つしか見つけられていませんが、
              どういうことなのでしょうか。
              実は、これは混合戦略を考慮していなかったことが原因で、混合戦略までちゃんと考えると
              （男性 ${\to} ${2\ /\ 3} の確率でスポーツ観戦,
                女性 ${\to} ${2\ /\ 3} の確率で映画鑑賞）
              という戦略の組もナッシュ均衡となります。
              ナッシュ均衡、完全に理解したわなどと言っていると足を掬われますのでご注意ください。
            }
          >
        >

        +subsection{CFR (Counterfactual Regret Minimization) アルゴリズム} <
          +p{
            さて、ナッシュ均衡を乗り越えたと思ったら、またしても仰々しいキーワードが出てきて
            しまいましたね。
            CFRアルゴリズムとは、一言で言えば「展開型の２人ゼロサムゲームにおいてナッシュ均衡が
            求まることが保証されているアルゴリズム」なのですが、それがどのようなアルゴリズムなのか、
            まずは数式を使わずに文章で雰囲気を掴んでいただければと思います。
          }

          +p{
            まず、キーワード中の\dfn{リグレット(regret)}の部分から説明を始めることにしましょう。
            リグレットとは直訳すると\dfn{後悔}のことで、日本語訳する際にリグレットと呼ばずに
            後悔という語がそのまま使われることもありますが、これは過去の自分のアクションに対して、
            あのときああすれば良かったのにという後悔の度合いを定量化したものです。
          }

          +p{
            例えば、再びじゃんけんを考えるとして、自分はグーを出し、相手にパーを出されて
            負けた（１点を失った）としましょう。
            このとき、もし自分もパーを出していれば引き分けとなり、１点を失わずに済んだはず
            だったので、パーに対するレグレットは１点となります。
            もっと言えば、自分がチョキを出せていれば勝ちとなり、むしろ１点を得ることが出来ていたはず
            だったので、チョキに対するリグレットは差し引き２点となります。
            なお、もし逆に自分がパーを出し、相手はグーを出して勝負に勝った（１点を得た）場合、
            グーとチョキに対するリグレットはそれぞれ－１点と－２点になり、リグレットは負の値を
            取ることになります。
          }

          +p{
            さて、じゃんけんも何戦かこなすと、これまでのリグレットの総和を計算することで、
            「過去にどのようなアクションを取るべきだったのか」が段々と見えてくることになるでしょう。
            このリグレットの総和（ただし総和が負の値になった場合はゼロとして扱う）に比例するように
            混合戦略を立てて、将来的なリグレットを最小化しようという、単純かつ強力な手法が
            \dfn{regret-matchingアルゴリズム}と呼ばれるものです。
            具体的には、例えばグー、チョキ、パーそれぞれに対する現在のリグレットの総和が
            ２点、－１点、８点であるとしましょう。
            このとき、負の値となっているチョキに対するリグレットの総和は０点として扱うこととし、
            ２点、０点、８点に比例するように、グー、チョキ、パーをそれぞれ20\%、0\%、80\%の確率で
            出す混合戦略を立てるということになります。
          }

          +p{
            このregret-matchingアルゴリズムを用いて\dfn{自己対戦}
            \footnote{同じアルゴリズムどうしを対戦させるという意味で、リグレットの値などを
            共有するわけではありません。}
            を繰り返し行うと、各時刻の混合戦略の平均
            \footnote{時刻が無限大のときの混合戦略そのものではない点に注意！}
            が\dfn{相関均衡}に収束することが知られています
            \footnote{S. Hart and A. Mas-Colell. \dfn{A simple adaptive procedure leading to
            correlated equilibrium.} Econometrica, 68(5):1127–1150. 2000.}。
            なお、ここで相関均衡とかいう用語が突然出てきてしまいましたが、これはナッシュ均衡を
            一般化した解概念で、今回の２人じゃんけんの例ではナッシュ均衡と一致するので
            特に気にしなくて大丈夫です。
            シンプルなアルゴリズムでありながら、均衡状態を近似的に必ず求めることができるという点で、
            このregret-matchingアルゴリズムは重要な発明であったと言えるでしょう
            （なお脚注に示したように論文が出たのは2000年と結構最近の話です）。
          }

          +p{
            さて、リグレットおよびregret-matchingアルゴリズムについてはこのあたりで良いとして、
            \dfn{counterfactual regret}とは何なのかについても見ていきましょう。
            counterfactualという英単語は「事実に反する」「反事実的」という意味の形容詞で、
            それだけ聞くと「そもそも後悔(regret)とは事実に反することに対してなされるものなので、
            “counterfactual regret”というのは『馬から落馬』『頭痛が痛い』のようなものではないか」
            と思われるかもしれませんが、そうではなくて“counterfactual value”と呼ばれる値を
            利得として解釈したときのリグレットなので、“counterfactual regret”と呼ばれるわけですね。
          }

          +p{
            Counterfactual valueは、\dfn{展開型ゲーム}と呼ばれる種別のゲームにおいて定義できる値です。
            展開型ゲームとは、ゲームの状態を表現する頂点と、アクションによって遷移できることを示す
            有向辺からなる\dfn{ゲーム木}を用いた、グラフ形式で表現されるゲームのことです。
            グラフに馴染みのない方にはまた何のことやらという感じだと思いますが、このモデルにおいて
            重要なのは、\dfn{「手番」}が存在してプレイヤーが１人ずつアクションを起こすようにゲームが
            モデル化されているという点です。
            一方で、\dfn{標準型ゲーム}（あるいは\dfn{戦略型ゲーム}）には手番は存在せず、すべての
            プレイヤーが同時に戦略を決定する必要があります（これまでに見てきた「男女の争い」や
            「じゃんけん」はこちらに含まれます）。
          }

          +p{
            展開型ゲームは標準型ゲームを内包する概念なので、すべての標準型ゲームは展開型ゲームに
            変換することもできます。
            具体的には、便宜上の手番を順番に割り振って、手番が後のプレイヤーはそれまでになされた
            アクションの区別がつかない、という設定にすれば良いだけです（例えばじゃんけんならば、
            あるプレイヤーがグーを出すことを予め決定しているものの、他のプレイヤーからはそれが
            観測できないというような状況を想定してください）。
          }

          +p{
            さて、ここまで説明してようやくcounterfactual regretの説明に移ることができます。
            と言っても、数式を使わずに説明するのは無理がある内容なので、ちゃんとした説明は
            記号を導入してから行いますが、各プレイヤーが取る戦略${\sigma}が固定されているもとでの、
            プレイヤー${i}の手番であるような状態${h}への\dfn{counterfactual-到達確率}を、
            「プレイヤー${i}の戦略${\sigma_i}の寄与を無視した状態${h}への到達確率」として
            まず定義します。
            これがなぜ“counterfactual”であるかというと、状態${h}にたどり着くには、
            プレイヤー${i}は戦略${\sigma_i}に従って実際には何らかの確率的選択を行っていたかも
            しれないのに、「過去の意思決定においては常に状態${h}を目指していました」と主張して
            到達確率を計算するわけですから、これは確かに事実に反していますよね。
            このcounterfactual-到達確率に、状態${h}における利得の期待値
            \footnote{ここで「利得の期待値」と言っているのは、利得は本来はゲームの終端状態のみに
            おいて定まる値ですが、各プレイヤーの戦略が固定されているので、非終端状態においても
            ボトムアップに計算を行うことで定まる利得の期待値のことを指しています。}
            を掛け合わせたものが\dfn{counterfactual value}になります。
            最後に、状態${h}における\dfn{counterfactual regret}は、利得の値を直接使う代わりに
            このcounterfactual valueを用いて計算されたリグレットのことです。
          }

          +p{
            ${\cdots\cdots}とまあ、文章だけでの説明を試みてみましたが、数式と具体例が無いと
            理解してもらうのは厳しい内容でしたね。
            後ほど数式を用いてちゃんと説明しなおすことにしましょう。
          }

          +p{
            このcounterfactual regretを用いてregret-matchingアルゴリズムを実行し、自己対戦を行うと、
            各時刻の戦略の重み付き平均を取ることで展開型の２人ゼロサムゲームのナッシュ均衡を
            求めることができます
            \footnote{M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. \dfn{Regret
            minimization in games with incomplete information.} In NIPS, pages 1729–1736. 2007.}
            \footnote{ナッシュ均衡の説明に引き続いて、またしても「２人ゼロサム」の条件が突然
            加えられてしまいましたが、今回も３人以上の設定でも「実用上それなりに強い」解を求められる
            ことが知られています。}。
            今回も説明が長くなってしまいましたが、これがCFR (Counterfactual Regret Minimization)
            アルゴリズムと呼ばれるものの正体で、ポーカーを含む不完全情報ゲームの近年の解析における
            基礎的なアルゴリズムの一つとなっています。
          }
        >
      >

      +section{記号の定義} <
        +p{
          本記事のタイトルを構成するキーワードについての大雑把な説明が済んだところで、
          数学的な議論を行えるようにいよいよ記号を導入していきましょう。
        }

        +listing{
          * ${N = \set{0,\ 1,\ \cdots,\ n - 1}}:
            ${n}人のプレイヤー全体の集合。

          * ${c}:
            \dfn{偶然手番}。
            アクションを予め定められた確率に基づいて実行する仮想的なプレイヤー。
            サイコロを振る、カードをシャッフルするといった行為は、この偶然手番が行っているものと
            見なします。

          * ${h \in H}:
            \dfn{履歴}${h}および履歴全体の集合${H}．
            履歴はゲーム木における頂点のことで、今まで「状態」と呼んでいたものに相当します。
            単に「状態」だと、例えば履歴は異なるが盤面などは同じという状況を同一視するようにも
            解釈できてしまうので、それらを区別することを明示するためにも「履歴」という語を用います。

          * ${Z \subseteq H}:
            \dfn{終端履歴}の集合。
            終端履歴${z \in Z}まで到達するとゲームは終了し、各プレイヤーに対する利得が定まります。

          * ${P: H \setminus Z \to N \cup \set{c}}:
            \dfn{手番関数}。
            ${\app{P}{h}}は非終端履歴${h \in H \setminus Z}において次にアクションを行うプレイヤーを表します。

          * ${\app{A}{h} = \set{a \mid ha \in H}}:
            \dfn{アクション集合}。
            非終端履歴${h \in H \setminus Z}においてプレイヤー${\app{P}{h}}が行うことが可能な
            アクションの集合を表します。

          * ${u_i: Z \to \mathbb{R}}:
            プレイヤー${i}の\dfn{利得関数}。
            ${\app{u_i}{z}}は、終端履歴${z \in Z}における、プレイヤー${i}にとっての利得の実数値を表します。
            ２人ゼロサムゲームの場合、${\app{u_0}{z} = -\app{u_1}{z}}が成り立ちます。

          * ${I_i \in \mathcal{I}_i}:
            プレイヤー${i}に関する\dfn{情報集合}${I_i}および\dfn{情報分割}${\mathcal{I}_i}．
            なおプレイヤーに関して興味が無い場合は、添字の${i}を省略することがあります。
            情報分割${\mathcal{I}_i}は、手番がプレイヤー${i}であるような履歴の集合
            ${\set{h \in H \mid \app{P}{h} = i}}の分割を表します。
            プレイヤー${i}に関する情報集合${I_i}は、情報分割${\mathcal{I}_i}の元ですから
            ${\app{P}{h} = i}なる履歴${h}の集合のことで、${h, h' \in I_i}のとき、
            つまり履歴${h}と${h'}がともに同じ情報集合${I_i}に属するとき、
            プレイヤー${i}からはこれらの履歴を区別することができないことを意味します。
            また、手番関数${P}やアクション集合${A}について、${h, h' \in I}ならば
            ${\app{P}{h} = \app{P}{h'},\ \app{A}{h} = \app{A}{h'}}が成り立ちますから、
            これらを${\app{P}{I},\ \app{A}{I}}と型を混同して記述することにします。

          * ${\sigma_i \in \Sigma_i}:
            プレイヤー${i}の\dfn{戦略}${\sigma_i}および戦略全体の集合${\Sigma_i}．
            戦略${\sigma_i}は情報集合${I_i}を受け取って、次に行うアクション候補の集合
            ${\app{A}{I_i}}上の確率分布を返す関数です。
            戦略${\sigma_i}のもとで、情報集合${I_i}においてアクション${a}を行う確率を
            ${\app{\sigma_i}{I_i,\ a}}と表記します。
            なお、${\app{\sigma_i}{I_i}}は確率分布なので、
            ${\sum_{a \in \app{A}{I_i}} \app{\sigma_i}{I_i,\ a} = 1}が成り立ちます。
            また、各プレイヤーの戦略${\sigma_0,\ \sigma_1,\ \cdots,\ \sigma_{n - 1}}をまとめたものを
            ${\sigma}と表記し、${\sigma}から${\sigma_i}を除いたものを${\sigma_{-i}}と表記します。

          * ${\pi^\sigma: H \to \pB{0,\ 1}}:
            ${\app{\pi^\sigma}{h}}は、各プレイヤーが戦略${\sigma}に従っているもとでの履歴${h}への
            \dfn{到達確率}を表します。
            また、${\app{\pi_i^\sigma}{h}}を${\app{\pi^\sigma}{h}}におけるプレイヤー${i}の
            寄与として定めます。
            すなわち、${\app{\pi^\sigma}{h} = \prod_{i \in N \cup \set{c}} \app{\pi_i^\sigma}{h}}．
            さらに、${\app{\pi_{-i}^\sigma}{h}}を${\app{\pi^\sigma}{h}}からプレイヤー${i}の寄与
            ${\app{\pi_i^\sigma}{h}}を除外した到達確率として定めます。
            すなわち、${\app{\pi_{-i}^\sigma}{h} = \app{\pi^\sigma}{h}\ /\ \app{\pi_i^\sigma}{h}}．

          * ${\pi^\sigma: \mathcal{I} \to \pB{0,\ 1}}:
            ${\app{\pi^\sigma}{I}}は、各プレイヤーが戦略${\sigma}に従っているもとでの、
            情報集合${I}に含まれるいずれかの履歴への到達確率を表します。
            すなわち、${\app{\pi^\sigma}{I} = \sum_{h \in I} \app{\pi^\sigma}{h}}．
            また、${\app{\pi_i^\sigma}{I}}および${\app{\pi_{-i}^\sigma}{I}}もこれまでと同様に
            定義します。

          * ${\app{u^\sigma_i}{h} = \sum_{z \in Z} \app{u_i}{z} \app{\pi^\sigma}{h \to z}}:
            戦略${\sigma}に従ったときの、履歴${h}におけるプレイヤー${i}の\dfn{利得の期待値}。
            ここで、${\app{\pi^\sigma}{h \to z}}は履歴${h}に到達した状態から、各プレイヤーが
            戦略${\sigma}に従って履歴${z}に到達する確率を表します。
            また、${\app{u_i}{\sigma}}をゲーム開始時におけるプレイヤー${i}の利得の期待値として
            定義します。
            すなわち、${\emptyset}をゲーム開始時の履歴としたとき、
            ${\app{u_i}{\sigma} = \app{u^\sigma_i}{\emptyset}}．
        }

        +p{
          定義が突然大量に出てきて混乱している方がほとんどかと思いますが、どれも今後の議論で
          必要となる定義ですので、必要に応じてここに立ち返ってきてください。
        }

        +subsection{ナッシュ均衡の定義} <
          +p{
            記号をちゃんと定義したので、ナッシュ均衡についてまずは定義を確認してみましょう。
            戦略の組${\sigma}が\dfn{ナッシュ均衡}であるとは、すべてのプレイヤー${i}に対して
            \eqn?:(AZMathEquation.notag)(${
              \app{u_i}{\sigma} = \max_{\sigma'_i \in \Sigma_i} \app{u_i}{\sigma'_i,\ \sigma_{-i}}
            });
            を満たすことを言います。
            また、ナッシュ均衡の近似である\dfn{${\epsilon}-ナッシュ均衡}についても定義して
            しまうことにすると、戦略の組${\sigma}が\dfn{${\epsilon}-ナッシュ均衡}であるとは、
            すべてのプレイヤー${i}に対して
            \eqn?:(AZMathEquation.notag)(${
              \app{u_i}{\sigma} + \epsilon \geq \max_{\sigma'_i \in \Sigma_i} \app{u_i}{\sigma'_i,\ \sigma_{-i}}
            });
            を満たすことを言います。
            CFRアルゴリズムによって求まるのは正確にはこちらです。
          }
        >

        +subsection{CFRアルゴリズムの定義} <
          +p{
            同様に、文章だけの説明ではいまいち良く分からなかったCFRアルゴリズム周りの定義についても
            確認していきましょう。
            先ほどの説明において、戦略${\sigma}が定まっているもとでの、プレイヤー${i}の手番で
            あるような履歴${h}への\dfn{counterfactual-到達確率}と呼んでいたものは、まさに先ほど
            定義した${\app{\pi_{-i}^\sigma}{h}}のことです。
            続いて、\dfn{counterfactual value}はこのcounterfactual-到達確率と利得の期待値の積のこと
            でしたから、次のように定義されます：

            \eqn?:(AZMathEquation.notag)(${
              \app{v_i}{\sigma,\ h} = \app{\pi_{-i}^\sigma}{h} \cdot \app{u^\sigma_i}{h}.
            });

            履歴${h}におけるアクション${a}に対する\dfn{counterfactual regret}は、
            次のように定義されます:

            \eqn?:(AZMathEquation.notag)(${
              \app{r_i}{h,\ a} = \app{v_i}{\sigma_{I \to a},\ h} - \app{v_i}{\sigma,\ h}.
            });

            ここで${\sigma_{I \to a}}は、履歴${h}を含むような情報集合${I}を受け取った場合のみ、
            必ずアクション${a}を取るように戦略${\sigma}を変更したものを表すこととします。
            さらに、情報集合${I}におけるアクション${a}に対する\dfn{counterfactual regret}を
            次のように定義します：

            \eqn?:(AZMathEquation.notag)(${
              \app{r_i}{I,\ a} = \sum_{h \in I} \app{r_i}{h,\ a}.
            });

            CFRアルゴリズムは自己対戦を繰り返し行うというものでしたから、時刻${t}における
            戦略${\sigma^t}のもとでのcounterfactual regretを${\app{r_i^t}{I,\ a}}で表すことにしましょう。
            このとき、counterfactual regretの累積値${\app{R_i^T}{I,\ a}}は単に次のように計算されます：

            \eqn?:(AZMathEquation.notag)(${
              \app{R_i^T}{I,\ a} = \sum_{t = 1}^{T} \app{r_i^t}{I,\ a}.
            });

            各時刻における戦略${\sigma^t}は、regret-matchingアルゴリズムによって、これまでの
            リグレットの総和に比例するように与えられることを思い出すと、各時刻における戦略は
            次のように表すことができます：

            \eqn?:(AZMathEquation.notag)(${
              \app{\sigma_i^{T + 1}}{I,\ a} = \cases{
                | \frac{\app{R_i^{T,+}}{I,\ a}}{\sum_{a' \in \app{A}{I}} \app{R_i^{T,+}}{I,\ a'}} | \text!{if} \sum_{a' \in \app{A}{I}} \app{R_i^{T,+}}{I,\ a'} > 0
                | \frac{1}{\pabs{\app{A}{I}}} | \text!{otherwise.}
              |}
            });

            なお、ここで${\app{R_i^{T,+}}{I,\ a} = \app{\max}{\app{R_i^T}{I,\ a},\ 0}}です。
            最後に、２人ゼロサムゲームにおいてナッシュ均衡に収束するのは、各時刻の戦略を
            ${\app{\pi_i^{\sigma^t}}{I}}で重み付けした場合の平均戦略であることに注意してください：

            \eqn?:(AZMathEquation.notag)(${
              \app{\bar{\sigma}_i^T}{I} = \frac
                {\sum_{t = 1}^T \p{\app{\pi_i^{\sigma^t}}{I} \cdot \app{\sigma_i^t}{I}}}
                {\sum_{t = 1}^T \app{\pi_i^{\sigma^t}}{I}}.
            });
          }

          +subsubsection{CFRアルゴリズムの改良} <
            +p{
              以上が「バニラ」なCFRアルゴリズムの定義ですが、収束を実用上早めるために様々な変種が
              提案されてもいます。
              ここでは、実装が比較的簡単でかつ効果の高い\dfn{Discounted CFR (DCFR) アルゴリズム}
              \footnote{N. Brown and T. Sandholm. \dfn{Solving imperfect-information games via
              discounted regret minimization.} In AAAI, pages 1829–1836. 2019.}
              を紹介することにしましょう。
            }

            +p{
              DCFRアルゴリズムは３つの実数パラメータ${\alpha,\ \beta,\ \gamma}を取り、先ほどの
              ${\app{R_i^T}{I,\ a}}と${\app{\bar{\sigma}_i^T}{I}}を次のように再定義します：

              \gather?:(AZMathEquation.notag)(${
                | \app{R_i^{T + 1}}{I,\ a} = \cases{
                  | \frac{T^\alpha}{T^\alpha + 1} \app{R_i^T}{I,\ a} + \app{r_i^{T + 1}}{I,\ a} | \text!{if}\ \app{R_i^T}{I,\ a} \geq 0
                  | \frac{T^\beta}{T^\beta + 1} \app{R_i^T}{I,\ a} + \app{r_i^{T + 1}}{I,\ a} | \text!{otherwise},
                |}
                | \app{\bar{\sigma}_i^T}{I} = \frac
                    {\sum_{t = 1}^T \p{t^\gamma \cdot \app{\pi_i^{\sigma^t}}{I} \cdot \app{\sigma_i^t}{I}}}
                    {\sum_{t = 1}^T \p{t^\gamma \cdot \app{\pi_i^{\sigma^t}}{I}}}.
              |});

              ${\p{\alpha,\ \beta,\ \gamma} = \p{\infty,\ \infty,\ 0}}のとき、バニラな
              CFRアルゴリズムと一致します
              （${T + 1 = 2}のときに変なことが起こりますがそれは無視することとして）。
              提案論文では、さまざまなパラメータを実験的に試した結果、
              ${\p{\alpha,\ \beta,\ \gamma} = \p{1.5,\ 0,\ 2}}が一貫して良いパフォーマンスを
              示すとされており、今後の実装においてもこのパラメータを採用することとします
              （特に、${\p{\alpha,\ \beta,\ \gamma} = \p{\infty,\ -\infty,\ 1}}の場合に相当する
              \dfn{CFR+アルゴリズム}
              \footnote{O. Tammelin, N. Burch, M. Johanson, and M. Bowling. \dfn{Solving heads-up
              limit texas hold'em.} In IJCAI, pages 645–652. 2015.}
              がそれまでのstate-of-the-artとされていたのですが、このCFR+に対して一貫してより
              優れた結果をもたらしています）。
            }

            +p{
              非常にアドホックな感がある改変ではありますが、なぜこれで上手くいくようになるのかを
              かなりざっくり説明すると、リグレットの累積値${\app{R_i^T}{I,\ a}}および平均戦略
              ${\app{\bar{\sigma}_i^T}{I}}の計算において、各時刻における値を均一に重み付けするの
              ではなく、後の時刻ほど重みが大きくなるように改変をしている点が本質です
              （最初の方の時刻の寄与が“discount”されているわけです）。
              時刻が後になるほど、徐々に賢い戦略を獲得していくアルゴリズムですから、後の時刻ほど
              重みを大きく設定することで収束を早くできるというのは直感的にも正しいように思いますが、
              いかがでしょうか。
            }
          >
        >

        +subsection{可搾取量(Exploitability)} <
          +p{\REMAINS{書けたら書く}}
        >
      >

      +section{CFRアルゴリズムの実装} <
        +p{\REMAINS{ゲーム木のノードのインターフェースを定義}}
        +p{\REMAINS{CFRアルゴリズムの実装を示す}}
      >

      +section{CFRの具体例1: Kuhn Poker} <
        +p{\REMAINS{Kuhn Pokerの紹介}}
        +p{\REMAINS{ゲーム木のノードのインスタンスを作成}}
        +p{\REMAINS{結果発表}}
      >

      +section{CFRの具体例2: Push/Fold Hold'em} <
        +p{\REMAINS{Push/Fold Hold'emの紹介}}
        +p{\REMAINS{ゲーム木のノードのインスタンスを作成}}
        +p{\REMAINS{結果発表}}
      >

      +section{まとめ} <
        +p{\REMAINS{まとめ}}
      >
    >
  >

end
